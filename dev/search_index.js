var documenterSearchIndex = {"docs":
[{"location":"#DSGE.jl-1","page":"Home","title":"DSGE.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"CurrentModule = DSGE","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The DSGE.jl* package implements the New York Fed DSGE model and provides general code to estimate many user-specified DSGE models. The package is introduced in the Liberty Street Economics blog post The FRBNY DSGE Model Meets Julia.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"This Julia-language implementation mirrors the MATLAB code included in the Liberty Street Economics blog post The FRBNY DSGE Model Forecast.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The New York Fed DSGE team is currently working on adding methods to solve nonlinear and heterogeneous agent DSGE models. Extensions of the DSGE model code may be released in the future at the discretion of the New York Fed.","category":"page"},{"location":"#Table-of-Contents-1","page":"Home","title":"Table of Contents","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Pages = [\n  \"model_design.md\",\n  \"special_model_types.md\",\n  \"running_existing_model.md\",\n  \"advanced_usage.md\",\n  \"input_data.md\",\n  \"frbny_data.md\",\n  \"implementation_details.md\",\n  \"solving.md\",\n  \"estimation.md\",\n  \"forecast.md\",\n  \"irf.md\",\n  \"means_bands.md\",\n  \"altpolicy.md\",\n  \"scenarios.md\",\n  \"forecast_decomposition.md\",\n  \"plotting.md\",\n  \"algorithms.md\",\n  \"contributing.md\",\n  \"MatlabToJuliaTransition.md\",\n  \"julia_forecasting.md\",\n  \"license.md\"\n]","category":"page"},{"location":"#Acknowledgments-1","page":"Home","title":"Acknowledgments","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Developers of this package at the New York Fed include","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Michael Cai\nWilliam Chen\nAbhi Gupta\nPearl Li\nEthan Matlin\nErica Moszkowski\nReca Sarfati\nMicah Smith","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Contributors to this package at QuantEcon include","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Zac Cranko\nSpencer Lyon\nPablo Winant","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The gensys and csminwel routines DSGE.gensys and DSGE.csminwel are based on routines originally copyright Chris Sims. The files are released here with permission of Chris Sims under the BSD-3 License.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The kalman_filter routine is loosely based on a version of the Kalman filter algorithm originally copyright Federal Reserve Bank of Atlanta and written by Iskander Karibzhanov. The files are released here with permission of the Federal Reserve Bank of Atlanta under the BSD-3 License.","category":"page"},{"location":"model_design/#Model-Design-1","page":"Model Design","title":"Model Design","text":"","category":"section"},{"location":"model_design/#","page":"Model Design","title":"Model Design","text":"DSGE.jl* is an object-oriented approach to solving the New York Fed DSGE model that takes advantage of Julia's type system, multiple dispatch, package-handling mechanism, and other features. A single model object[1] centralizes  all information about the model's parameters, states, equilibrium conditions, and settings in a single data structure. The model object also keeps track of file locations for all I/O operations.","category":"page"},{"location":"model_design/#","page":"Model Design","title":"Model Design","text":"The following objects define a model:","category":"page"},{"location":"model_design/#","page":"Model Design","title":"Model Design","text":"Parameters: Have values, bounds, fixed-or-not status, priors. An instance of the AbstractParameter type houses all information about a given parameter in a single data structure. See  The AbstractParameter Type in the documentation for ModelConstructors.jl*.\nModel Indices: Mappings of state, shock, observable, and pseudo-observable names to indices (e.g. y_t -> 1). See Defining Indices.\nObservables and PseudoObservables: Mapping of names to indices, as well as information necessary for transformations. See The Observable and PseudoObservable Types in the documentation for ModelConstructors.jl*.\nEquilibrium Conditions: A function that takes parameters and model indices, then returns Γ0, Γ1, C, Ψ, and Π (which fully describe the model in canonical form).\nMeasurement Equation: A function mapping states to observables.\nPseudo-Measurement Equation: A function mapping states to what we call \"pseudo-observables\", i.e. linear combinations of existing states which are not observed. (Note that this is not strictly required to implement a model, but we often use the pseudo-measurement equation instead of adding new states in order to achieve a more parsimonious model.)","category":"page"},{"location":"model_design/#","page":"Model Design","title":"Model Design","text":"These are enough to define the model structure. Everything else is essentially a function of these basics, and we can solve the model and forecast observables via the following chain:","category":"page"},{"location":"model_design/#","page":"Model Design","title":"Model Design","text":"Parameters + Model Indices + Equilibrium conditions -> Transition matrices in state-space form\nTransition matrices + Data -> Estimated parameter values\nEstimated parameters + Transition matrices + Data -> Forecast","category":"page"},{"location":"model_design/#","page":"Model Design","title":"Model Design","text":"[1]: As of v0.7.3, DSGE.jl no longer houses the code for creating a                   model object. We have created a separate package                   ModelConstructors.jl*,                   which defines a bare-bones AbstractModel type                   for a generic model a user might want to estimate. In DSGE.jl,                   we now define a subtype AbstractDSGEModel that includes additional                   methods, defaults, etc. that a standard DSGE model would have.","category":"page"},{"location":"special_model_types/#Special-Model-Types-1","page":"Special Model Types","title":"Special Model Types","text":"","category":"section"},{"location":"special_model_types/#","page":"Special Model Types","title":"Special Model Types","text":"CurrentModule = DSGE","category":"page"},{"location":"special_model_types/#","page":"Special Model Types","title":"Special Model Types","text":"In addition to the AbstractDSGEModel type, DSGE.jl* has several special types that implement models which are designed to interface with DSGEs.","category":"page"},{"location":"special_model_types/#The-PoolModel-Type-1","page":"Special Model Types","title":"The PoolModel Type","text":"","category":"section"},{"location":"special_model_types/#","page":"Special Model Types","title":"Special Model Types","text":"Unlike the other models contained in DSGE, the PoolModel type is not a proper DSGE model. It is a wrapper object for different methods to perform model averaging for two different models, which do not have to be DSGE models. For example, a user could average two different vector auto-regressions. Generally, a user only needs to provide the predictive density scores of the two models that the user wants to average. The reason is that we treat the predictive density scores as non-FRED observables. This approach makes interfacing with the rest of the machinery provided by DSGE.jl* very simple.","category":"page"},{"location":"special_model_types/#","page":"Special Model Types","title":"Special Model Types","text":"PoolModel","category":"page"},{"location":"special_model_types/#DSGE.PoolModel","page":"Special Model Types","title":"DSGE.PoolModel","text":"PoolModel{T}\n\nImplements several model averaging methods to pool the predictions of different models. Currently, PoolModel only works for pooling two different models, although it can be extended to more models. Confer with the paper for details.\n\nThe available methods are\n\ndynamic: weights on pooled models evolve over time and are          treated as a stochastic process.\nstatic:  weights on pooled models are assumed time-invariant.\nequal:   weights are set to 1/2.\nbma:     weights are updated according to Bayesian model averaging.\n\nThe default method is dynamic, which implements the Dynamic Pools method developed by Del Negro et al. (2016). To choose another method, use the keyword weight_type::Symbol, e.g.\n\npm = PoolModel(weight_type = :static) # creates a static PoolModel\n\nThe weight_type is stored as a setting, so users can retrieve at any point by using get_setting.\n\nFields\n\nParameters\n\nparameters::Vector{AbstractParameter}: Vector of all time-invariant hyperparameters   for the chosen method of model averaging.\nkeys::OrderedDict{Symbol,Int}: Maps human-readable names for predictive densities   of pooled models.\n\nInputs to Measurement and Equilibrium Condition Equations\n\nmodel::OrderedDict{Symbol,AbstractDSGEModel}: Maps name to its underlying model object.\n\nModel Specifications and Settings\n\nspec::String: The model specification identifier, \"an_schorfheide\", cached here for filepath computation.\nsubspec::String: The model subspecification number, indicating that some parameters from the original model spec (\"ss0\") are initialized differently. Cached here for filepath computation.\nsettings::Dict{Symbol,Setting}: Settings/flags that affect computation without changing the economic or mathematical setup of the model.\ntest_settings::Dict{Symbol,Setting}: Settings/flags for testing mode\n\nOther Fields\n\nrng::MersenneTwister: Random number generator. Can be is seeded to ensure reproducibility in algorithms that involve randomness (such as Metropolis-Hastings).\ntesting::Bool: Indicates whether the model is in testing mode. If true, settings from m.test_settings are used in place of those in m.settings.\nobservable_mappings::OrderedDict{Symbol,Observable}: A dictionary that stores data sources, series mnemonics, and transformations to/from model units. DSGE.jl will fetch data from the Federal Reserve Bank of St. Louis's FRED database; all other data must be downloaded by the user. See load_data and Observable for further details.\n\n\n\n\n\n","category":"type"},{"location":"special_model_types/#","page":"Special Model Types","title":"Special Model Types","text":"See Del Negro et al. (2016) for theoretical details on the model averaging methods listed in the documentation.","category":"page"},{"location":"special_model_types/#","page":"Special Model Types","title":"Special Model Types","text":"To facilitate analysis with the PoolModel type, we also provide the following functions.","category":"page"},{"location":"special_model_types/#","page":"Special Model Types","title":"Special Model Types","text":"estimate_bma\nsample_λ\npropagate_λ\ncompute_Eλ","category":"page"},{"location":"special_model_types/#DSGE.estimate_bma","page":"Special Model Types","title":"DSGE.estimate_bma","text":"estimate_bma(m, data, prior; return_output = false, filestring_addl = [])\n\nEstimate a Bayesian Model Average.\n\nArguments:\n\nm::PoolModel: PoolModel object\n\nOptional Arguments:\n\ndata: well-formed data as Matrix or DataFrame. If this is not provided, the load_data routine will be executed.\nprior::Float64: prior probability between the two models\n\nKeyword Arguments:\n\nreturn_output::Bool: option to return output. If false, nothing is returned.\nfilestring_addl::Vector{String}: Additional strings to append to output files.\n\n\n\n\n\n","category":"function"},{"location":"special_model_types/#DSGE.sample_λ","page":"Special Model Types","title":"DSGE.sample_λ","text":"sample_λ(m, pred_dens, θs, T = -1; parallel = true) where S<:AbstractFloat\nsample_λ(m, pred_dens, T = -1; parallel = true) where S<:AbstractFloat\n\nComputes and samples from the conditional density p(λt|θ, It, P) for particle in θs, which represents the posterior distribution. The sampled λ particles represent the posterior distribution p(λ{t|t} | It, P).\n\nIf no posterior distribution is passed in, then the function computes the distribution of λ_{t|t} for a static pool.\n\nInputs\n\nm::PoolModel{S}: PoolModel object\npred_dens::Matrix{S}: matrix of predictive densities\nθs::Matrix{S}: matrix of particles representing posterior distribution of θ\nT::Int64: final period for tempered particle filter\n\nwhere S<:AbstractFloat.\n\nKeyword Argument\n\nparallel::Bool: use parallel computing to compute and sample draws of λ\n\nOutputs\n\nλ_sample::Vector{Float64}: sample of draws of λs; together with (θ,λ) represents a joint density\n\n```\n\n\n\n\n\n","category":"function"},{"location":"special_model_types/#DSGE.propagate_λ","page":"Special Model Types","title":"DSGE.propagate_λ","text":"propgate_λ(λvec, h, m, θvec) where T<:AbstractFloat\n\nPropagates a λ particle h periods forward.\n\nInputs\n\nλ::T: λ sample from (θ,λ) joint distribution\nh::Int64: forecast horizon\nm::PoolModel: PoolModel object\nθvec::Vector{T}: optional vector of parameters to update PoolModel\n\n```\n\n\n\n\n\n","category":"function"},{"location":"special_model_types/#DSGE.compute_Eλ","page":"Special Model Types","title":"DSGE.compute_Eλ","text":"compute_Eλ(m, h, λvec, θmat = [], weights = [];\n    current_period = true, parallel = true) where T<:AbstractFloat\n\nComputes and samples from the conditional density p(λt|θ, It, P) for particle in θs, which represents the posterior distribution.\n\nInputs\n\nm::PoolModel{T}: PoolModel object\nh::Int64: forecast horizon\nλvec::Vector{T}: vector of particles of λ samples from (θ,λ) joint distribution\n`θmat::Matrix{T}': matrix of posterior parameter samples\nweights::Vector{T}: weights of λ particles, defaults to equal weights\n\nKeyword Argument\n\ncurrent_period::Bool: compute Eλ for current period t\nparallel::Bool: use parallel computing to compute and sample λ\nget_dpp_pred_dens::Bool: compute predictive densities according to dynamic prediction pools\n\nOutputs\n\nλhat_tplush::Float64: E[λ{t+h|t} | It^P, P]\nλhat_t::Float64: E[λ{t|t} | It^P, P]\n\n```\n\n\n\n\n\n","category":"function"},{"location":"special_model_types/#DSGE-VARs-and-the-DSGEVAR-Type-1","page":"Special Model Types","title":"DSGE-VARs and the DSGEVAR Type","text":"","category":"section"},{"location":"special_model_types/#","page":"Special Model Types","title":"Special Model Types","text":"We can approximate the dynamics of a linearized DSGE with a VAR(p), where p is the number of lags. This approximation gives a mapping from the parameters of a DSGE to the parameters of a VAR (the coefficients and innovations variance-covariance matrix). Since the number of parameters in a VAR are generally larger than the number of parameters in a DSGE, this mapping can be interpreted as cross-restrictions imposed by a DSGE on the parameters of a VAR. A DSGE-VAR combines a DSGE with a VAR to, among other reasons, evaluate the mis-specification of the DSGE and improve the DSGE's forecasting performance.","category":"page"},{"location":"special_model_types/#","page":"Special Model Types","title":"Special Model Types","text":"For more details on the theory, see Del Negro and Schorfheide (2004), Del Negro and Schorfheide (2006), and Del Negro and Schorfheide (2009).","category":"page"},{"location":"special_model_types/#","page":"Special Model Types","title":"Special Model Types","text":"We implement DSGE-VARs with the DSGEVAR concrete type so that it is easy to interface them with DSGE models. A DSGEVAR type holds information about the VAR with which we want to combine a given DSGE model and can be easily constructed, given a DSGE object. Once we have constructed a DSGEVAR object, then it is straightoforward to estimate the object and compute impulse responses.","category":"page"},{"location":"special_model_types/#","page":"Special Model Types","title":"Special Model Types","text":"DSGEVAR","category":"page"},{"location":"special_model_types/#DSGE.DSGEVAR","page":"Special Model Types","title":"DSGE.DSGEVAR","text":"DSGEVAR{T} <: AbstractDSGEVARModel{T}\n\nimplements a simple interface for combining a given DSGE model with a VAR to create a DSGE-VAR. Confer with Del Negro and Schorfheide (2004), Del Negro and Schorfheide (2006), and Del Negro and Schorfheide (2009) for details about DSGE-VARs.\n\nThe recommended constructor requires the user to provide (1) an AbstractDSGEModel object, (2) which structural shocks from the DSGE to use, and (3) the subspec (optional, defaults to \"ss0\"). If the subspec \"ss0\" is used, then the result is a DSGEVAR whose VAR component is \"empty\" in that the observables, lags, and λ weight are not specified. The reason why this constructor requires the user to specify which structural shocks of DSGE to use is that this information is DSGE-specific rather than information about the VAR.\n\nHowever, we can also construct a DSGEVAR without having to specify the structural shocks when calling the constructor, although we still need to give an instance of an AbstractDSGEModel.\n\nExample\n\nThe code below instantiates an empty DSGEVAR with AnSchorfheide as the underlying DSGE and then calls update! on the empty DSGE-VAR to add information about the desired DSGE-VAR spec.\n\ndsgevar = DSGEVAR(AnSchorfheide())\nDSGE.update!(dsgevar, shocks = [:rm_sh, :z_sh, :g_sh],\n    observables = [:obs_gdp, :obs_cpi, :obs_nominalrate],\n    λ = 1., lags = 4)\n\nFields\n\nDSGE object\n\ndsge::AbstractDSGEModel{T}: underlying DSGE model object\n\nDSGE-VAR Information\n\nobservables::OrderedDict{Symbol,Int}: dictionary mapping observables   of the VAR to their index in the matrices representing the DSGE-VAR\nshocks::OrderedDict{Symbol,Int}: dictionary mapping structural   shocks in the DSGE to their index in the matrices representing the DSGE-VAR\nlags::Int: number of lags in the VAR\nλ::T: weight on the DSGE prior\n\nAuxiliary Information\n\nspec::String: concatenates dsgevar with the spec of the DSGE, e.g.   for AnSchorfheide, we have dsgevar_an_schorfheide.\nsubspec::String: specifies the model subspecification.   Cached here for filepath computation.\ntesting::Bool: indicates whether the model is in testing mode.   Currently, this setting has no uses in practice\n\n\n\n\n\n","category":"type"},{"location":"special_model_types/#Tips-for-Using-DSGEVARs-1","page":"Special Model Types","title":"Tips for Using DSGEVARs","text":"","category":"section"},{"location":"special_model_types/#","page":"Special Model Types","title":"Special Model Types","text":"When extensively using DSGE-VARs, we recommend defining your own subspecs in subspecs.jl because it simplifies the process of saving, reading, and analyzing output from estimating and calculating impulse responses for DSGE-VARs. See Advanced Usage for a more detailed explanation on changing subspecs.\nThe names of the observables must exist as either observables or pseudo-observables in the DSGE because for most DSGEVAR methods, we need to construct the state space representation of the DSGEVAR using information from the underlying DSGE.\nIt is important to be careful about the order of the observables when constructing a DSGEVAR. Whether you define the names of the observables by calling update! or by creating a subspec, we assume that the order of the observables corresponds to the observable's index in the data and in the state space representation of the DSGEVAR. In the example provided above, if we estimate the DSGEVAR on data or construct the state space representation of the DSGEVAR, we assume that the order of observables in the data array, which has dimensions nobs x nperiods, is :obs_gdp in the first row, :obs_cpi in the second row, and :obs_nominalrate in the third row.\nWhen using functions that use the DSGE as a prior for a VAR (as opposed to a VAR approximation of the DSGE), then an intercept term is assumed and cannot be turned off. For example, the following two functions computes the VAR coefficients and innovations variance-covariance matrix for a DSGEVAR object m. The first one is for a VAR approximation of the DSGE in m, and it allows the user to specify whether or not they want an intercept term using the keyword use_intercept. The second function is for using the DSGE as a prior for a VAR estimated on data. This function does not have the use_intercept keyword because we require an intercept term when using the DSGE as a prior for a VAR.","category":"page"},{"location":"special_model_types/#","page":"Special Model Types","title":"Special Model Types","text":"compute_system(m; apply_altpolicy = false,\n               regime_switching = false, n_regimes = 2,\n               check_system = false, get_system = false,\n               get_population_moments = false, use_intercept = false,\n               verbose = :high)\ncompute_system(m, data; apply_altpolicy = false,\n               regime_switching = false, n_regimes = 2,\n               check_system = false, get_system = false,\n               get_population_moments = false,\n               verbose = :high)","category":"page"},{"location":"running_existing_model/#Running-an-Existing-Model-1","page":"Running An Existing Model","title":"Running an Existing Model","text":"","category":"section"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"The DSGE.jl package provides several example models:","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"A simple three-equation DSGE model from An and Schorfheide (2006)\nThe well-known Smets and Wouters (2007) model\nThe New York Fed DSGE model (version 990.2), which was introduced in this blog post\nThe New York Fed DSGE model (version 1002.9), which is documented here\nThe New York Fed DSGE model (version 1010.18)\nSeveral methods for model averaging over two models from Del Negro et al. (2016) through the concrete type PoolModel.","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"You can run these models using the description provided here. If you were to implement another model using DSGE.jl, these procedures can also be used to estimate those models.","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"Please see examples/ on the GitHub or the equivalent folder inside your Julia packages directory for example scripts we have created.","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"run_default.jl: a simple example of the standard workflow with DSGE.jl*\nrun_dsgevar.jl: a example showing how to use a DSGEVAR.\nmake_packet.jl: basic auto-generation of a packet of plots and figures which help the user analyze estimation, forecast, and impulse response results. This script also provides an example of how we recommend structuring \"master\" files that launch a forecast and generate results with one \"click.\"\ntest_smc.jl: using SMC to estimate DSGE models.","category":"page"},{"location":"running_existing_model/#Running-with-Default-Settings-1","page":"Running An Existing Model","title":"Running with Default Settings","text":"","category":"section"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"To estimate and forecast in Julia, simply create an instance of the model object and call estimate and forecast_all. A minimal example is reproduced below.","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"# estimate as of 2015-Q3 using the default data vintage from 2015 Nov 27\ncustom_settings = Dict{Symbol, Setting}(\n    :data_vintage        => Setting(:data_vintage, \"151127\"),\n    :date_forecast_start => Setting(:date_forecast_start, quartertodate(\"2015-Q4\")))\n\n# construct a model object\nm = Model990(custom_settings = custom_settings)\n\n# reoptimize parameter vector, compute Hessian at mode, and full posterior\n# parameter sampling\nestimate(m)\n\n# produce LaTeX tables of parameter moments\ncompute_moments(m)\n\n# forecast and compute means and bands using 10 processes\nmy_procs = addprocs(10)\n@everywhere using DSGE\n\nforecast_one(m, :full, :none, [:forecaststates, :forecastobs])\ncompute_meansbands(m, :full, :none, [:forecaststates, :forecastobs])\nrmprocs(my_procs)","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"For more details on changing the model's default settings, parameters, equilibrium conditions, etc., see Advanced Usage.","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"By default, the estimate routine loads the dataset, reoptimizes the initial parameter vector, computes the Hessian at the mode, and conducts full posterior parameter sampling using Metropolis-Hastings. (The initial parameter vector used is specified in the model's constructor.) Further options for estimation are described in Estimation:","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"To use updated data or alternative user-specified datasets, see Input Data.\nThe user may want to avoid reoptimizing the parameter vector and calculating the Hessian matrix at this new vector. Please see Reoptimizing.","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"For more information on the many types of forecasts that can be run on an existing or user-defined model, see Forecasting.","category":"page"},{"location":"running_existing_model/#Input/Output-Directory-Structure-1","page":"Running An Existing Model","title":"Input/Output Directory Structure","text":"","category":"section"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"The DSGE.jl* estimation uses data files as input and produces large data files as outputs. One estimation saves several GB of parameter draws and related outputs. It is useful to understand how these files are loaded/saved and how to control this behavior.","category":"page"},{"location":"running_existing_model/#Directory-Tree-1","page":"Running An Existing Model","title":"Directory Tree","text":"","category":"section"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"The following subdirectory tree indicates the default locations of these input and outputs. Square brackets indicate directories in the tree that will become relevant as future features are implemented.","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"Note that this directory tree is not linked, although it appears to be.","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"Pages = [\"io_dirtree.md\"]\nDepth = 5","category":"page"},{"location":"running_existing_model/#Directory-Paths-1","page":"Running An Existing Model","title":"Directory Paths","text":"","category":"section"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"By default, input/output directories are located in the DSGE.jl* package, along with the source code. Default values of the input/output directory roots:","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"saveroot(m): \"$(Pkg.dir())/DSGE/save\"\ndataroot(m): \"$(Pkg.dir())/DSGE/save/input_data\"","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"Note these locations can be overridden as desired. See Advanced Usage for more details.","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"m <= Setting(:saveroot, \"path/to/my/save/root\")\nm <= Setting(:dataroot, \"path/to/my/data/root\")","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"Utility functions are provided to create paths to input/output files. These should be used for best results.","category":"page"},{"location":"running_existing_model/#","page":"Running An Existing Model","title":"Running An Existing Model","text":"DSGE.inpath\nDSGE.rawpath\nDSGE.logpath\nDSGE.workpath\nDSGE.tablespath\nDSGE.figurespath","category":"page"},{"location":"advanced_usage/#advanced-usage-1","page":"Advanced Usage","title":"Advanced Usage","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"CurrentModule = DSGE","category":"page"},{"location":"advanced_usage/#Package-Directory-Structure-1","page":"Advanced Usage","title":"Package Directory Structure","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"The package directory structure follows Julia module conventions. Directories in square brackets indicate future additions. Note that this directory tree is not linked, although it appears to be.","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"Pages = [\"pkg_structure.md\"]\nDepth = 5","category":"page"},{"location":"advanced_usage/#Working-with-Settings-1","page":"Advanced Usage","title":"Working with Settings","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"There are many computational settings that affect how the code runs without affecting the mathematical definition of the model. While the default settings loaded are intended to be comprehensive rather than the minimal number of settings, users will generally want to check that these three settings are properly chosen:","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"saveroot::String: The root directory for model output.\ndataroot::String: The root directory for model input data.\ndata_vintage::String: Data vintage, formatted yymmdd. By default, data_vintage is set to today's date. It is (currently) the only setting printed to output filenames by default.","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"Many functions in DSGE.jl* will either require input data or create output data, so it is important to check that the saveroot and dataroot are set as the user intends. Setting the data vintage is also useful for reproducibility. Economic data like GDP are frequently revised, which can pose issues for reproducing results. Setting the data vintage allows users to guarantee the correct vintage of data is used when generating results. By default, the data vintage is set to the current date, so a user will need to manually set the data vintage to the desired date.","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"Below, we describe several important settings for package usage.","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"For more details on implementation and usage of settings, see ModelConstructors.jl*.","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"See defaults.jl for the complete description of default settings.","category":"page"},{"location":"advanced_usage/#General-1","page":"Advanced Usage","title":"General","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"saveroot::String: The root directory for model output.\nuse_parallel_workers::Bool: Use available parallel workers in computations.\nn_anticipated_shocks: Number of anticipated policy shocks.","category":"page"},{"location":"advanced_usage/#Data-and-I/O-1","page":"Advanced Usage","title":"Data and I/O","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"dataroot::String: The root directory for model input data.\ndata_vintage::String: Data vintage, formatted yymmdd. By default, data_vintage is set to today's date. It is (currently) the only setting printed to output filenames by default.\ndataset_id::Int: Dataset identifier. There should be a unique dataset ID for each set of observables.\ncond_vintage::String: Conditional data vintage, formatted yymmdd.\ncond_id::Int: Conditional dataset identifier. There should be a unique conditional dataset ID for each set of input, raw data mnemonics (not observables!).\ncond_semi_names::Vector{Symbol} and cond_full_names::Vector{Symbol}: names of observables for which we want to use semi- and full conditional data. All other observables are NaNed out in the conditional data periods.\npopulation_mnemonic::Nullable{Symbol}: population series mnemonic in form Nullable(:<mnemonic>__<source>) (for example, Nullable(:CNP16OV__FRED)), or Nullable{Symbol}() if the model doesn't use population data","category":"page"},{"location":"advanced_usage/#Dates-1","page":"Advanced Usage","title":"Dates","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"date_presample_start::Date: Start date of pre-sample.\ndate_mainsample_start::Date: Start date of main sample.\ndate_zlb_start::Date: Start date of zero lower bound regime.\ndate_zlb_end::Date: End date of zero lower bound regime.\ndate_forecast_start::Date: Start date of forecast period (or the period after the last period for which we have GDP data).\ndate_forecast_end::Date: End date of forecast, i.e. how far into the future to forecast.\ndate_conditional_end::Date: Last date for which we have conditional data. This is typically the same as date_forecast_start when we condition on nowcasts and current quarter financial data.","category":"page"},{"location":"advanced_usage/#Estimation-1","page":"Advanced Usage","title":"Estimation","text":"","category":"section"},{"location":"advanced_usage/#Metropolis-Hastings-Settings-1","page":"Advanced Usage","title":"Metropolis-Hastings Settings","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"reoptimize::Bool: Whether to reoptimize the posterior mode. If true (the   default), estimate begins reoptimizing from the model object's parameter   vector.  See Optimizing or Reoptimizing for   more details.\ncalculate_hessian::Bool: Whether to compute the Hessian. If true (the   default), estimate calculates the Hessian at the posterior mode.\nn_mh_simulations::Int: Number of draws from the posterior distribution per block.\nn_mh_blocks::Int: Number of blocks to run Metropolis-Hastings.\nn_mh_burn::Int: Number of blocks to discard as burn-in for Metropolis-Hastings.\nmh_thin::Int: Metropolis-Hastings thinning step.\nparallel::Bool: Flag for running algorithm in parallel.\nn_parts::Int: Number of particles.\nn_blocks::Int: Number of parameter blocks in mutation step.\nn_mh_steps::Int: Number of Metropolis Hastings steps to attempt during the mutation step.","category":"page"},{"location":"advanced_usage/#Sequential-Monte-Carlo-Settings-1","page":"Advanced Usage","title":"Sequential Monte Carlo Settings","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"λ::S: The 'bending coefficient' λ in Φ(n) = (n/N(Φ))^λ\nn_Φ::Int: Number of stages in the tempering schedule.\nresampling_method::Symbol: Which resampling method to use.\n:systematic: Will use sytematic resampling.\n:multinomial: Will use multinomial resampling.\n:polyalgo: Samples using a polyalgorithm.\nthreshold_ratio::S: Threshold s.t. particles will be resampled when the population   drops below threshold * N\nc::S: Scaling factor for covariance of the particles. Controls size of steps in mutation step.\nα::S: The mixture proportion for the mutation step's proposal distribution.\ntarget::S: The initial target acceptance rate for new particles during mutation.\nuse_chand_recursion::Bool: Flag for using Chandrasekhar Recursions in Kalman filter.\nuse_fixed_schedule::Bool: Flag for whether or not to use a fixed tempering (ϕ) schedule.\ntempering_target::S: Coefficient of the sample size metric to be targeted when solving   for an endogenous ϕ.\nold_data::Matrix{S}:\nold_cloud::ParticleCloud:\nold_vintage::String: String for vintage date of old data\nsmc_iteration::Int: The iteration index for the number of times SMC has been run on the    same data vintage. Primarily for numerical accuracy/testing purposes.\nrun_test::Bool: Flag for when testing accuracy of program\nfilestring_addl::Vector{String}: Additional file string extension for loading old cloud.\ncontinue_intermediate::Bool: Flag to indicate whether one is continuing SMC from an   intermediate stage/\nintermediate_stage_start::Int: Intermediate stage at which one wishes to begin the estimation.\nsave_intermediate::Bool: Flag for whether one wants to save intermediate Cloud objects\nintermediate_stage_increment::Int: Save Clouds at every increment   (1 = each stage, 10 = every 10th stage, etc.)","category":"page"},{"location":"advanced_usage/#Forecasting-1","page":"Advanced Usage","title":"Forecasting","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"forecast_jstep::Int: Forecast thinning step.\nforecast_block_size::Int: Number of draws in each forecast block before thinning by forecast_jstep.\nforecast_input_file_overrides::Dict{Symbol, String}: Maps input_type(s) to the file name containing input draws for that type of forecast. See Forecasting.\nforecast_horizons::Int: Number of periods to forecast.\nimpulse_response_horizons::Int: Number of periods for which to calculate IRFs.","category":"page"},{"location":"advanced_usage/#Alternative-Policy-1","page":"Advanced Usage","title":"Alternative Policy","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"alternative_policy::AltPolicy: See Alternative Policies.","category":"page"},{"location":"advanced_usage/#Accessing-Settings-1","page":"Advanced Usage","title":"Accessing Settings","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"The function get_setting(m::AbstractModel, s::Symbol) returns the value of the setting s in m.settings. Some settings also have explicit getter methods that take only the model object m as an argument. Note that not all are exported.","category":"page"},{"location":"advanced_usage/#Overwriting-Default-Settings-1","page":"Advanced Usage","title":"Overwriting Default Settings","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"To overwrite default settings added during model construction, a user must create a Dict{Symbol, Setting} and pass that into the model constructor as the keyword argument custom_settings. If the print, code, and description fields of the new Setting object are not provided, the fields of the existing setting will be maintained. If new values for print, code, and description are specified, and if these new values are distinct from the defaults for those fields, the fields of the existing setting will be updated.","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"For example, overwriting use_parallel_workers should look like this:","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"custom_settings = Dict{Symbol, Setting}(\n    :use_parallel_workers => Setting(:use_parallel_workers, true))\nm = Model990(custom_settings = custom_settings)","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"Or like this:","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"m = Model990()\nm <= Setting(:use_parallel_workers, true)","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"Note that using this second method will not work for all settings, e.g. n_anticipated_shocks is a setting that must be passed into the model during construction, as in the first example.","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"By default, passing in custom_settings overwrites the entries in the model object's settings field. However, with the additional keyword argument testing = true, it will overwrite the entries in test_settings:","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"m = Model990(custom_settings = custom_settings, testing = true)","category":"page"},{"location":"advanced_usage/#editing-extending-model-1","page":"Advanced Usage","title":"Editing or Extending a Model","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"Users may want to extend or edit Model990 in a number of different ways.  The most common changes are listed below, in decreasing order of complexity:","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"Add new parameters\nModify equilibrium conditions or measurement equations\nChange the values of various parameter fields (i.e. initial value, prior, transform, etc.)\nChange the values of various computational settings (i.e. reoptimize, n_mh_blocks)","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"Points 1 and 2 often go together (adding a new parameter guarantees a change in equilibrium conditions), and are such fundamental changes that they increment the model specification number and require the definition of a new subtype of AbstractModel (for instance, Model991).  See Model specification for more details.","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"Any changes to the initialization of preexisting parameters are defined as a new model sub-specification, or subspec. While less significant than a change to the model's equilibrium conditions, changing the values of some parameter fields (especially priors) can have economic significance over and above settings we use for computational purposes. Parameter definitions should not be modified in the model object's constructor. First, incrementing the model's sub-specification number when parameters are changed improves model-level (as opposed to code-level) version control. Second, it avoids potential output filename collisions, preventing the user from overwriting output from previous estimations with the original parameters. The protocol for defining new sub-specifications is described in Model sub-specifications.","category":"page"},{"location":"advanced_usage/#model-specification-mspec-1","page":"Advanced Usage","title":"Model specification (m.spec)","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"A particular model, which corresponds to a subtype of AbstractModel, is defined as a set of parameters, equilibrium conditions (defined by the eqcond function) and measurement equations (defined by the measurement function).  Therefore, the addition of new parameters, states, or observables, or any changes to the equilibrium conditions or measurement equations necessitate the creation of a new subtype of AbstractModel.","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"To create a new model object, we recommend doing the following:","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"Duplicate the m990 directory within the models directory. Name the new directory mXXX.jl, where XXX is your chosen model specification number or string. Rename m990.jl in this directory to mXXX.jl.\nIn the mXXX/ directory, change all references to Model990 to ModelXXX.\nEdit the m990.jl, eqcond.jl, and measurement.jl files as you see fit.  If adding new states, equilibrium conditions, shocks, or observables, be sure to add them to the appropriate list in init_model_indices.\nOpen the module file, src/DSGE.jl. Add ModelXXX to the list of functions to export, and include each of the files in src/model/mXXX.","category":"page"},{"location":"advanced_usage/#model-sub-specifications-msubspec-1","page":"Advanced Usage","title":"Model sub-specifications (m.subspec)","text":"","category":"section"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"Model990 sub-specifications are initialized by overwriting initial parameter definitions before the model object is fully constructed. This happens via a call to init_subspec in the Model990 constructor. (Clearly, an identical protocol should be followed for new model types as well.)","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"To create a new sub-specification (e.g., subspec 1) of Model990, edit the file src/models/subspecs.jl as follows (note that this example is not actually sub-specification 1 of Model990. In the source code, our sub-specification 5 is provided as additional example.):","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"Step 1. Define a new function, ss1, that takes an object of type Model990 (not    AbstractModel!) as an argument. In this function, construct new parameter objects and    overwrite existing model parameters using the <= syntax. For example,","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"function ss1(m::Model990)\n    m <= parameter(:ι_w, 0.000, (0.0, .9999), (0.0,0.9999), DSGE.Untransformed(), Normal(0.0,1.0), fixed=false,\n                   description=\"ι_w: Some description.\",\n                   tex_label=\"\\\\iota_w\")\n    m <= parameter(:ι_p, 0.0, fixed=true,\n                   description= \"ι_p: Some description\"\n                   tex_label=\"\\\\iota_p\")\nend","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"Step 2. Add an elseif condition to init_subspec:","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"    ...\n    elseif subspec(m) == \"ss1\"\n        return ss1(m)\n    ...","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"To construct an instance of Model990, ss1, call the constructor for Model990 with ss1 as an argument. For example,","category":"page"},{"location":"advanced_usage/#","page":"Advanced Usage","title":"Advanced Usage","text":"m = Model990(\"ss1\")","category":"page"},{"location":"input_data/#input-data-step-1","page":"Input Data","title":"Input Data","text":"","category":"section"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"CurrentModule = DSGE","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Given all of the hard work put into specifying the model, one should be able to maintain the input data painlessly. To that extent, DSGE.jl* provides facilities to download appropriate vintages of data series from FRED (Federal Reserve Economic Data).","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Note that a sample input dataset for use with model m990 is provided; see New York Fed Model 990 Data for more details. To update this sample dataset for use with model m990, see Update sample input data.","category":"page"},{"location":"input_data/#Setup-1","page":"Input Data","title":"Setup","text":"","category":"section"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"To take advantage of the ability to automatically download data series from FRED via the FredData.jl package, set up your FRED API access by following the directions here.","category":"page"},{"location":"input_data/#Loading-data-1","page":"Input Data","title":"Loading data","text":"","category":"section"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"At the most basic, loading data looks like this:","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"m = Model990()\ndf = load_data(m)","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"By default, load_data will look on the disk first to see if an appropriate vintage of data is already present. If data on disk are not present, or if the data are invalid for any reason, a fresh vintage will be downloaded from FRED and merged with the other data sources specified. See load_data for more details.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"The resulting DataFrame df contains all the required data series for this model, fully transformed. The first row is given by the Setting date_presample_start and the last row is given by date_mainsample_end. The first n_presample_periods rows of df are the presample.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Driver functions including estimate accept this df as an argument and convert it into a Matrix suitable for computations using df_to_matrix, which sorts the data, ensures the full sample is present, discards the date column, and sorts the observable columns according to the observables field of the model object.","category":"page"},{"location":"input_data/#Non-FRED-data-sources-1","page":"Input Data","title":"Non-FRED data sources","text":"","category":"section"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Some data series may not be available from FRED or one may simply wish to use a different data source, for whatever reason. The data sources and series are specified in the data_series field of the model object. For each data source that is not :fred, a well-formed CSV of the form <source>_<yymmdd>.csv is expected in the directory indicated by inpath(m, \"raw\").  For example, the following might be the contents of a data source for two series :series1 and :series2:","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"date,series1,series2\n1959-06-30,1.0,NaN\n1959-09-30,1.1,0.5\n# etc.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Note that quarters are represented by the date of the last day of the quarter and missing values are specified by NaN.","category":"page"},{"location":"input_data/#Example-1","page":"Input Data","title":"Example","text":"","category":"section"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Let's consider an example dataset comprised of 10 macro series sourced from FRED and one survey-based series sourced from, say, the Philadelphia Fed's Survey of Professional Forecasters via Haver Analytics:","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"julia> m.data_series\nDict{Symbol,Array{Symbol,1}} with 2 entries:\n :spf   => [:ASACX10]\n :fred  => [:GDP, :PCE, ...] # etc","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"If the data vintage specified for the model is 151127 (Nov. 27, 2015), then the following files are expected in inpath(m, \"raw\"):","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"spf_151127.csv\nfred_151127.csv","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"The FRED series will be downloaded and the fred_151127.csv file will be automatically generated, but the spf_151127.csv file must be manually compiled as shown above:","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"date,ASACX10\n1991-12-31,4.0\n# etc.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Now, suppose that we set the data vintage to 151222, to incorporate the BEA's third estimate of GDP. The fred_151222.csv file will be downloaded, but there are no updates to the SPF dataset during this period. Regardless, the file spf_151222.csv must be present to match the data vintage. The solution in this case is to manually copy and rename the older SPF dataset. Although this is not an elegant approach, it is consistent with the concept of a vintage as the data available at a certain point in time –- in this example, it just so happens that the SPF data available on Nov. 27 and Dec. 22 are the same.","category":"page"},{"location":"input_data/#Incorporate-population-forecasts-1","page":"Input Data","title":"Incorporate population forecasts","text":"","category":"section"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Many variables enter the model in per-capita terms. To that extent, we use data on population levels to adjust aggregate variables into per-capita variables. Furthermore, we apply the Hodrick-Prescott filter (\"H-P filter\") to the population levels to smooth cyclical components.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"The user will ultimately want to produce forecasts of key variables such as GDP and then represent these forecasts in standard terms. That is, one wants to report GDP forecasts in aggregate terms, which is standard, rather than per-capita terms. To do this, we either extrapolate from the last periods of population growth in the data, or use external population forecasts.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Note that if external population forecasts are provided, non-forecast procedures, such as model estimation, are also affected because the H-P filter smoothes back from the latest observation.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"To incorporate population forecasts,","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Set the model setting use_population_forecast to true.\nProvide a file population_forecast_<yymmdd>.csv to inpath(m, \"raw\"). Population forecasts should be in levels, and represent the same series as given by the population_mnemonic setting (defaults to :CNP16OV, or \"Civilian Noninstitutional Population, Thousands\"). If your population forecast is in growth rates, convert it to levels yourself. The first row of data should correspond to the last period of the main sample, such that growth rates can be computed. As many additional rows of forecasts as desired can be provided.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"The file should look like this:","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"date,POPULATION\n2015-12-31,250000\n2016-03-31,251000\n# etc.","category":"page"},{"location":"input_data/#Dataset-creation-implementation-details-1","page":"Input Data","title":"Dataset creation implementation details","text":"","category":"section"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Let's quickly walk through the steps DSGE.jl* takes to create a suitable dataset.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"First, a user provides a detailed specification of the data series and transformations used for their model.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"the user specifies m.observables; the keys of this dictionary name   the series to be used in estimating the model.\nthe user specifies m.observable_mappings; the keys of this dictionary name observed variables, and the values correspond to the observable object, which contains information about the forward and reverse transforms as well as the input data series from which the observable is constructed.\nFor a given observable, an input series, e.g.   m.observable_mappings[:obs_gdp].input_series, is an array of mnemonics to be   accessed from the data source listed after the mnemonic (separated by the double   underscore).Note that these mnemonics do not correspond to observables one-to-one,   but rather are usually series in levels that will be further transformed.\nThere are also both forward and reverse transforms for a given observable,   e.g. m.observable_mappings[:obs_gdp].fwd_transform and   m.observable_mappings[:obs_gdp].rev_transform. The forward transform operates on a   single argument, levels, which is a DataFrame of the data in levels returned by the   function load_data_levels. The reverse transform operates on a forward transformed   series (which is in model units) transforming it into human-readable units, such   as one quarter percent changes or per-capita adjustments. Both transforms return a   DataArray for a single series. These functions could do nothing, or they could   perform a more complex transformation. See   Data Transforms and Utilities for more information about series-specific   transformations.\nthe user adjusts data-related settings, such as data_vintage, data_id,   dataroot, date_presample_start, date_zlb_start, date_forecast_start,   and use_population_forecast.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Second, DSGE.jl* attempts to construct the dataset given this setup through a call to load_data. See load_data for more details.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Intermediate data in levels are loaded. See load_data_levels for more details.\nTransformations are applied to the data in levels. See transform_data for more details.\nThe data are saved to disk. See save_data for more details.","category":"page"},{"location":"input_data/#Common-pitfalls-1","page":"Input Data","title":"Common pitfalls","text":"","category":"section"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Given the complexity of the data download, you may find that the dataset generated by load_data is not exactly as you expect. Here are some common pitfalls to look out for:","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Ensure that the data_vintage model setting is as you expect. (Try checking   data_vintage(m).)\nEnsure that the data_id model setting is correct for the given model.\nEnsure that the date_forecast_start model setting is as you expect, and that is not   logically incompatible with data_vintage.\nEnsure that the data_series field of the model object is set as expected.\nDouble check the transformations specified in the data_transforms field of the model   object.\nEnsure that the keys of the observables and data_transforms fields of the model object   match.\nCheck the input files for Non-FRED data sources. They should be   in the directory indicated by inpath(m, \"raw\"), be named appropriately given the   vintage of data expected, and be formatted appropriately. One may have to copy and   rename files of non-FRED data sources to match the specified vintage, even if the   contents of the files would be identical.\nLook for any immediate issues in the final dataset saved   (data_dsid=<xx>_vint=<yymmdd>.csv). If a data series in this file is all   NaN values, then likely a non-FRED data source was not provided correctly.\nEnsure that the column names of the data CSV match the keys of the observables field of   the model object.\nYou may receive a warning that an input data file \"does not contain the entire date range   specified\". This means that observations are not provided for some periods in which the   model requires data. This is perfectly okay if your data series starts after   date_presample_start.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"If you experience any problems using FredData.jl, ensure your API key is provided correctly and that there are no issues with your firewall, etc. Any issues with FredData.jl proper should be reported on that project's page.","category":"page"},{"location":"input_data/#Update-sample-input-data-1","page":"Input Data","title":"Update sample input data","text":"","category":"section"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"A sample dataset is provided for the 2015 Nov 27 vintage. To update this dataset:","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Step 1. See Setup to setup automatic data pulls using FredData.jl.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Step 2. Specify the exact data vintage desired:","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"julia>  m <= Setting(:data_vintage, \"yymmdd\")","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Step 3. Create data files for the non-FRED data sources (specified in    m.data_series). For model m990, the required data files include    spf_<yymmdd>.csv (with column ASACX10), longrate_<yymmdd>.csv (with    column FYCCZA), and fernald_<yymmdd>.csv (with columns TFPJQ and    TFPKQ). To include data on expected interest rates, the file    ois_<yymmdd>.csv is also required. To include data on population forecasts,    the file population_forecst_<yymmdd>.csv is also required (see    Incorporate population forecasts. See    New York Fed Model Input Data for details on the series    used and links to data sources.","category":"page"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Step 4. Run load_data(m); series from FRED will be downloaded and merged with the series from    non-FRED data sources that you have already created. See Common pitfalls for some potential issues.","category":"page"},{"location":"input_data/#Data-Transforms-and-Utilities-1","page":"Input Data","title":"Data Transforms and Utilities","text":"","category":"section"},{"location":"input_data/#","page":"Input Data","title":"Input Data","text":"Modules = [DSGE]\nPages   = [\"load_data.jl\", \"fred_data.jl\", \"transform_data.jl\", \"transformations.jl\", \"src/data/util.jl\"]\nOrder   = [:function, :type]","category":"page"},{"location":"input_data/#DSGE.df_to_matrix-Tuple{Union{AbstractDSGEModel, AbstractVARModel},DataFrames.DataFrame}","page":"Input Data","title":"DSGE.df_to_matrix","text":"df_to_matrix(m, df; cond_type = :none, in_sample = true)\n\nReturn df, converted to matrix of floats, and discard date column. Also ensure that rows are sorted by date and columns by m.observables, with the option to specify whether or not the out of sample rows are discarded. The output of this function is suitable for direct use in estimate, posterior, etc.\n\nKeyword Arguments:\n\ninclude_presample::Bool: indicates whether or not there are presample periods.\nin_sample::Bool: indicates whether or not to discard rows that are out of sample. Set this flag to false in\n\nthe case that you are calling filter_shocks! in the scenarios codebase.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.load_cond_data_levels-Tuple{AbstractDSGEModel}","page":"Input Data","title":"DSGE.load_cond_data_levels","text":"load_cond_data_levels(m::AbstractDSGEModel; verbose::Symbol=:low)\n\nCheck on disk in inpath(m, \"cond\") for a conditional dataset (in levels) of the correct vintage and load it.\n\nThe following series are also loaded from inpath(m, \"raw\") and either appended or merged into the conditional data:\n\nThe last period of (unconditional) data in levels (data_levels_<yymmdd>.csv), used to calculate growth rates\nThe first period of forecasted population (population_forecast_<yymmdd>.csv), used for per-capita calculations\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.load_data-Tuple{AbstractDSGEModel}","page":"Input Data","title":"DSGE.load_data","text":"load_data(m::AbstractDSGEModel; try_disk::Bool = true, verbose::Symbol = :low,\n          check_empty_columns::Bool = true, summary_statistics::Symbol = :low)\n\nCreate a DataFrame with all data series for this model, fully transformed.\n\nFirst, check the disk to see if a valid dataset is already stored in inpath(m, \"data\"). A dataset is valid if every series in m.observable_mappings is present and the entire sample is contained (from date_presample_start to date_mainsample_end. If no valid dataset is already stored, the dataset will be recreated. This check can be eliminated by passing try_disk=false.\n\nIf the dataset is to be recreated, in a preliminary stage, intermediate data series as specified in m.observable_mappings are loaded in levels using load_data_levels. See ?load_data_levels for more details.\n\nThen, the series in levels are transformed as specified in m.observable_mappings. See ?transform_data for more details.\n\nIf m.testing is false, then the resulting DataFrame is saved to disk as data_<yymmdd>.csv. The data are then returned to the caller.\n\nThe keyword check_empty_columns throws an error whenever a column is completely empty in the loaded data set if it is set to true.\n\nThe keyword summary_statistics prints out a variety of summary statistics on the loaded data. When set to :low, we print only the number of missing/NaNs for each data series. When set to :high, we also print means, standard deviations,\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.load_data_levels-Tuple{AbstractDSGEModel}","page":"Input Data","title":"DSGE.load_data_levels","text":"load_data_levels(m::AbstractDSGEModel; verbose::Symbol=:low)\n\nLoad data in levels by appealing to the data sources specified for the model. Data from FRED is loaded first, by default; then, merge other custom data sources.\n\nCheck on disk in inpath(m, \"data\") datasets, of the correct vintage, corresponding to the ones required by the entries in m.observable_mappings. Load the appropriate data series (specified in m.observable_mappings[key].input_series) for each data source.\n\nTo accomodate growth rates and other similar transformations, more rows of data may be downloaded than otherwise specified by the date model settings. (By the end of the process, these rows will have been dropped.)\n\nData from FRED (i.e. the :fred data source) are treated separately. These are downloaded using load_fred_data. See ?load_fred_data for more details.\n\nData from non-FRED data sources are read from disk, verified, and merged.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.parse_data_series-Tuple{AbstractDSGEModel}","page":"Input Data","title":"DSGE.parse_data_series","text":"parse_data_series(m::AbstractDSGEModel)\n\nParse m.observable_mappings for the data sources and mnemonics to read in.\n\nReturns a Dict{Symbol, Vector{Symbol}} mapping sources => mnemonics found in that data file.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.save_data-Tuple{AbstractDSGEModel,DataFrames.DataFrame}","page":"Input Data","title":"DSGE.save_data","text":"save_data(m::AbstractDSGEModel, df::DataFrame; cond_type::Symbol = :none)\n\nSave df to disk as CSV. File is located in inpath(m, \"data\").\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.load_fred_data-Tuple{AbstractDSGEModel}","page":"Input Data","title":"DSGE.load_fred_data","text":"load_fred_data(m::AbstractDSGEModel; start_date=\"1959-03-31\", end_date=prev_quarter())\n\nChecks in inpath(m, raw) for a FRED dataset corresponding to data_vintage(m). If a FRED vintage exists on disk, any required FRED series that is contained therein will be imported. All missing series will be downloaded directly from FRED using the FredData package. The full dataset is written to the appropriate data vintage file and returned.\n\nArguments\n\nm::AbstractDSGEModel: the model object\nstart_date: starting date.\nend_date: ending date.\n\nNotes\n\nThe FRED API reports observations according to the quarter-start date. load_fred_data returns data indexed by quarter-end date for compatibility with other datasets.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.transform_data-Tuple{AbstractDSGEModel,DataFrames.DataFrame}","page":"Input Data","title":"DSGE.transform_data","text":"transform_data(m::AbstractDSGEModel, levels::DataFrame; cond_type::Symbol = :none,\n    verbose::Symbol = :low)\n\nTransform data loaded in levels and order columns appropriately for the DSGE model. Returns DataFrame of transformed data.\n\nThe DataFrame levels is output from load_data_levels. The series in levels are transformed as specified in m.observable_mappings.\n\nTo prepare for per-capita transformations, population data are filtered using hpfilter. The series in levels to use as the population series is given by the population_mnemonic setting. If use_population_forecast(m), a population forecast is appended to the recorded population levels before the filtering. Both filtered and unfiltered population levels and growth rates are added to the levels data frame.\nThe transformations are applied for each series using the levels DataFrame as input.\n\nConditional data (identified by cond_type in [:semi, :full]) are handled slightly differently: If use_population_forecast(m), we drop the first period of the population forecast because we treat the first forecast period date_forecast_start(m) as if it were data. We also only apply transformations for the observables given in cond_full_names(m) or cond_semi_names(m).\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.annualtoquarter-Tuple{Any}","page":"Input Data","title":"DSGE.annualtoquarter","text":"annualtoquarter(v)\n\nConvert from annual to quarter frequency... by dividing by 4.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.difflog-Tuple{Array{T,1} where T}","page":"Input Data","title":"DSGE.difflog","text":"difflog(x::AbstractVector)\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.difflog-Tuple{Array}","page":"Input Data","title":"DSGE.difflog","text":"difflog(x::Array{AbstractFloat})\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.hpfilter-Tuple{AbstractArray{T,1} where T,Real}","page":"Input Data","title":"DSGE.hpfilter","text":"yt, yf = hpfilter(y, λ)\n\nApplies the Hodrick-Prescott filter (\"H-P filter\"). The smoothing parameter λ is applied to the columns of y, returning the trend component yt and the cyclical component yf. For quarterly data, one can use λ=1600.\n\nConsecutive missing values at the beginning or end of the time series are excluded from the filtering. If there are missing values within the series, the filtered values are all missing.\n\nSee also:\n\nHodrick, Robert; Prescott, Edward C. (1997). \"Postwar U.S. Business Cycles: An Empirical\nInvestigation\". Journal of Money, Credit, and Banking 29 (1): 1–16.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.loggrowthtopct-Tuple{AbstractArray}","page":"Input Data","title":"DSGE.loggrowthtopct","text":"loggrowthtopct(y)\n\nTransform from annualized quarter-over-quarter log growth rates to annualized quarter-over-quarter percent change.\n\nNote\n\nThis should only be used in Model 510, which has the core PCE inflation observable in annualized log growth rates.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.loggrowthtopct_4q_approx","page":"Input Data","title":"DSGE.loggrowthtopct_4q_approx","text":"loggrowthtopct_4q_approx(y, data = fill(NaN, 3))\n\nTransform from log growth rates to approximate 4-quarter percent change.\n\nThis method should only be used to transform scenarios forecasts, which are in   deviations from baseline.\n\nInputs\n\ny: the data we wish to transform to aggregate 4-quarter percent change from log per-capita growth rates. y is either a vector of length nperiods or an ndraws xnperiods` matrix.\ndata: if y = [y_t, y_{t+1}, ..., y_{t+nperiods-1}], then data = [y_{t-3}, y_{t-2}, y_{t-1}]. This is necessary to compute 4-quarter percent changes for the first three periods.\n\n\n\n\n\n","category":"function"},{"location":"input_data/#DSGE.loggrowthtopct_annualized-Tuple{AbstractArray}","page":"Input Data","title":"DSGE.loggrowthtopct_annualized","text":"loggrowthtopct_annualized(y)\n\nTransform from log growth rates to annualized quarter-over-quarter percent change.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.loggrowthtopct_annualized_percapita-Tuple{AbstractArray,AbstractArray{T,1} where T}","page":"Input Data","title":"DSGE.loggrowthtopct_annualized_percapita","text":"loggrowthtopct_annualized_percapita(y, pop_growth)\n\nTransform from log per-capita growth rates to annualized aggregate (not per-capita) quarter-over-quarter percent change.\n\nNote\n\nThis should only be used for output, consumption, investment and GDP deflator (inflation).\n\nInputs\n\ny: the data we wish to transform to annualized percent change from quarter-over-quarter log growth rates. y is either a vector of length nperiods or an ndraws xnperiods` matrix.\npop_growth::Vector: the length nperiods vector of log population growth rates.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.loggrowthtopct_percapita-Tuple{AbstractArray,AbstractArray{T,1} where T}","page":"Input Data","title":"DSGE.loggrowthtopct_percapita","text":"loggrowthtopct_percapita(y, pop_growth)\n\nTransform from annualized quarter-over-quarter log per-capita growth rates to annualized quarter-over-quarter aggregate percent change.\n\nNote\n\nThis should only be used in Model 510, which has the output growth observable in annualized log per-capita growth rates.\n\nInputs\n\ny: the data we wish to transform to annualized percent change from annualized log growth rates. y is either a vector of length nperiods or an ndraws xnperiods` matrix.\npop_growth::Vector: the length nperiods vector of log population growth rates.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.logleveltopct_4q_approx","page":"Input Data","title":"DSGE.logleveltopct_4q_approx","text":"logleveltopct_4q_approx(y, data = fill(NaN, 4))\n\nTransform from log levels to approximate 4-quarter percent change.\n\nThis method should only be used to transform scenarios forecasts, which are in   deviations from baseline.\n\nInputs\n\ny: the data we wish to transform to 4-quarter percent change from log levels. y is either a vector of length nperiods or an ndraws xnperiods` matrix.\ndata: if y = [y_t, y_{t+1}, ..., y_{t+nperiods-1}], then data = [y_{t-4}, y_{t-3}, y_{t-2}, y_{t-1}]. This is necessary to compute 4-quarter percent changes for the first three periods.\n\n\n\n\n\n","category":"function"},{"location":"input_data/#DSGE.logleveltopct_annualized","page":"Input Data","title":"DSGE.logleveltopct_annualized","text":"logleveltopct_annualized(y, y0 = NaN)\n\nTransform from log levels to annualized quarter-over-quarter percent change.\n\nInputs\n\ny: the data we wish to transform to annualized quarter-over-quarter percent change from log levels. y is either a vector of length nperiods or an ndraws xnperiods` matrix.\ny0: the last data point in the history (of state or observable) corresponding to the y variable. This is required to compute a percent change for the first period.\n\n\n\n\n\n","category":"function"},{"location":"input_data/#DSGE.logleveltopct_annualized_approx","page":"Input Data","title":"DSGE.logleveltopct_annualized_approx","text":"logleveltopct_annualized_approx(y, y0 = NaN)\n\nTransform from log levels to approximate annualized quarter-over-quarter percent change.\n\nThis method should only be used to transform scenarios forecasts, which are in   deviations from baseline.\n\nInputs\n\ny: the data we wish to transform to annualized quarter-over-quarter percent change from log levels. y is either a vector of length nperiods or an ndraws xnperiods` matrix.\ny0: the last data point in the history (of state or observable) corresponding to the y variable. This is required to compute a percent change for the first period.\n\n\n\n\n\n","category":"function"},{"location":"input_data/#DSGE.logleveltopct_annualized_percapita","page":"Input Data","title":"DSGE.logleveltopct_annualized_percapita","text":"logleveltopct_annualized_percapita(y, pop_growth, y0 = NaN)\n\nTransform from per-capita log levels to annualized aggregate (not per-capita) quarter-over-quarter percent change.\n\nNote\n\nThis is usually applied to labor supply (hours worked per hour), and probably shouldn't be used for any other observables.\n\nInputs\n\ny: the data we wish to transform to annualized aggregate quarter-over-quarter percent change from per-capita log levels. y is either a vector of length nperiods or an ndraws xnperiods` matrix.\npop_growth::Vector: the length nperiods vector of log population growth rates.\ny0: The last data point in the history (of state or observable) corresponding to the y variable. This is required to compute a percent change for the first period.\n\n\n\n\n\n","category":"function"},{"location":"input_data/#DSGE.nominal_to_real-Tuple{Symbol,DataFrames.DataFrame}","page":"Input Data","title":"DSGE.nominal_to_real","text":"nominal_to_real(col, df; deflator_mnemonic = :GDPDEF)\n\nConverts nominal to real values using the specified deflator.\n\nArguments\n\ncol: Symbol indicating which column of df to transform\ndf: DataFrame containining series for proper population measure and col\n\nKeyword arguments\n\ndeflator_mnemonic: indicates which deflator to use to calculate real values. Default value is the FRED GDP Deflator mnemonic.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.oneqtrpctchange-Tuple{AbstractArray{T,1} where T}","page":"Input Data","title":"DSGE.oneqtrpctchange","text":"oneqtrpctchange(y)\n\nCalculates the quarter-to-quarter percentage change of a series.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.percapita-Tuple{AbstractDSGEModel,Symbol,DataFrames.DataFrame}","page":"Input Data","title":"DSGE.percapita","text":"percapita(m, col, df)\npercapita(col, df, population_mnemonic)\n\nConverts data column col of DataFrame df to a per-capita value.\n\nThe first method checks hpfilter_population(m). If true, then it divides by the filtered population series. Otherwise it divides by the result of parse_population_mnemonic(m)[1].\n\nArguments\n\ncol: Symbol indicating which column of data to transform\ndf: DataFrame containining series for proper population measure and col\npopulation_mnemonic: a mnemonic found in df for some population measure\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.quartertoannual-Tuple{Any}","page":"Input Data","title":"DSGE.quartertoannual","text":"quartertoannual(v)\n\nConvert from quarter to annual frequency... by multiplying by 4.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.quartertoannualpercent-Tuple{Any}","page":"Input Data","title":"DSGE.quartertoannualpercent","text":"quartertoannualpercent(v)\n\nConvert from quarter to annual frequency in percent... by multiplying by 400.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.get_data_filename-Tuple{AbstractDSGEModel,Symbol}","page":"Input Data","title":"DSGE.get_data_filename","text":"get_data_filename(m, cond_type)\n\nReturns the data file for m, which depends on data_vintage(m), and if cond_type in [:semi, :full], also on cond_vintage(m) and cond_id(m).\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.iterate_quarters-Tuple{Dates.Date,Int64}","page":"Input Data","title":"DSGE.iterate_quarters","text":"iterate_quarters(start::Date, quarters::Int)\n\nReturns the date corresponding to start + quarters quarters.\n\nInputs\n\nstart: starting date\nquarters: number of quarters to iterate forward or backward\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.quartertodate-Tuple{String}","page":"Input Data","title":"DSGE.quartertodate","text":"quartertodate(string::String)\n\nConvert string in the form \"YYqX\", \"YYYYqX\", or \"YYYY-qX\" to a Date of the end of the indicated quarter. \"X\" is in {1,2,3,4} and the case of \"q\" is ignored.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.subtract_quarters-Tuple{Dates.Date,Dates.Date}","page":"Input Data","title":"DSGE.subtract_quarters","text":"subtract_quarters(t1::Date, t0::Date)\n\nCompute the number of quarters between t1 and t0, including t0 and excluding t1.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.data_to_df-Union{Tuple{T}, Tuple{AbstractDSGEModel,Array{T,2},Date}} where T<:AbstractFloat","page":"Input Data","title":"DSGE.data_to_df","text":"data_to_df(m, data, start_date)\n\nCreate a DataFrame out of the matrix data, including a :date column beginning in start_date.  Variable names and indices are obtained from m.observables.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.has_saved_data-Tuple{AbstractDSGEModel}","page":"Input Data","title":"DSGE.has_saved_data","text":"has_saved_data(m::AbstractDSGEModel; cond_type::Symbol = :none)\n\nDetermine if there is a saved dataset on disk for the required vintage and conditional type.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.isvalid_data-Tuple{AbstractDSGEModel,DataFrames.DataFrame}","page":"Input Data","title":"DSGE.isvalid_data","text":"isvalid_data(m::AbstractDSGEModel, df::DataFrame; cond_type::Symbol = :none,\n    check_empty_columns::Bool = true)\n\nReturn if dataset is valid for this model, ensuring that all observables are contained and that all quarters between the beginning of the presample and the end of the mainsample are contained. Also checks to make sure that expected interest rate data is available if n_anticipated_shocks(m) > 0.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.read_data-Tuple{AbstractDSGEModel}","page":"Input Data","title":"DSGE.read_data","text":"read_data(m::AbstractDSGEModel; cond_type::Symbol = :none)\n\nRead CSV from disk as DataFrame. File is located in inpath(m, \"data\").\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.read_population_data-Tuple{AbstractDSGEModel}","page":"Input Data","title":"DSGE.read_population_data","text":"read_population_data(m; verbose = :low)\n\nread_population_data(filename; verbose = :low)\n\nRead in population data stored in levels, either from inpath(m, \"raw\", \"population_data_levels_[vint].csv\") or filename.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.read_population_forecast-Tuple{AbstractDSGEModel}","page":"Input Data","title":"DSGE.read_population_forecast","text":"read_population_forecast(m; verbose = :low)\n\nread_population_forecast(filename, population_mnemonic, last_recorded_date; verbose = :low)\n\nRead in population forecast in levels, either from inpath(m, \"raw\", \"population_forecast_[vint].csv\") or filename. If that file does not exist, return an empty DataFrame.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.transform_population_data-Tuple{DataFrames.DataFrame,DataFrames.DataFrame,Symbol}","page":"Input Data","title":"DSGE.transform_population_data","text":"transform_population_data(population_data, population_forecast,\n    population_mnemonic; verbose = :low)\n\nLoad, HP-filter, and compute growth rates from population data in levels. Optionally do the same for forecasts.\n\nInputs\n\npopulation_data: pre-loaded DataFrame of historical population data containing the columns :date and population_mnemonic. Assumes this is sorted by date.\npopulation_forecast: pre-loaded DataFrame of population forecast containing the columns :date and population_mnemonic\npopulation_mnemonic: column name for population series in population_data and population_forecast\n\nKeyword Arguments\n\nverbose: one of :none, :low, or :high\nuse_hpfilter: whether to HP filter population data and forecast. See Output below.\npad_forecast_start::Bool: Whether you want to re-size\n\nthe populationforecast such that the first index is one quarter ahead of the last index of populationdata. Only set to false if you have manually constructed population_forecast to artificially start a quarter earlier, so as to avoid having an unnecessary missing first entry.\n\nOutput\n\nTwo dictionaries containing the following keys:\n\npopulation_data_out:\n:filtered_population_recorded: HP-filtered historical population series (levels)\n:dlfiltered_population_recorded: HP-filtered historical population series (growth rates)\n:dlpopulation_recorded: Non-filtered historical population series (growth rates)\npopulation_forecast_out:\n:filtered_population_forecast: HP-filtered population forecast series (levels)\n:dlfiltered_population_forecast: HP-filtered population forecast series (growth rates)\n:dlpopulation_forecast: Non-filtered historical population series (growth rates)\n\nIf population_forecast_file is not provided, the r\"forecast\" fields will be empty. If use_hpfilter = false, then the r\"filtered*\" fields will be empty.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.get_irf_transform-Tuple{Function}","page":"Input Data","title":"DSGE.get_irf_transform","text":"get_irf_transform(transform::Function)\n\nReturns the IRF-specific transformation, which doesn't add back population growth (since IRFs are given in deviations).\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.get_nopop_transform-Tuple{Function}","page":"Input Data","title":"DSGE.get_nopop_transform","text":"get_nopop_transform(transform::Function)\n\nReturns the corresponding transformation which doesn't add back population growth. Used for shock decompositions, deterministic trends, and IRFs, which are given in deviations.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.get_scenario_transform-Tuple{Function}","page":"Input Data","title":"DSGE.get_scenario_transform","text":"get_scenario_transform(transform::Function)\n\nGiven a transformation used for usual forecasting, return the transformation used for scenarios, which are forecasted in deviations from baseline.\n\nThe 1Q deviation from baseline should really be calculated by 1Q transforming the forecasts (in levels) under the baseline (call this y_b) and alternative scenario (y_s), then subtracting baseline from alternative scenario (since most of our 1Q transformations are nonlinear). Let y_d = y_s - y_b. Then, for example, the most correct loggrowthtopct_annualized transformation is:\n\ny_b_1q = 100*(exp(y_b/100)^4 - 1)\ny_s_1q = 100*(exp(y_s/100)^4 - 1)\ny_d_1q = y_b_1q - y_s_1q\n\nInstead, we approximate this by transforming the deviation directly:\n\ny_d_1q ≈ 4*(y_b - y_s)\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.get_transform4q-Tuple{Function}","page":"Input Data","title":"DSGE.get_transform4q","text":"get_transform4q(transform::Function)\n\nReturns the 4-quarter transformation associated with the annualizing transformation.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.lag-Tuple{AbstractArray,Int64}","page":"Input Data","title":"DSGE.lag","text":"series_lag_n = lag(series, n)\n\nReturns a particular data series lagged by n periods\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.loggrowthtopct_4q","page":"Input Data","title":"DSGE.loggrowthtopct_4q","text":"loggrowthtopct_4q(y, data = fill(NaN, 3))\n\nTransform from log growth rates to 4-quarter percent change.\n\nInputs\n\ny: the data we wish to transform to aggregate 4-quarter percent change from log per-capita growth rates. y is either a vector of length nperiods or an ndraws xnperiods` matrix.\ndata: if y = [y_t, y_{t+1}, ..., y_{t+nperiods-1}], then data = [y_{t-3}, y_{t-2}, y_{t-1}]. This is necessary to compute 4-quarter percent changes for the first three periods.\n\n\n\n\n\n","category":"function"},{"location":"input_data/#DSGE.loggrowthtopct_4q_percapita","page":"Input Data","title":"DSGE.loggrowthtopct_4q_percapita","text":"loggrowthtopct_4q_percapita(y, pop_growth, data = fill(NaN, 3))\n\nTransform from log per-capita growth rates to aggregate 4-quarter percent change.\n\nNote\n\nThis should only be used for output, consumption, investment, and GDP deflator (inflation).\n\nInputs\n\ny: the data we wish to transform to aggregate 4-quarter percent change from log per-capita growth rates. y is either a vector of length nperiods or an ndraws xnperiods` matrix.\npop_growth::Vector: the length nperiods vector of log population growth rates.\ndata: if y = [y_t, y_{t+1}, ..., y_{t+nperiods-1}], then data = [y_{t-3}, y_{t-2}, y_{t-1}]. This is necessary to compute 4-quarter percent changes for the first three periods.\n\n\n\n\n\n","category":"function"},{"location":"input_data/#DSGE.logleveltopct_4q","page":"Input Data","title":"DSGE.logleveltopct_4q","text":"logleveltopct_4q(y, data = fill(NaN, 4))\n\nTransform from log levels to 4-quarter percent change.\n\nInputs\n\ny: the data we wish to transform to 4-quarter percent change from log levels. y is either a vector of length nperiods or an ndraws xnperiods` matrix.\ndata: if y = [y_t, y_{t+1}, ..., y_{t+nperiods-1}], then data = [y_{t-4}, y_{t-3}, y_{t-2}, y_{t-1}]. This is necessary to compute 4-quarter percent changes for the first three periods.\n\n\n\n\n\n","category":"function"},{"location":"input_data/#DSGE.logleveltopct_4q_percapita","page":"Input Data","title":"DSGE.logleveltopct_4q_percapita","text":"logleveltopct_4q_percapita(y, pop_growth, data = fill(NaN, 4))\n\nTransform from per-capita log levels to 4-quarter aggregate percent change.\n\nNote\n\nThis is usually applied to labor supply (hours worked), and probably shouldn't be used for any other observables.\n\nInputs\n\ny: the data we wish to transform to 4-quarter aggregate percent change from per-capita log levels. y is either a vector of length nperiods or an ndraws xnperiods` matrix.\npop_growth::Vector: the length nperiods vector of log population growth rates.\ndata: if y = [y_t, y_{t+1}, ..., y_{t+nperiods-1}], then data = [y_{t-4}, y_{t-3}, y_{t-2}, y_{t-1}]. This is necessary to compute 4-quarter percent changes for the first three periods.\n\n\n\n\n\n","category":"function"},{"location":"input_data/#DSGE.prepend_data-Tuple{AbstractArray,AbstractArray{T,1} where T}","page":"Input Data","title":"DSGE.prepend_data","text":"prepend_data(y, data)\n\nPrepends data necessary for running 4q transformations.\n\nInputs:\n\ny: ndraws x t array representing a timeseries for variable y\ndata: vector representing a timeseries to prepend to y\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.datetoquarter-Tuple{Dates.Date}","page":"Input Data","title":"DSGE.datetoquarter","text":"datetoquarter(date::Date)\n\nConvert string in the form \"YYqX\", \"YYYYqX\", or \"YYYY-qX\" to a Date of the end of the indicated quarter. \"X\" is in {1,2,3,4} and the case of \"q\" is ignored.\n\nReturn an integer from the set {1,2,3,4}, corresponding to one of the quarters in a year given a Date object.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.datetoymdvec-Tuple{Dates.Date}","page":"Input Data","title":"DSGE.datetoymdvec","text":"datetoymdvec(dt)\n\nconverts a Date to a vector/matrix holding the year, month, and date.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.format_dates!-Tuple{Symbol,DataFrames.DataFrame}","page":"Input Data","title":"DSGE.format_dates!","text":"format_dates!(col, df)\n\nChange column col of dates in df from String to Date, and map any dates given in the interior of a quarter to the last day of the quarter.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.get_quarter_ends-Tuple{Dates.Date,Dates.Date}","page":"Input Data","title":"DSGE.get_quarter_ends","text":"get_quarter_ends(start_date::Date,end_date::Date)\n\nReturns an Array of quarter end dates between start_date and end_date.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.missing2nan-Tuple{Array}","page":"Input Data","title":"DSGE.missing2nan","text":"missing2nan(a::Array)\n\nConvert all elements of Union{X, Missing.Missing} or Missing.Missing to type Float64.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.missing_cond_vars!-Tuple{AbstractDSGEModel,DataFrames.DataFrame}","page":"Input Data","title":"DSGE.missing_cond_vars!","text":"missing_cond_vars!(m, df; cond_type = :none)\n\nMake conditional period variables not in cond_semi_names(m) or cond_full_names(m) missing if necessary.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.na2nan!-Tuple{Array}","page":"Input Data","title":"DSGE.na2nan!","text":"na2nan!(df::Array)\n\nConvert all NAs in an Array to NaNs.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.na2nan!-Tuple{DataFrames.DataFrame}","page":"Input Data","title":"DSGE.na2nan!","text":"na2nan!(df::DataFrame)\n\nConvert all NAs in a DataFrame to NaNs.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.next_quarter","page":"Input Data","title":"DSGE.next_quarter","text":"next_quarter(q::TimeType = now())\n\nReturns Date identifying last day of the next quarter\n\n\n\n\n\n","category":"function"},{"location":"input_data/#DSGE.prev_quarter","page":"Input Data","title":"DSGE.prev_quarter","text":"prev_quarter(q::TimeType = now())\n\nReturns Date identifying last day of the previous quarter\n\n\n\n\n\n","category":"function"},{"location":"input_data/#DSGE.quartertofloats-Tuple{Dates.Date}","page":"Input Data","title":"DSGE.quartertofloats","text":"quartertofloats(dt)\n\nconverts a Date to a floating point number based on the quarter\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.reconcile_column_names-Tuple{DataFrames.DataFrame,DataFrames.DataFrame}","page":"Input Data","title":"DSGE.reconcile_column_names","text":"reconcile_column_names(a::DataFrame, b::DataFrame)\n\nadds columns of missings to a and b so that both have the same set of column names.\n\n\n\n\n\n","category":"method"},{"location":"input_data/#DSGE.vinttodate-Tuple{String}","page":"Input Data","title":"DSGE.vinttodate","text":"function vinttodate(vint)\n\nReturn the string given by data_vintage(m), which is in the format YYYYMMDD, to a Date object.\n\n\n\n\n\n","category":"method"},{"location":"frbny_data/#frbny-data-1","page":"FRBNY Model Input Data","title":"New York Fed DSGE Model 990 Data","text":"","category":"section"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"CurrentModule = DSGE","category":"page"},{"location":"frbny_data/#Data-Series-1","page":"FRBNY Model Input Data","title":"Data Series","text":"","category":"section"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"The New York Fed DSGE Model takes an CSV file containing a matrix of data as input. The columns of this file contain transformations of the following series (the number corresponds to the column of data matrix):","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"Output Growth (Bureau of Economic Analysis)\nHours Worked (Bureau of Labor Statistics)\nReal Wage Growth (Bureau of Labor Statistics)\nInflation (GDP Deflator) (Bureau of Economic Analysis)\nInflation (Core PCE) (Bureau of Economic Analysis)\nFederal Funds Rate (Board of Governors of the Federal Reserve System)\nConsumption Growth (Bureau of Economic Analysis)\nInvestment Growth (Bureau of Economic Analysis)\nSpread (Baa) (Board of Governors of the Federal Reserve System)\n10-year Inflation Expectations (Federal Reserve Bank of Philadelphia)\n10-year Interest Rate (Board of Governors of the Federal Reserve System)\nTotal Factor Productivity (Federal Reserve Bank of San Francisco)","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"The following series are used to transform some series into per capita terms:","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"Civilian Noninstitutional Population 16 Years and Over (Bureau of Labor Statistics)","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"Most data series used to construct the above are retrieved from FRED (Federal Reserve Bank of St. Louis). Other data sources include:","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"The Total Factor Productivity series components are made available by the Federal Reserve   Bank of San Francisco, and can be found   here (series   alpha and dtfp from the linked spreadsheet). Alternatively, they can be   found as series TFPJQ@USECON (alpha) and TFPKQ@USECON (dtfp) via Haver Analytics. For more details on the series, see","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"Fernald, John. \"A Quarterly, Utilization-Adjusted Series on Total Factor Productivity.\"\nFederal Reserve Bank of San Francisco Working Paper 19 (2012): 20912.","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"The 10-year Inflation Expectations series from the Survey of Professional   Forecasters is made available by the Federal Reserve Bank of Philadelphia, and   can be found   here   (series INFCPI10YR from the linked spreadsheet). Alternatively, it can be found as series   ASACX10@SURVEYS via Haver Analytics.\nThe 10-year Treasury Yield (zero-coupon, continuously compounded) series is made   available by the Board of Governors of the Federal Reserve System, and can be found   here (series SVENY10   from the linked spreadsheet). Alternatively, it can be found as series FYCCZA@DAILY via Haver   Analytics. For more details on the series, see","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"Gurkaynak, Refet S., Brian Sack, and Jonathan H. Wright. \"The U.S. Treasury Yield Curve:\n1961 to the Present.\" Journal of Monetary Economics 54.8 (2007): 2291-2304.","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"For additional details on the series, including mnemonics and transformations used, please see Appendix A.I of","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"Del Negro, Marco, Marc P. Giannoni, and Frank Schorfheide. \"Inflation in the\nGreat Recession and New Keynesian Models.\" American Economic Journal:\nMacroeconomics 7.1 (2015): 168-196.","category":"page"},{"location":"frbny_data/#Interest-Rate-Expectations-Data-1","page":"FRBNY Model Input Data","title":"Interest Rate Expectations Data","text":"","category":"section"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"In our model (as used to compute the forecasts referenced in Liberty Street Economics posts), we treat the zero lower bound by adding anticipated policy shocks and data on the market-implied Federal Funds rate path. We do this by giving the model the market-implied Federal Funds rate path for the next n_anticipated_shocks quarters and forcing the model's interest rate path to hit those values in those quarters. Afterwards, the path is unconstrained. The model is trained on data that includes six quarters of interest rate expectations. The user is responsible for procuring interest rate expectations and appending it to the provided sample data set, as discussed in this documentation.","category":"page"},{"location":"frbny_data/#Implementation-1","page":"FRBNY Model Input Data","title":"Implementation","text":"","category":"section"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"If you are able to access data on the market-implied FFR path (or another form of interest rate expectations), you can augment the sample dataset or your own dataset to enable the anticipated policy shocks feature. We use internal data from the Federal Reserve Board on the implied Federal Funds Rate derived from OIS quotes. (One could also use interest rate expectations from Blue Chip Financial Forecasts or Survey of Professional Forecasters.)","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"Step 1. Choose a value for n_anticipated_shocks (we suggest 6):","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"m <= Setting(:n_anticipated_shocks, 6, true, \"nant\", \"Number of ant. pol. shocks\")","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"Step 2. Add implied FFR data to the data matrix:","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"2a. Append n_anticipated_shocks columns of NaN values to the end of the        data matrix.","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"2b. Construct a matrix of data, say ImpliedFFR, on anticipated policy        shocks. Define","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"For t from first quarter ZLB binds to last quarter ZLB binds\n   For h from 1 quarter ahead to n_anticipated_shocks quarters ahead\n       ImpliedFFR[t,h] := FFR at horizon h quarters ahead implied as of quarter t.\n   End\nEnd","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"2c. Fill in the data matrix with the ImpliedFFR matrix. The first    row of the ImpliedFFR matrix should go in the row of the data matrix in    which the ZLB first bound and the last row of the ImpliedFFR matrix should    go in the row of the data matrix in which the ZLB last bound.","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"Step 3. With your updated input data matrix, the code will add the appropriate   number of states, shocks, equilibrium condition equations, and measurement   equations.","category":"page"},{"location":"frbny_data/#Discussion-1","page":"FRBNY Model Input Data","title":"Discussion","text":"","category":"section"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"The implementation of anticipated policy shocks may not be immediately clear. Consider the following made-up data matrix:","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"t GDP FFR Inf ... Spread ImpFFR_1 ... ImpFFR_H\n1960Q1 2.5 5.0 2.5 ... 1.5 NaN ... NaN\n1960Q2 2.2 5.2 1.5 ... 1.3 NaN ... NaN\n... ... ... ... ... ... ... ... ...\n2008Q3 1.1 2.2 1.0 ... 1.5 NaN ... NaN\n2008Q4 -4.5 2.0 2.0 ... 1.3 1.0 ... 1.5\n... ... ... ... ... ... ... ... ...\n2013Q1 2.2 0.2 1.7 ... 1.7 0.2 ... 1.5\n2013Q2 2.3 0.2 1.8 ... 1.6 0.2 ... 1.4","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"Interpret this as follows:","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"For periods before 2008Q4, there was no forward guidance or ZLB to enforce, and we have no implied FFR values to enter.\nIn 2008Q4, actual FFR (made-up) was 2.2. Market prices implied that markets expected an interest rate of 1.0 in 2009Q1 – 1 period from now – and 1.5 n_anticipated_shocks periods from 2008Q4.\nIn 2013Q2, actual FFR (made-up) was 0.2. Markets expected FFR to remain at 0.2 in 2013Q3, ..., and expected FFR of 1.4 n_anticipated_shocks periods from 2013Q2.","category":"page"},{"location":"frbny_data/#References-1","page":"FRBNY Model Input Data","title":"References","text":"","category":"section"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"For a more comprehensive treatment of anticipated policy shocks, see NY Fed Staff Report The FRBNY DSGE Model","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"page 12, for how the anticipated policy shocks are incorporated into the monetary policy rule,\npage 16, for how the anticipated policy shocks entered the log-linear equilibrium conditions,\npage 18, for how the anticipated policy shocks and data on market expectations enter the measurement equation,\npage 23, for how the anticipated policy shocks propagate through the model.","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"For more in depth discussion of anticipated policy shocks/forward guidance and the impact on the macroeconomy, see NY Fed Working Paper The Forward Guidance Puzzle by Marco Del Negro, Marc Giannoni, and Christina Patterson.","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"Thanks to Matthew Cocci for the Discussion.","category":"page"},{"location":"frbny_data/#Disclaimer-1","page":"FRBNY Model Input Data","title":"Disclaimer","text":"","category":"section"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"The sample input data provided with DSGE.jl* is made available for purposes of demonstrating the function of the model only. By using the data you acknowledge and agree to the following terms and conditions. If you do not agree to these terms and conditions, do not use the data provided with the model.","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"Some data provided with DSGE.jl* may be copyrighted by its owner, and permission to use such copyrighted materials other than to demonstrate the functioning of DSGE.jl* for your personal use must be obtained from the owner. The Federal Reserve Bank of New York cannot provide permission to use the data other than as permitted in this agreement.","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"You may not use the name of the Federal Reserve Bank of New York to endorse or promote products derived from the use of the data provided with DSGE.jl*, nor for any other commercial purpose.","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"The Federal Reserve Bank of New York does not guarantee the completeness or accuracy of the data and does not provide updates or corrections to the data provided with the model. By downloading and using the data, you acknowledge and agree that your use of the data is at your own risk and that none of the parties involved in creating, producing or delivering DSGE.jl* is liable for any loss, injury, claim, liability or damage of any kind resulting in any way from: (a) any errors in or omissions from the data; (b) your use of the data or any conclusions you draw from it, regardless of whether you received any assistance from the Federal Reserve Bank of New York or its employees with regard to the data; or (c) the files containing the data or use of the website from which the data files were downloaded, including anything caused by any viruses, bugs or malfunctions.","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"ALL DATA AND MATERIALS ARE PROVIDED ON AN \"AS IS\", \"AS AVAILABLE\" BASIS WITHOUT WARRANTY OF ANY KIND. THE FEDERAL RESERVE BANK OF NEW YORK EXPRESSLY DISCLAIMS ALL WARRANTIES EXPRESS AND IMPLIED, INCLUDING WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, QUALITY AND NON-INFRINGEMENT. NEITHER THE FEDERAL RESERVE BANK OF NEW YORK NOR ANY EMPLOYEE OR AFFILIATE SHALL BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL OR CONSEQUENTIAL DAMAGES OF ANY KIND WHATSOEVER (INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOSS OF PROFITS, BUSINESS INTERRUPTION, LOSS OF INFORMATION, OR ATTORNEYS' FEES) IN ANY WAY DUE TO, RESULTING FROM OR ARISING IN CONNECTION WITH THE USE OR PERFORMANCE OF, OR INABILITY TO USE DATA OR MATERIALS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, AND REGARDLESS OF THE NEGLIGENCE OF THE BANK OR ANY EMPLOYEE OR AFFILIATE, EVEN IF THE FEDERAL RESERVE BANK OF NEW YORK HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"Reference to any specific commercial product, process or service does not constitute or imply its endorsement, recommendation or favoring by the Federal Reserve Bank of New York.","category":"page"},{"location":"frbny_data/#","page":"FRBNY Model Input Data","title":"FRBNY Model Input Data","text":"Company and product names mentioned in connection with the data remain the trademark and property of their respective owners.","category":"page"},{"location":"implementation_details/#Implementation-Details-1","page":"Implementation Details","title":"Implementation Details","text":"","category":"section"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"CurrentModule = DSGE","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"This section describes important functions and implementation features in greater detail. If the user is interested only in running the default model and reproducing the estimation results, this section can be ignored. Additional documentation can also be found in function documentation or in-line.","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"This section focuses on what the code does and why. Docstrings and the code itself (including comments) provide detailed information regarding how these basic procedures are implemented.","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"As of DSGE.jl v0.7.3, many types housed in the DSGE.jl package have been moved to ModelConstructors.jl*. The following types now belong in ModelConstructors.jl*:","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"AbstractModel\nAbstractParameter\nParameter{T<:Number, U<:Transform}: The abstract supertype for parameters that are directly estimated.\nUnscaledParameter{T<:Number, U:<Transform}: Concrete type for parameters that do not need to be scaled for equilibrium conditions.\nScaledParameter{T<:Number, U:<Transform}: Concrete type for parameters that are scaled for equilibrium conditions.\nSteadyStateParameter{T<:Number}: Concrete type for steady-state parameters.\nSetting\nObservable\nPseudoObservable\nTypes and functions used to define and work with priors","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"We refer users to the documentation provided for ModelConstructors.jl* for information about the implementation of these types. Below, we document the implementation of DSGE.jl specific types.","category":"page"},{"location":"implementation_details/#The-AbstractDSGEModel-Type-1","page":"Implementation Details","title":"The AbstractDSGEModel Type","text":"","category":"section"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"The AbstractModel type has been rewritten to be an abstract type for any model with parameters. We have replaced the AbstractModel type in DSGE.jl with the AbstractDSGEModel type, which is a subtype of AbstractModel and includes various methods that standard DSGE models need.","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"The AbstractDSGEModel type provides a common interface for all DSGE model objects, which greatly facilitates the implementation of new model specifications. Any concrete subtype of AbstractDSGEModel can be passed to any function defined for AbstractDSGEModel, provided that the concrete type has the fields that the function expects to be available.","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"Model990 is one example of a concrete subtype of AbstractDSGEModel that implements a single specification of the New York Fed DSGE model. All model objects must have these fields so that the interface for AbstractDSGEModel objects works correctly.  See Editing or Extending a Model for more detail.","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"Model990","category":"page"},{"location":"implementation_details/#DSGE.Model990","page":"Implementation Details","title":"DSGE.Model990","text":"Model990{T} <: AbstractRepModel{T}\n\nThe Model990 type defines the structure of the New York Fed DSGE model.\n\nFields\n\nParameters and Steady-States\n\nparameters::Vector{AbstractParameter}: Vector of all time-invariant model parameters.\nsteady_state::Vector{AbstractParameter}: Model steady-state values, computed as a function of elements of parameters.\nkeys::OrderedDict{Symbol,Int}: Maps human-readable names for all model parameters and steady-states to their indices in parameters and steady_state.\n\nInputs to Measurement and Equilibrium Condition Equations\n\nThe following fields are dictionaries that map human-readable names to row and column indices in the matrix representations of of the measurement equation and equilibrium conditions.\n\nendogenous_states::OrderedDict{Symbol,Int}: Maps each state to a column in the measurement and equilibrium condition matrices.\nexogenous_shocks::OrderedDict{Symbol,Int}: Maps each shock to a column in the measurement and equilibrium condition matrices.\nexpected_shocks::OrderedDict{Symbol,Int}: Maps each expected shock to a column in the measurement and equilibrium condition matrices.\nequilibrium_conditions::OrderedDict{Symbol,Int}: Maps each equlibrium condition to a row in the model's equilibrium condition matrices.\nendogenous_states_augmented::OrderedDict{Symbol,Int}: Maps lagged states to their columns in the measurement and equilibrium condition equations. These are added after gensys solves the model.\nobservables::OrderedDict{Symbol,Int}: Maps each observable to a row in the model's measurement equation matrices.\npseudo_observables::OrderedDict{Symbol,Int}: Maps each pseudo-observable to a row in the model's pseudo-measurement equation matrices.\n\nModel Specifications and Settings\n\nspec::String: The model specification identifier, \"m990\", cached here for filepath computation.\nsubspec::String: The model subspecification number, indicating that some parameters from the original model spec (\"ss3\") are initialized differently. Cached here for filepath computation.\nsettings::Dict{Symbol,Setting}: Settings/flags that affect computation without changing the economic or mathematical setup of the model.\ntest_settings::Dict{Symbol,Setting}: Settings/flags for testing mode\n\nOther Fields\n\nrng::MersenneTwister: Random number generator. Can be is seeded to ensure reproducibility in algorithms that involve randomness (such as Metropolis-Hastings).\ntesting::Bool: Indicates whether the model is in testing mode. If true, settings from m.test_settings are used in place of those in m.settings.\nobservable_mappings::OrderedDict{Symbol,Observable}: A dictionary that stores data sources, series mnemonics, and transformations to/from model units. DSGE.jl will fetch data from the Federal Reserve Bank of St. Louis's FRED database; all other data must be downloaded by the user. See load_data and Observable for further details.\npseudo_observable_mappings::OrderedDict{Symbol,PseudoObservable}: A dictionary that stores names and transformations to/from model units. See PseudoObservable for further details.\n\n\n\n\n\n","category":"type"},{"location":"implementation_details/#Defining-Indices-1","page":"Implementation Details","title":"Defining Indices","text":"","category":"section"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"The model's equilibrium conditions and observables are represented as fairly large matrices, and keeping track of which rows and columns correspond to which states, shocks, equations, etc. can be confusing. To improve clarity, we define several dictionaries that map variable names to indices in these matrices:","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"endogenous_states: Indices of endogenous model states\nexogenous_shocks: Indices of exogenous shocks\nexpected_shocks: Indices of expectation shocks\nequilibrium_conditions: Indices of equilibrium condition equations\nendogenous_states_augmented: Indices of model states, after model solution and system augmentation\nobservables:  Indices of named observables","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"This approach has a number of advantages. Most importantly, it is robust to inadvertent typos or indexing errors. Since the actual index number doesn't matter to us, the user only needs to define the names of their equilibrium conditions, states, and other variables. Adding states is easy - we have only to add them to the appropriate list in the model constructor, and they will be assigned an index.","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"As an example, consider the model's equilibrium conditions. The canonical representation of the equilibrium conditions is","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"Γ0 s_t = Γ1 s_{t-1} + C + Ψ ε_t + Π η_t","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"where Γ0, Γ1, C, Ψ, and Π are matrices of coefficients for s_t (states at time t), s_{t-1} (lagged states), ε_t (exogenous shocks) and η_t (expectational shocks). Each row of these matrices corresponds to an equilibrium condition, which we define using a descriptive name (for example, we name the consumption Euler equation :euler). States (columns of Γ0 and Γ1), exogenous shocks (columns of Ψ), and expectational shocks (columns Π) also have names.","category":"page"},{"location":"implementation_details/#Type-Interfaces-1","page":"Implementation Details","title":"Type Interfaces","text":"","category":"section"},{"location":"implementation_details/#AbstractDSGEModel-Interface-1","page":"Implementation Details","title":"AbstractDSGEModel Interface","text":"","category":"section"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"DSGE.update!\nDSGE.transform_to_model_space!\nDSGE.load_parameters_from_file\nDSGE.specify_mode!\nDSGE.specify_hessian","category":"page"},{"location":"implementation_details/#DSGE.update!","page":"Implementation Details","title":"DSGE.update!","text":"update!(m::AbstractDSGEModel, values::Vector{T}) where T<:AbstractFloat\n\nUpdate m.parameters with values, recomputing the steady-state parameter values.\n\nArguments:\n\nm: the model object\nvalues: the new values to assign to non-steady-state parameters.\n\n\n\n\n\nupdate!(m::AbstractDSGEModel, values::ParameterVector{T}) where T\n\nUpdate m.parameters with values, recomputing the steady-state parameter values.\n\nArguments:\n\nm: the model object\nvalues: the new values to assign to non-steady-state parameters.\n\n\n\n\n\n","category":"function"},{"location":"implementation_details/#DSGE.transform_to_model_space!","page":"Implementation Details","title":"DSGE.transform_to_model_space!","text":"transform_to_model_space!(m::AbstractDSGEModel, values::Vector{T}) where T<:AbstractFloat\n\nTransforms values from the real line to the model space, and assigns values[i] to m.parameters[i].value for non-steady-state parameters. Recomputes the steady-state paramter values.\n\nArguments\n\nm: the model object\nvalues: the new values to assign to non-steady-state parameters.\n\n\n\n\n\ntransform_to_model_space!(m::AbstractDSGEVARModel, values::Vector{T}) where T<:AbstractFloat\n\nTransforms values from the real line to the model space, and assigns values[i] to m.parameters[i].value for non-steady-state parameters. Recomputes the steady-state paramter values.\n\nArguments\n\nm: the model object\nvalues: the new values to assign to non-steady-state parameters.\n\n\n\n\n\n","category":"function"},{"location":"implementation_details/#DSGE.load_parameters_from_file","page":"Implementation Details","title":"DSGE.load_parameters_from_file","text":"load_parameters_from_file(m::AbstractDSGEModel,path::String)\n\nReturns a vector of parameters, read from a file, suitable for updating m.\n\n\n\n\n\n","category":"function"},{"location":"implementation_details/#DSGE.specify_mode!","page":"Implementation Details","title":"DSGE.specify_mode!","text":"specify_mode!(m::AbstractDSGEModel, mode_file::String=\"\"; verbose=:low)\n\nUpdates the values of m.parameters with the values from mode_file. Sets reoptimize setting to false.\n\nUsage: should be run before calling estimate(m), e.g.:\n\nm = Model990()\nspecify_mode!(m, modefile)\nestimate(m)\n\n\n\n\n\n","category":"function"},{"location":"implementation_details/#Parameter-Interface-1","page":"Implementation Details","title":"Parameter Interface","text":"","category":"section"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"Modules = [DSGE]\nPages = [\"parameters.jl\"]\nOrder = [:function]","category":"page"},{"location":"implementation_details/#Setting-Interface-1","page":"Implementation Details","title":"Setting Interface","text":"","category":"section"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"Modules = [DSGE]\nPages = [\"settings.jl\"]\nOrder = [:function]","category":"page"},{"location":"implementation_details/#The-PoolModel-Type-1","page":"Implementation Details","title":"The PoolModel Type","text":"","category":"section"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"A PoolModel has a very similar structure to concrete subtypes of AbstractDSGEModel, but certain fields have been removed because they are not necessary for a PoolModel, such as exogenous_shocks. We chose to define PoolModel as a concrete subtype of AbstractDSGEModel because, for the foreseeable future, we have no plans to implement a more complex type hierarchy for types that perform model averaging. By defining PoolModel as a subtype of a AbstractDSGEModel and taking advantage of multiple dispatch, less refactoring was required to make PoolModel compatible with DSGE.jl* functions like estimate.","category":"page"},{"location":"implementation_details/#The-DSGEVAR-Type-1","page":"Implementation Details","title":"The DSGEVAR Type","text":"","category":"section"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"Like the PoolModel type, the DSGEVAR is not a DSGE model, but unlike the PoolModel, which is still a subtype of AbstractDSGEModel, DSGEVAR has the following type hierarchy:","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"DSGEVAR <: AbstractDSGEVARModel <: AbstractVARModel <: AbstractModel","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"The behavior of DSGEVAR is sufficiently distinct from AbstractDSGEModel and requires enough specialized functions that simply using multiple dispatch did not seem an effective way to implement DSGEVAR. Moreover, several functions, like the impulse_responses code, apply to generic VARs. Restricting these functions to DSGEVAR did not seem like the best idea.","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"In the near term, there are no plans to further flesh out the VAR capabilities of DSGE.jl, but in the longer term, we may add VAR routines to DSGE.jl or implement them in a separate package. If we do create a separate package, then DSGEVAR will be refactored to be compatible with this new package.","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"Features that have not been fully implemented for DSGEVAR include","category":"page"},{"location":"implementation_details/#","page":"Implementation Details","title":"Implementation Details","text":"Loading data directly from the DSGEVAR\nCalling forecast_one on a DSGEVAR\nCalling compute_meansbands on a DSGEVAR\nPlotting with a DSGEVAR\nAlternative policy","category":"page"},{"location":"solving/#solving-dsge-doc-1","page":"Solving the Model","title":"Solving the Model","text":"","category":"section"},{"location":"solving/#The-gensys-routine-1","page":"Solving the Model","title":"The gensys routine","text":"","category":"section"},{"location":"solving/#","page":"Solving the Model","title":"Solving the Model","text":"The DSGE model is written down in its canonical representation:","category":"page"},{"location":"solving/#","page":"Solving the Model","title":"Solving the Model","text":"Gamma_0 s_t = Gamma_1 s_t-1 + C + Psi epsilon_t + Pi eta_t","category":"page"},{"location":"solving/#","page":"Solving the Model","title":"Solving the Model","text":"where Gamma_0, Gamma_1, C, Psi, and Pi are matrices of coefficients for s_t (states at time t), s_t-1 (lagged states), epsilon_t (exogenous shocks) and eta_t (expectational shocks).","category":"page"},{"location":"solving/#","page":"Solving the Model","title":"Solving the Model","text":"DSGE.jl solves the model to obtain its state-space form:","category":"page"},{"location":"solving/#","page":"Solving the Model","title":"Solving the Model","text":"beginalign*\ns_t = T s_t-1 + R epsilon_t + C  epsilon_t sim N(0 Q)  mathrm(transition) \ny_t = Z s_t + D + u_t  u_t sim N(0 E)  mathrm(measurement)\nendalign*","category":"page"},{"location":"solving/#","page":"Solving the Model","title":"Solving the Model","text":"using the gensys routine of Chris Sims, introduced in this paper. We provide a standalone native Julia implementation of the routine (gensys) as well as a wrapper for AbstractDSGEModel subtypes (solve). When the Gensys.jl package becomes ready for use, we intend to deprecate our gensys code and substitute the gensysdt method for our code.","category":"page"},{"location":"solving/#","page":"Solving the Model","title":"Solving the Model","text":"DSGE.solve","category":"page"},{"location":"solving/#DSGE.solve","page":"Solving the Model","title":"DSGE.solve","text":"solve(m::AbstractDSGEModel; apply_altpolicy = false)\n\nDriver to compute the model solution and augment transition matrices.\n\nInputs\n\nm: the model object\n\nKeyword Arguments\n\napply_altpolicy::Bool: whether or not to solve the model under the alternative policy. This should be true when we solve the model to forecast, but false when computing smoothed historical states (since the past was estimated under the baseline rule).\nregime_switching::Bool: true if the state space system features regime switching\nregime::Int: specifies the specific regime to solve for.\n\nOutputs\n\nTTT, RRR, and CCC matrices of the state transition equation:  S_t = TTT*S_{t-1} + RRR*ϵ_t + CCC\n\n\n\n\n\nsolve(m::PoolModel) ```\n\nDriver to compute the model solution when using the PoolModel type\n\nInputs\n\nm: the PoolModel object\n\nOutputs\n\nΦ: transition function\nF_ϵ: distribution of structural shock\nFλ: prior on the initial λ0\n\n\n\n\n\nsolvect(m::AbstractCTModel; reduction_settings_check::Bool = false)\n\nDriver to compute the model solution and augment transition matrices.\n\nInputs\n\nm: the model object\n\nOutputs\n\nTTT, RRR, and CCC matrices of the state transition equation:  S_t = TTT*S_{t-1} + RRR*ϵ_t + CCC\n\n\n\n\n\n","category":"function"},{"location":"estimation/#estimation-step-1","page":"Estimation","title":"Estimation","text":"","category":"section"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"CurrentModule = DSGE","category":"page"},{"location":"estimation/#Procedure-1","page":"Estimation","title":"Procedure","text":"","category":"section"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"The goal of the estimation step is to sample from the posterior distribution of the model parameters. DSGE.jl provides two estimation routines. The default is a Metropolis-Hastings sampler to do this, which requires as a proposal covariance matrix and the Hessian matrix corresponding to the posterior mode. The second routine is a Sequential Monte Carlo sampler, which is called from the SMC.jl package. Both routines implement adaptive proposal densities and parameter blocking.[1]","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"The function estimate implements the entire procedure for either routine. Below, we explain the MH algorithm. For documentation of the SMC algorithm, see here.","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"Main Steps:","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"Initialization: Read in and transform raw data from save/input_data/. See Input Data for more details.\nReoptimize parameter vector: The main program will call the csminwel optimization routine (located in csminwel.jl) to find modal parameter estimates.\nCompute Hessian matrix: Computing the Hessian matrix to scale the proposal distribution in the Metropolis-Hastings algorithm.\nSample from Posterior: Posterior sampling is performed using the Metropolis-Hastings algorithm. A proposal distribution is constructed centered at the posterior mode and with proposal covariance scaled by the inverse of the Hessian matrix. Settings for the number of sampling blocks and the size of those blocks can be altered as described in Editing or Extending a Model.","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"Remark: For the MH sampler, in addition to saving each mh_thin-th draw of the parameter vector, the estimation program also saves the resulting posterior value and transition equation matrices implied by each draw of the parameter vector. This is to save time in the forecasting step since that code can avoid recomputing those matrices.","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"For the SMC sampler, we save a jld2 file containing a Cloud object which holds any relevant information about the particles approximating the posterior, the normalized weights of the particles, and the unnormalized weights of particles. We also save a draw of parameters to an h5 file.","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"To run the entire procedure, the user simply calls the estimate routine:","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"DSGE.estimate","category":"page"},{"location":"estimation/#DSGE.estimate","page":"Estimation","title":"DSGE.estimate","text":"estimate(m, data; verbose=:low, proposal_covariance=Matrix(), method=:SMC)\n\nEstimate the DSGE parameter posterior distribution.\n\nArguments:\n\nm::AbstractDSGEModel or m::AbstractVARModel: model object\n\nOptional Arguments:\n\ndata: well-formed data as Matrix or DataFrame. If this is not provided, the load_data routine will be executed.\n\nKeyword Arguments:\n\nverbose::Symbol: The desired frequency of function progress messages printed to standard out.\n:none: No status updates will be reported.\n:low: Status updates will be provided in csminwel and at each block in Metropolis-Hastings.\n:high: Status updates provided at each iteration in Metropolis-Hastings.\nproposal_covariance::Matrix: Used to test the metropolis_hastings algorithm with a precomputed covariance matrix for the proposal distribution. When the Hessian is singular, eigenvectors corresponding to zero eigenvectors are not well defined, so eigenvalue decomposition can cause problems. Passing a precomputed matrix allows us to ensure that the rest of the routine has not broken.\nmethod::Symbol: The method to use when sampling from the posterior distribution. Can   be either :MH for standard Metropolis Hastings Markov Chain Monte Carlo, or :SMC   for Sequential Monte Carlo.\nmle: Set to true if parameters should be estimated by maximum likelihood directly.   If this is set to true, this function will return after estimating parameters.\nsampling: Set to false to disable sampling from the posterior.\nfilestring_addl::Vector{String}: Additional strings to add to the file name   of estimation output as a way to distinguish output from each other.\ncontinue_intermediate::Bool: set to true if the estimation is starting   from an intermediate stage that has been previously saved.\nintermediate_stage_start::Int: number of the stage from which the user wants   to continue the estimation (see continue_intermediate)\nsave_intermediate::Bool: set to true to save intermediate stages when using SMC\nintermediate_stage_increment::Int: number of stages that must pass before saving   another intermediate stage.\ntempered_update_prior_weight::Float64: when using a tempered update, the user   can create a bridge distribution as a convex combination of the prior and a   previously ran estimation. This keyword is the relative weight on the prior   in the convex combination.\nrun_csminwel::Bool: by default, csminwel is run after a SMC estimation finishes   to recover the true mode of the posterior. Set to false to avoid this step   (csminwel can take hours for medium-scale DSGE models).\n\n\n\n\n\n","category":"function"},{"location":"estimation/#Metropolis-Hastings-Sampler-1","page":"Estimation","title":"Metropolis-Hastings Sampler","text":"","category":"section"},{"location":"estimation/#Computing-the-Posterior-1","page":"Estimation","title":"Computing the Posterior","text":"","category":"section"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"In DSGE.jl, the function posterior computes the value of the posterior distribution at a given parameter vector. It calls the likelihood function, which in turn calls the filter routine. See Estimation routines for more details on these functions.","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"We implement the Kalman Filter via the filter function to compute the log-likelihood, and add this to the log prior to obtain the log posterior. See StateSpaceRoutines.jl for a model-independent implementation of the Kalman filter.","category":"page"},{"location":"estimation/#estimation-reoptimizing-1","page":"Estimation","title":"Optimizing or Reoptimizing","text":"","category":"section"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"Generally, the user will want to reoptimize the parameter vector (and consequently, calculate the Hessian at this new mode) every time they conduct posterior sampling; that is, when:","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"the input data are updated with a new quarter of observations or revised\nthe model sub-specification is changed\nthe model is derived from an existing model with different equilibrium conditions or measurement equation.","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"This behavior can be controlled more finely.","category":"page"},{"location":"estimation/#Reoptimize-from-a-Specified-Starting-Vector-1","page":"Estimation","title":"Reoptimize from a Specified Starting Vector","text":"","category":"section"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"Reoptimize the model starting from the parameter values supplied in a specified file. Ensure that you supply an HDF5 file with a variable named params that is the correct dimension and data type.","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"m = Model990()\nparams = load_parameters_from_file(m, \"path/to/parameter/file.h5\")\nupdate!(m, params)\nestimate(m)","category":"page"},{"location":"estimation/#Skip-Reoptimization-Entirely-1","page":"Estimation","title":"Skip Reoptimization Entirely","text":"","category":"section"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"You can provide a modal parameter vector and optionally a Hessian matrix calculated at that mode to skip the reoptimization entirely. These values are usually computed by the user previously.","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"You can skip reoptimization of the parameter vector entirely.","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"m = Model990()\nspecify_mode!(m, \"path/to/parameter/mode/file.h5\")\nestimate(m)","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"The specify_mode! function will update the parameter vector to the mode and skip reoptimization by setting the reoptimize model setting. Ensure that you supply an HDF5 file with a variable named params that is the correct dimension and data type. (See also the utility function load_parameters_from_file.)","category":"page"},{"location":"estimation/#Random-Walk-Metropolis-Hastings-1","page":"Estimation","title":"Random Walk Metropolis-Hastings","text":"","category":"section"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"For relatively simple problems, a random walk MH sampler is sufficient and avoids unnecessary computations like calculating the Hessian. Suppose we want to estimate all the parameters of a DSGE model m. The following code implements RWMH for m.","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"m = Model990()\nm <= Setting(:hessian_path, \"path/to//matrix/with/right/dimensions/saved/as/mh_hessian.h5\")\nestimate(m; proposal_covariance = Matrix{Float64}(I,size(m.parameters)))","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"The saved Hessian needs to have the same dimensions as the number of parameters. The simplest option is save an identity matrix. The proposal covariance also needs to have the same dimensions as the number of parameters. If the user does not want to estimate every parameter (i.e. some parameters are fixed), then the user needs to zero out the rows of the proposal covariance that correspond to fixed parameters.","category":"page"},{"location":"estimation/#Calculating-the-Hessian-1","page":"Estimation","title":"Calculating the Hessian","text":"","category":"section"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"By default, estimate will recompute the Hessian matrix. You can skip calculation of the Hessian matrix entirely if you provide a file with a Hessian that has been pre-computed.","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"m = Model990()\nspecify_mode!(m, \"path/to/parameter/mode/file.h5\")\nspecify_hessian(m, \"path/to/Hessian/matrix/file.h5\")\nestimate(m)","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"The specify_hessian function will cause estimate to read in the Hessian matrix rather than calculating it directly.  Ensure that you supply an HDF5 file with a variable named hessian that is the correct dimension and data type. Specifying the Hessian matrix but not the parameter mode results in undefined behavior.","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"See [Hessian Approximation] for more details on the Hessian computation.","category":"page"},{"location":"estimation/#Estimation-routines-1","page":"Estimation","title":"Estimation routines","text":"","category":"section"},{"location":"estimation/#Prior,-Likelihood-and-Posterior-calculations-1","page":"Estimation","title":"Prior, Likelihood and Posterior calculations","text":"","category":"section"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"DSGE.prior\nDSGE.likelihood\nDSGE.posterior\nDSGE.posterior!","category":"page"},{"location":"estimation/#ModelConstructors.prior","page":"Estimation","title":"ModelConstructors.prior","text":"prior(m::AbstractDSGEModel{T})\n\nCalculates log joint prior density of m.parameters.\n\n\n\n\n\n","category":"function"},{"location":"estimation/#DSGE.likelihood","page":"Estimation","title":"DSGE.likelihood","text":"likelihood(m::AbstractDSGEModel, data::Matrix{T};\n           sampler::Bool = false, catch_errors::Bool = false) where {T<:AbstractFloat}\n\nEvaluate the DSGE likelihood function. Can handle two-part estimation where the observed sample contains both a normal stretch of time (in which interest rates are positive) and a stretch of time in which interest rates reach the zero lower bound. If there is a zero-lower-bound period, then we filter over the 2 periods separately. Otherwise, we filter over the main sample all at once.\n\nArguments\n\nm: The model object\ndata: matrix of data for observables\n\nOptional Arguments\n\nsampler: Whether metropolis_hastings or smc is the caller. If sampler=true, the   transition matrices for the zero-lower-bound period are returned in a dictionary.\ncatch_errors: If sampler = true, GensysErrors should always be caught.\n\n\n\n\n\nlikelihood(m::AbstractVARModel, data::Matrix{T};\n           sampler::Bool = false, catch_errors::Bool = false,\n           verbose::Symbol = :high) where {T<:AbstractFloat}\n\nEvaluate a VAR likelihood function.\n\nArguments\n\nm: The model object\ndata: matrix of data for observables\n\nOptional Arguments\n\nsampler: Whether metropolis_hastings or smc is the caller. If sampler=true, the   transition matrices for the zero-lower-bound period are returned in a dictionary.\ncatch_errors: If sampler = true, GensysErrors should always be caught.\n\n\n\n\n\n","category":"function"},{"location":"estimation/#ModelConstructors.posterior","page":"Estimation","title":"ModelConstructors.posterior","text":"posterior(m::Union{AbstractDSGEModel{T},AbstractVARModel{T}}, data::Matrix{T};\n          sampler::Bool = false, catch_errors::Bool = false,\n          φ_smc::Float64 = 1) where {T<:AbstractFloat}\n\nCalculates and returns the log of the posterior distribution for m.parameters:\n\nlog posterior  = log likelihood + log prior + const\nlog Pr(Θ|data) = log Pr(data|Θ) + log Pr(Θ) + const\n\nArguments\n\nm: the model object\ndata: matrix of data for observables\n\nOptional Arguments\n\n-sampler: Whether metropolishastings or smc is the caller. If sampler=true,     the log likelihood and the transition matrices for the zero-lower-bound     period are also returned. -`catcherrors: Whether to catch errors of typeGensysErrororParamBoundsError`\n\nφ_smc: a tempering factor to change the relative weighting of the prior and    the likelihood when calculating the posterior. It is used primarily in SMC.\n\n\n\n\n\n","category":"function"},{"location":"estimation/#ModelConstructors.posterior!","page":"Estimation","title":"ModelConstructors.posterior!","text":"posterior!(m::Union{AbstractDSGEModel{T},AbstractVARModel{T}},\n           parameters::Vector{T}, data::Matrix{T};\n           sampler::Bool = false, catch_errors::Bool = false,\n           φ_smc::Float64 = 1.) where {T<:AbstractFloat}\n\nEvaluates the log posterior density at parameters.\n\nArguments\n\nm: The model object\nparameters: New values for the model parameters\ndata: Matrix of input data for observables\n\nOptional Arguments\n\nsampler: Whether metropolis_hastings or smc is the caller. If sampler=true,    the log likelihood and the transition matrices for the zero-lower-bound    period are also returned.\ncatch_errors: Whether to catch errors of type GensysError or ParamBoundsError    If sampler = true, both should always be caught.\nφ_smc: a tempering factor to change the relative weighting of the prior and    the likelihood when calculating the posterior. It is used primarily in SMC.\n\n\n\n\n\n","category":"function"},{"location":"estimation/#Optimization-1","page":"Estimation","title":"Optimization","text":"","category":"section"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"See Optimization","category":"page"},{"location":"estimation/#Full-Estimation-Routine-1","page":"Estimation","title":"Full Estimation Routine","text":"","category":"section"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"See estimate","category":"page"},{"location":"estimation/#Output-Analysis-1","page":"Estimation","title":"Output Analysis","text":"","category":"section"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"Modules = [DSGE]\nPages   = [\"moments.jl\"]\nOrder   = [:function, :type]","category":"page"},{"location":"estimation/#DSGE.compute_Eλ-Union{Tuple{T}, Tuple{PoolModel{T},Int64,Array{T,1}}, Tuple{PoolModel{T},Int64,Array{T,1},Array{T,2}}, Tuple{PoolModel{T},Int64,Array{T,1},Array{T,2},Array{T,1}}} where T<:AbstractFloat","page":"Estimation","title":"DSGE.compute_Eλ","text":"compute_Eλ(m, h, λvec, θmat = [], weights = [];\n    current_period = true, parallel = true) where T<:AbstractFloat\n\nComputes and samples from the conditional density p(λt|θ, It, P) for particle in θs, which represents the posterior distribution.\n\nInputs\n\nm::PoolModel{T}: PoolModel object\nh::Int64: forecast horizon\nλvec::Vector{T}: vector of particles of λ samples from (θ,λ) joint distribution\n`θmat::Matrix{T}': matrix of posterior parameter samples\nweights::Vector{T}: weights of λ particles, defaults to equal weights\n\nKeyword Argument\n\ncurrent_period::Bool: compute Eλ for current period t\nparallel::Bool: use parallel computing to compute and sample λ\nget_dpp_pred_dens::Bool: compute predictive densities according to dynamic prediction pools\n\nOutputs\n\nλhat_tplush::Float64: E[λ{t+h|t} | It^P, P]\nλhat_t::Float64: E[λ{t|t} | It^P, P]\n\n```\n\n\n\n\n\n","category":"method"},{"location":"estimation/#DSGE.find_density_bands-Union{Tuple{T}, Tuple{AbstractArray,Array{T,1}}} where T<:AbstractFloat","page":"Estimation","title":"DSGE.find_density_bands","text":"find_density_bands(draws::Matrix, percents::Vector{T}; minimize::Bool=true) where T<:AbstractFloat\n\nReturns a 2 x cols(draws) matrix bands such that percent of the mass of draws[:,i] is above bands[1,i] and below bands[2,i].\n\nArguments\n\ndraws: Matrix of parameter draws (from Metropolis-Hastings, for example)\npercent: percent of data within bands (e.g. .9 to get 90% of mass within bands)\n\nOptional Arguments\n\nminimize: if true, choose shortest interval, otherwise just chop off lowest and highest (percent/2)\n\n\n\n\n\n","category":"method"},{"location":"estimation/#DSGE.find_density_bands-Union{Tuple{T}, Tuple{AbstractArray,T}} where T<:AbstractFloat","page":"Estimation","title":"DSGE.find_density_bands","text":"find_density_bands(draws::Matrix, percent::AbstractFloat; minimize::Bool=true)\n\nReturns a 2 x cols(draws) matrix bands such that percent of the mass of draws[:,i] is above bands[1,i] and below bands[2,i].\n\nArguments\n\ndraws: ndraws by nperiods matrix of parameter draws (from Metropolis-Hastings, for example)\npercent: percent of data within bands (e.g. .9 to get 90% of mass within bands)\n\nOptional Arguments\n\nminimize: if true, choose shortest interval, otherwise just chop off lowest and highest (percent/2)\n\n\n\n\n\n","category":"method"},{"location":"estimation/#DSGE.load_posterior_moments-Tuple{AbstractDSGEModel}","page":"Estimation","title":"DSGE.load_posterior_moments","text":"function load_posterior_moments(m; load_bands = true, include_fixed = false)\n\nLoad posterior moments (mean, std) of parameters for a particular sample, and optionally also load 5% and 95% lower and upper bands.\n\nKeyword Arguments\n\ncloud::ParticleCloud: Optionally pass in a cloud that you want to load the sample from. If the cloud is non-empty then the model object will only be used to find fixed indices and parameter tex labels\nload_bands::Bool: Optionally include the 5% and 95% percentiles for the sample of parameters in the returned df\ninclude_fixed::Bool: Optionally include the fixed parameters in the returned df\nexcl_list::Vector{Symbol}: List parameters by their key that you want to exclude from\n\nloading\n\nOutputs\n\ndf: A dataframe containing the aforementioned moments/bands\n\n\n\n\n\n","category":"method"},{"location":"estimation/#DSGE.moment_tables-Tuple{AbstractDSGEModel}","page":"Estimation","title":"DSGE.moment_tables","text":"moment_tables(m; percent = 0.90, subset_inds = 1:0, subset_string = \"\",\n    groupings = Dict{String, Vector{Parameter}}(), use_mode = false,\n    tables = [:prior_posterior_means, :moments, :prior, :posterior],\n    caption = true, outdir = \"\", verbose = :none)\n\nComputes prior and posterior parameter moments. Tabulates prior mean, posterior mean, and bands in various LaTeX tables. These tables will be saved in outdir if it is nonempty, or else in tablespath(m, \"estimate\").\n\nInputs\n\nm::AbstractDSGEModel: model object\n\nKeyword Arguments\n\npercent::AbstractFloat: the percentage of the mass of draws from Metropolis-Hastings included between the bands displayed in output tables.\nsubset_inds::AbstractRange{Int64}: indices specifying the draws we want to use\nsubset_string::String: short string identifying the subset to be appended to the output filenames. If subset_inds is nonempty but subset_string is empty, an error is thrown\ngroupings::Dict{String, Vector{Parameter}}: see ?parameter_groupings\nuse_mode::Bool: use the modal parameters instead of the mean in the priorposteriormeans table\ntables::Vector{Symbol}: which tables to produce\ncaption::Bool: whether to include table captions\noutdir::String: where to save output tables\nverbose::Symbol: desired frequency of function progress messages printed to standard out. One of :none, :low, or :high\n\n\n\n\n\n","category":"method"},{"location":"estimation/#DSGE.sample_λ-Union{Tuple{S}, Tuple{PoolModel{S},Array{S,2},Array{S,2}}, Tuple{PoolModel{S},Array{S,2},Array{S,2},Int64}} where S<:AbstractFloat","page":"Estimation","title":"DSGE.sample_λ","text":"sample_λ(m, pred_dens, θs, T = -1; parallel = true) where S<:AbstractFloat\nsample_λ(m, pred_dens, T = -1; parallel = true) where S<:AbstractFloat\n\nComputes and samples from the conditional density p(λt|θ, It, P) for particle in θs, which represents the posterior distribution. The sampled λ particles represent the posterior distribution p(λ{t|t} | It, P).\n\nIf no posterior distribution is passed in, then the function computes the distribution of λ_{t|t} for a static pool.\n\nInputs\n\nm::PoolModel{S}: PoolModel object\npred_dens::Matrix{S}: matrix of predictive densities\nθs::Matrix{S}: matrix of particles representing posterior distribution of θ\nT::Int64: final period for tempered particle filter\n\nwhere S<:AbstractFloat.\n\nKeyword Argument\n\nparallel::Bool: use parallel computing to compute and sample draws of λ\n\nOutputs\n\nλ_sample::Vector{Float64}: sample of draws of λs; together with (θ,λ) represents a joint density\n\n```\n\n\n\n\n\n","category":"method"},{"location":"estimation/#DSGE.moments-Tuple{ModelConstructors.Parameter}","page":"Estimation","title":"DSGE.moments","text":"moments(θ::Parameter)\n\nIf θ's prior is a RootInverseGamma, τ and ν. Otherwise, returns the mean and standard deviation of the prior. If θ is fixed, returns (θ.value, 0.0).\n\n\n\n\n\n","category":"method"},{"location":"estimation/#DSGE.posterior_table-Tuple{AbstractDSGEModel,Array{T,1} where T,Array{T,2} where T}","page":"Estimation","title":"DSGE.posterior_table","text":"posterior_table(m, post_means, post_bands; percent = 0.9, subset_string = \"\",\n    groupings = Dict{String, Vector{Parameter}}(), caption = true, outdir = \"\")\n\n\n\n\n\n","category":"method"},{"location":"estimation/#DSGE.prior_posterior_moments_table-Tuple{AbstractDSGEModel,Array{T,1} where T,Array{T,2} where T}","page":"Estimation","title":"DSGE.prior_posterior_moments_table","text":"prior_posterior_moments_table(m, post_means, post_bands; percent = 0.9,\n    subset_string = \"\", groupings = Dict{String, Vector{Parameter}}(),\n    caption = true, outdir = \"\")\n\nProduces a table of prior means, prior standard deviations, posterior means, and 90% bands for posterior draws.\n\n\n\n\n\n","category":"method"},{"location":"estimation/#DSGE.prior_posterior_table-Tuple{AbstractDSGEModel,Array{T,1} where T}","page":"Estimation","title":"DSGE.prior_posterior_table","text":"prior_posterior_table(m, post_values; subset_string = \"\",\n    groupings = Dict{String, Vector{Parameter}}(), use_mode = false,\n    caption = true, outdir = \"\")\n\nProduce a table of prior means and posterior means or mode.\n\n\n\n\n\n","category":"method"},{"location":"estimation/#DSGE.prior_table-Tuple{AbstractDSGEModel}","page":"Estimation","title":"DSGE.prior_table","text":"prior_table(m; subset_string = \"\", groupings = Dict{String, Vector{Parameter}}(),\n    caption = true, outdir = \"\")\n\n\n\n\n\n","category":"method"},{"location":"estimation/#DSGE.propagate_λ-Union{Tuple{T}, Tuple{T,Int64,PoolModel}, Tuple{T,Int64,PoolModel,Any}} where T<:AbstractFloat","page":"Estimation","title":"DSGE.propagate_λ","text":"propgate_λ(λvec, h, m, θvec) where T<:AbstractFloat\n\nPropagates a λ particle h periods forward.\n\nInputs\n\nλ::T: λ sample from (θ,λ) joint distribution\nh::Int64: forecast horizon\nm::PoolModel: PoolModel object\nθvec::Vector{T}: optional vector of parameters to update PoolModel\n\n```\n\n\n\n\n\n","category":"method"},{"location":"estimation/#SMC-Sampler-1","page":"Estimation","title":"SMC Sampler","text":"","category":"section"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"See here for the settings adjusting the SMC algorithm. To use these with a DSGE model object, either add them after the definition of a model object as a Setting or add them directly in the definition of the model type. For example, the following code sets the sampler as SMC and the number of particles used by SMC as 10,000:","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"m = Model990()\nm <= Setting(:sampling_method, :SMC)\nm <= Setting(:n_particles, 10000)","category":"page"},{"location":"estimation/#","page":"Estimation","title":"Estimation","text":"[1]: We document the details of implementing adaptive proposal densities and   parameter blocking in SMC.jl.","category":"page"},{"location":"forecast/#forecast-step-1","page":"Forecasting","title":"Forecasting","text":"","category":"section"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"CurrentModule = DSGE","category":"page"},{"location":"forecast/#Procedure-1","page":"Forecasting","title":"Procedure","text":"","category":"section"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"In the forecast step, we compute smoothed histories, forecast, compute shock decompositions, and compute impulse response functions (IRFs) for states, observables, shocks, and pseudo-observables. To run a forecast on one combination of input parameter type (e.g. modal parameters or full-distribution) and conditional type, call forecast_one.","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Main Steps:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Prepare forecast inputs: Add required output types, load data, and load draws of parameter vectors saved from the estimation step.\nCompute forecast outputs: Carry out desired combination of smoothing, forecasting, computing shock decompositions, and computing IRFs. See Forecast Outputs for a list of possible forecast outputs.\nSave forecast outputs: Save each forecast output as an array to its own file, along with some metadata.","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"DSGE.forecast_one","category":"page"},{"location":"forecast/#DSGE.forecast_one","page":"Forecasting","title":"DSGE.forecast_one","text":"forecast_one(m, input_type, cond_type, output_vars; df = DataFrame(),\n    subset_inds = 1:0, forecast_string = \"\", verbose = :low)\n\nCompute and save output_vars for input draws given by input_type and conditional data case given by cond_type.\n\nInputs\n\nm::AbstractDSGEModel: model object\ninput_type::Symbol: one of:\n\n  - `:mode`: forecast using the modal parameters only\n  - `:mean`: forecast using the mean parameters only\n  - `:init`: forecast using the initial parameter values only\n  - `:full`: forecast using all parameters (full distribution)\n  - `:subset`: forecast using a well-defined user-specified subset of draws\n\ncond_type::Symbol: one of:\n\n  - `:none`: no conditional data\n  - `:semi`: use \"semiconditional data\" - average of quarter-to-date\n    observations for high frequency series\n  - `:full`: use \"conditional data\" - semiconditional plus nowcasts for\n    desired observables\n\noutput_vars::Vector{Symbol}: vector of desired output variables. See ?forecast_one_draw.\n\nKeyword Arguments\n\ndf::DataFrame: Historical data. If cond_type in [:semi, :full], then the  final row of df should be the period containing conditional data. If not  provided, will be loaded using load_data with the appropriate cond_type\nsubset_inds::AbstractRange{Int64}: indices specifying the draws we want to use. If a more sophisticated selection criterion is desired, the user is responsible for determining the indices corresponding to that criterion. If input_type is not subset, subset_inds will be ignored\nforecast_string::String: short string identifying the subset to be appended to the output filenames. If input_type = :subset and forecast_string is empty, an error is thrown.\nverbose::Symbol: desired frequency of function progress messages printed to standard out. One of :none, :low, or :high.\n\nOutputs\n\nNone. Output is saved to files returned by get_forecast_output_files(m, input_type, cond_type, output_vars).\n\n\n\n\n\n","category":"function"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"For example, to do an unconditional forecast of states and observables using the modal parameters, call:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"m = AnSchorfheide()\nforecast_one(m, :mode, :none, [:forecaststates, forecastobs])","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Full-Distribution Forecasts:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Full-distribution forecasts are computed in blocks. The size of each block defaults to 5000 draws (before thinning by get_setting(m, :forecast_jstep)), but can be set using the :forecast_block_size Setting. For each block, draws are read in on the originator process, then computation proceeds in parallel using pmap. When all draws in the block are finished, the forecast outputs are reassembled on the originator process and appended to the HDF5 dataset in their respective output files.","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"To fully take advantage of the parallelization, the user is responsible for adding processes before calling forecast_one, either by calling addprocs or using one of the functions defined in ClusterManagers.jl. For example, to run a full-distribution unconditional forecast using 10 processes:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"my_procs = addprocs(10)\n@everywhere using DSGE\n\nm = AnSchorfheide()\nforecast_one(m, :full, :none, [:forecaststates, forecastobs])\n\nrmprocs(my_procs)","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Notice that it is necessary to load DSGE on all processes using @everywhere using DSGE before calling forecast_one. It is also sometimes necessary to load OrderedCollections on all processes using @everywhere using OrderedCollections.","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"By default, full-distribution forecasts start from the first block. However, if you want to start the forecast from a later block, you can also do so. For example:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"m <= Setting(:forecast_start_block, 2,\n    \"Block at which to resume forecasting (possibly null)\")","category":"page"},{"location":"forecast/#Forecast-Outputs-1","page":"Forecasting","title":"Forecast Outputs","text":"","category":"section"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"A forecast output (i.e. an output_var) is a combination of what we call a \"product\" and a \"class\". The possible classes are states (:states), observables (:obs), pseudo-observables (:pseudo), and standardized (:stdshocks) and unstandardized shocks (:shocks). The possible forecast products are:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Smoothed histories (:hist): use the smoother specified by forecast_smoother(m) to get smoothed histories of each class.\nForecasts (:forecast): iterate the state space forward from the last filtered state, either using a specified set of shock innovations or by drawing these from a distribution. Forecasts in which we enforce the zero lower bound are denoted as :bddforecast.\nShock decompositions (:shockdec): starting from an initial state of zero, iterate the state space forward from the first historical period up through the last forecast horizon. Use the smoothed historical shocks for one shock at a time during the historical periods and no shocks during the forecast periods.\nDeterministic trends (:dettrend): iterate the state space forward from first historical state up through the last forecast horizon without any shocks.\nTrends (:trend): for each class, just the constant term in that class's equation, i.e. the CCC vector from the transition equation for states, the DD vector from the measurement equation for observables, and the DD_pseudo vector from the pseuodo-measurement equation for pseudo-observables.\nIRFs (:irf): see Impulse response. Our IRFs are in response to a shock of size -1 standard deviation.","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"An output_var is then just a Symbol with a product and class concatenated, e.g. :histstates for smoothed historical states.","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"It is not necessary to compute all forecast outputs in one call to forecast_one. Which steps are run depends on which output_vars are passed in.","category":"page"},{"location":"forecast/#Preparing-Forecast-Inputs-1","page":"Forecasting","title":"Preparing Forecast Inputs","text":"","category":"section"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Adding Required output_vars:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"This step is done by add_requisite_output_vars:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"If :forecast<class> is in output_vars, then :bddforecast<class> is also added. Hence we always forecast both with and without enforcing the ZLB.\nIf :shockdec<class> is in output_vars, then :dettrend<class> and :trend<class> are also added. This is because to plot shock decompositions, we also need the trend and the deterministic trend.","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Loading Data:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"This is done the usual way, using load_data with the appropriate cond_type.","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Loading Draws:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"By default, the draws are loaded from the file whose path is given by get_forecast_input_file. However, you can override the default input file for a given input type by adding entries to the Dict{Symbol, ASCIIString} returned from forecast_input_file_overrides(m). For example:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"overrides = forecast_input_file_overrides(m)\noverrides[:mode] = \"path/to/input/file.h5\"","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Note that load_draws expects an HDF5 dataset called either params (for input_type in [:mode, :mean]) or mhparams (for input_type in [:full, :subset]).","category":"page"},{"location":"forecast/#Computing-Forecast-Outputs-1","page":"Forecasting","title":"Computing Forecast Outputs","text":"","category":"section"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Smoothing:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Smoothing is necessary if either:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"You explicitly want the smoothed histories, or\nYou want to compute shock decompositions or deterministic trends, which use the smoothed historical shocks","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"It is not necessary to keep track of these cases, however - forecast_one will deduce from the specified output_vars whether or not it is necessary to filter and smooth in order to produce your output_vars.","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Forecasting:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Forecasting begins from the last filtered historical state, which is obtained from the Kalman filter. forecast accepts a keyword argument enforce_zlb, which indicates whether to enforce the zero lower bound. If enforce_zlb = true, then if in a given period, the forecasted interest rate goes below forecast_zlb_value(m), we solve for the interest rate shock necessary to push it up to the ZLB. A forecast in which the ZLB is enforced corresponds to the product :bddforecast.","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Shock Decompositions, Deterministic Trends, and Trends:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Since shock decompositions have an additional dimension (e.g. nstates x nperiods x nshocks for a single draw of state shock decompositions, compared to nstates x nperiods for a single draw of forecasted states), we usually wish to truncate some periods before returning. This behavior is governed by the Settings :shockdec_starttdate and :shockdec_enddate, which are of type Nullable{Date}.","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Deterministic trends are also saved only for date_shockdec_start(m) and date_shockdec_end(m). Trends are not time-dependent.","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Note that shock decompositions are memory intensive. For a typical forecast of Model1002 without shock decompositions, only 1-2GB of memory are needed. For a forecast with shock decompositions, roughtly 14-16GB will be needed.","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Impulse Response Functions:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Like shock decompositions, IRFs have three dimensions (e.g. nstates x nperiods x nshocks) for each draw.","category":"page"},{"location":"forecast/#Saving-Forecast-Outputs-1","page":"Forecasting","title":"Saving Forecast Outputs","text":"","category":"section"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Forecast outputs are saved in the location specified by get_forecast_output_files(m), which is typically a subdirectory of saveroot(m). Each output_var is saved in its own JLD file, which contains the following datasets:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"arr::Array: actual array of forecast outputs. For trends, this array is of size ndraws x nvars. For histories, forecasts, and deterministic trends, it is ndraws x nvars x nperiods. For shock decompositions and IRFs, it is ndraws x nvars x nperiods x nshocks. (In all of these, nvars refers to the number of variables of the output class.)\ndate_indices::Dict{Date, Int}: maps Dates to their indices along the nperiods dimension of arr. Not saved for IRFs.\n<class>_names::Dict{Symbol, Int}: maps names of variables of the output class (e.g. :OutputGap) into their indices along the nvars dimension of arr.\n<class>_revtransforms::Dict{Symbol, Symbol}: maps names of variables to the names of the reverse transforms (from model units into plotting units) associated with those variables. For example, pseudoobservable_revtransforms[:π_t] = :quartertoannual.\nshock_names::Dict{Symbol, Int}: for shock decompositions and IRFs only, maps names of shocks into their indices along the nshocks dimension of arr.","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"Some helpful functions for getting file names, as well as reading and writing forecast outputs, include:","category":"page"},{"location":"forecast/#","page":"Forecasting","title":"Forecasting","text":"get_forecast_input_file\nget_forecast_filename\nget_forecast_output_files\nwrite_forecast_outputs\nwrite_forecast_block\nwrite_forecast_metadata\nread_forecast_metadata\nread_forecast_output","category":"page"},{"location":"irf/#Impulse-Responses-1","page":"Impulse Response Functions","title":"Impulse Responses","text":"","category":"section"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"CurrentModule = DSGE","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"We provide many different types of impulse responses for DSGEs, VARs, and DSGE-VARs. The forecast step allows the user to automatically compute \"structural\" impulse responses specifically for DSGEs, but for some purposes, a user may just want impulse responses without having to compute any other quantities. We provide this functionality with the impulse_responses function. See the end of this page for the docstrings of all available impulse response functions.","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"We overload impulse_responses to cover specific use cases. For any AbstractDSGEModel, we can compute the impulse responses over a specified horizon for all endogenous state variables, observables, and pseudo-observables by running","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"m = AnSchorfheide()\nsystem = compute_system(m)\nhorizon = 40\nstates_irf, obs_irf, pseudo_irf = impulse_response(system, horizon)\n``\n\nFor an `AbstractRepModel` (a subtype of `AbstractDSGEModel` for representative\nagent models), we can also grab the impulse responses by running\n\n","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"julia statesirf, obsirf, pseudoirf = impulseresponse(m, system)","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"\nThis use case requires the user to add a setting\nunder the key `:impulse_response_horizons`, which is\nset by default to 40.\n\n\nIf a user wants to specify a subset of the exogenous shocks\nand the size of those shocks, the user can run\n","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"julia shocknames = [:gsh, :bsh] shockvalues = [1.0, 1.0] impulseresponses(m, system, horizon, shocknames, shock_values)","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"\nFor the response of an endogenous state or observable to a specific shock,\n","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"julia shockname  =  :gsh varname = :obsgdp varvalue = 0. impulseresponses(m, system, horizon, shockname , varname, var_value)","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"\n## DSGE Impulse Responses\nThere are two categories of impulse responses for DSGEs provided by DSGE.jl*.\nIt is easy to distinguish them by examining the state space form of a DSGE model (see [Solving](@ref solving-dsge-doc)):","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"math \\begin{align} st &= T s{t-1} + R \\epsilont + C & \\epsilont &\\sim N(0, Q) & \\mathrm{(transition)} \\\nyt &= Z st + D + ut & ut &\\sim N(0, E) & \\mathrm{(measurement)} \\end{align}","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"Impulse responses in the first category are \"structural\" impulse responses, which are\nthe response of states and observables to the exogenous structural shocks ``\\epsilon_t``.\n\n\nImpulse responses in the second category are \"observables-identified\" impulse responses.\nFirst, we may suppose that the measurement equation generically follows\n","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"math \\begin{align} yt &= F(st) + \\eta_t, \\end{align}","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"\nwhere ``F(\\cdot)`` is some function of the unknown states ``s_t``,\nand ``\\eta_t`` are random innovation to the observables ``y_t``.\nBy innovations, we mean that these random variables are potentially\nendogenous shocks, i.e. shocks which do not have a causal interpretation.\nAn \"observables-identified\" impulse response specifies\na certain response of ``y_t`` to the innovations ``\\eta_t``,\nand uses this response to identify the underlying structural shocks which are\nconsistent with these innovations. A DSGE identifies these innovations\nusing the state space form of a DSGE.\n\nWe provide three types of \"observables-identified\" impulse responses for DSGEs.\n\n- Short-Run Cholesky\n- Long-Run Cholesky\n- Maximizing Explained Cyclical Variance\n\nWe document the details of the identification in the docstrings of these impulse\nresponse functions. For the first two types of impulse responses,\nsearch the docstrings at the end of the page for\n","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"function impulseresponses(system::System{S}, horizon::Int, permutemat::AbstractMatrix{T},                            shocks::AbstractVector{S} = Vector{S}(undef, 0);                            restriction::Symbol = :shortrun, flipshocks::Bool = false,                            get_shocks::Bool = false) where {S <: Real, T <: Number}","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"\nFor the third type of impulse response, search for\n","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"function impulseresponses(system::System{S}, horizon::Int, frequencyband::Tuple{S,S},                            nobsshock::Int; flipshocks::Bool = false,                            getshocks::Bool = false) where {S <: Real}","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"\n\n## VAR Impulse Responses\nWhile we have not yet implemented a VAR model, we do have impulse\nresponses often used on VARs because of DSGE-VARs. Consider the VAR\n","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"math yt = Xt \\beta + \\eta_t,","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"\nwhere ``X_t`` is a matrix of the lags of ``y_t``, ``\\beta`` are the\nVAR coefficients, and ``\\eta_t \\sim N(0, \\Omega)`` are the innovations\nto observables.\n\nWe provide three types of impulse responses, each of which\nprovide a different way of identifying orthogonalized shocks\nfrom the innovations.\n\n- Short-Run Cholesky\n- Long-Run Cholesky\n- Maximizing Explained Cyclical Variance\n\nThese impulse responses are named the same as the observables-identified\nimpulse responses for DSGEs because they are considering the same\nresponse of observables to the innovations.\nHowever, the treatment of identification is different when using a VAR\nbecause the mathematical structure of a VAR is not the same as a DSGE's.\nAs a result, there are slight differences between these impulse responses\nand the observables-identified DSGE impulse responses.\n\n","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"impulseresponses(β, Σ, nobsshock, horizon, shocksize = 1;     method = :cholesky, flipshocks = false, useintercept = true,     frequency_band = (2π/32, 2π/6)) where {S<:Real}","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"\n\n\n## DSGE-VAR Impulse Responses\n\nThere are two types of impulse responses we can compute for a DSGE-VAR.\nFor both types, we first draw from the posterior distributions of the ``VAR(p)`` coefficients\nand innovations variance-covariance matrix, where ``p`` is\nthe number of lags. With these draws, we can do one of two things:\n\n1. Compute the VAR impulse response implied by the draws.\n2. Use the DSGE's structural impact response (i.e. the first period of an impulse response)\n   to identify a mapping from the (endogenous) innovations in the VAR\n   to the structural shocks of the DSGE.\n\nThe first type of impulse response uses the same code as the VAR impulse responses\nonce we compute the coefficients and innovations variance-covariance matrix.\nWe call the second type of impulse responses\n\"DSGE-VAR rotation impulse responses\" because we effectively use the DSGE\nto identify a rotation matrix.\n\nFor the first type of impulse response, see\n","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"function impulseresponses(m::AbstractDSGEVARModel{S}, data::AbstractArray{S}, method::Symbol,                            nobsshock::Int; horizon::Int = 0 ,useintercept::Bool = false,                            flip_shocks::Bool = false, verbose::Symbol = :none) where {S <: Real}","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"function impulseresponses(m::AbstractDSGEVARModel{S}, method::Symbol,                            nobsshock::Int; horizon::Int = 0 ,useintercept::Bool = false,                            flip_shocks::Bool = false, verbose::Symbol = :none) where {S <: Real}","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"\nThe second function is for the specific case when ``\\lambda = \\infty``, where the data does not matter for the impulse response.\n\nFor the second type of impulse responses, see\n","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"function impulseresponses(m::AbstractDSGEVARModel{S}, data::AbstractArray{S},     X̂::Matrix{S} = Matrix{S}(undef, 0, 0);     horizon::Int = 0, MM::Matrix{S} = Matrix{S}(undef, 0, 0),     flipshocks::Bool = false, draw_shocks::Bool = false,     verbose::Symbol = :none) where {S <: Real}","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"\n## Wrappers for Impulse Response Functions\n\nThe `forecast_one` function provides a wrapper for computing\nstructural DSGE impulse responses when drawing from a distribution of parameters\nand for saving these impulse responses as `MeansBands` objects\n(see [Computing Means and Bands](@ref means-bands)).\n\nHowever, `forecast_one` currently cannot\ncompute observables-identified DSGE impulse responses, VAR impulse responses,\nor DSGE-VAR impulse responses, and we do not plan on modifying `forecast_one`\nto make it possible to do so. Instead, we provide three wrapper functions\nspecifically for computing means and bands for these impulse responses.\nPlease see their docstrings for details. The first wrapper is for\nobservables-identified DSGE impulse responses, and the second two\nare for DSGE-VAR impulse responses. The first one applies generically to\na DSGE-VAR with any prior weight ``\\lambda``, but the second one\nis a convenience wrapper for the case of ``\\lambda = \\infty``,\nwhich is equivalent to computing the impulse responses of the\nVAR approximation to a DSGE.\n\nFor observables-identified DSGE impulse responses, find\n","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"function impulseresponses(m::AbstractDSGEModel, paras::Matrix{S},                            inputtype::Symbol, method::Symbol, nobsshock::Int,                            outputvars::Vector{Symbol} =                            [:irfstates, :irfobs, :irfpseudo]; parallel::Bool = false,                            permutemat::Matrix{S} = Matrix{Float64}(undef,0,0),                            frequencyband::Tuple{S,S} = (2π/32, 2π/6),                            flipshocks::Bool = false,                            densitybands::Vector{Float64} = [.5, .6, .7, .8, .9],                            createmeansbands::Bool = false, testmeansbands::Bool = false,                            minimize::Bool = true,                            forecaststring::String = \"\",                            dorevtransform::Bool = false,                            verbose::Symbol = :high) where {S<:Real}","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"\nFor DSGE-VAR impulse responses, find\n","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"function impulseresponses(m::AbstractDSGEVARModel{S}, paras::Matrix{S},                            data::Matrix{S}, inputtype::Symbol, method::Symbol;                            parallel::Bool = false,                            frequencyband::Tuple{S,S} = (2π/32, 2π/6),                            nobsshock::Int = 1, drawshocks::Bool = false,                            flipshocks::Bool = false,                            densitybands::Vector{Float64} = [.5, .6, .7, .8, .9],                            createmeansbands::Bool = false, testmeansbands::Bool = false,                            minimize::Bool = true,                            forecast_string::String = \"\",                            verbose::Symbol = :high) where {S<:Real}","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"\n\nFor the impulse response of a VAR approximation to a DSGE, find\n","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"function impulseresponses(m::AbstractDSGEModel, paras::Union{Vector{S}, Matrix{S}},                            inputtype::Symbol, method::Symbol,                            lags::Int, observables::Vector{Symbol},                            shocks::Vector{Symbol},                            nobsshock::Int; parallel::Bool = false,                            frequencyband::Tuple{S,S} = (2π/32, 2π/6),                            flipshocks::Bool = false,                            useintercept::Bool = false,                            densitybands::Vector{Float64} = [.5, .6, .7, .8, .9],                            createmeansbands::Bool = false,                            minimize::Bool = true,                            forecaststring::String = \"\",                            verbose::Symbol = :high) where {S<:Real}","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"\n\n\n## Docstrings\n","category":"page"},{"location":"irf/#","page":"Impulse Response Functions","title":"Impulse Response Functions","text":"@docs DSGE.impulse_responses ```","category":"page"},{"location":"means_bands/#means-bands-1","page":"Computing Means and Bands","title":"Computing Means and Bands","text":"","category":"section"},{"location":"means_bands/#","page":"Computing Means and Bands","title":"Computing Means and Bands","text":"CurrentModule = DSGE","category":"page"},{"location":"means_bands/#Procedure-1","page":"Computing Means and Bands","title":"Procedure","text":"","category":"section"},{"location":"means_bands/#","page":"Computing Means and Bands","title":"Computing Means and Bands","text":"After running a full-distribution forecast, we are often interested in finding means and density bands of the various forecast outputs. This will allow us to plot our estimation of the full distribution of the forecast outputs.","category":"page"},{"location":"means_bands/#","page":"Computing Means and Bands","title":"Computing Means and Bands","text":"Main Steps:","category":"page"},{"location":"means_bands/#","page":"Computing Means and Bands","title":"Computing Means and Bands","text":"Load data: Load and transform data, population data, and population forecast (required for forecast output transformations)\nRead in forecast outputs: Read in the outputs saved by forecast_one, one variable (e.g. one observable) and one output_type at a time.\nTransform forecast outputs: If necessary, use reverse_transform to apply transformations specified in the Observable or PseudoObservable type to the given forecast output series.\nCompute means and bands: Compute the means and density bands of the forecast output.\nWrite to file: For each output_var, Write a MeansBands object (see The MeansBands Type below) to the file specified by get_meansbands_output_file.","category":"page"},{"location":"means_bands/#","page":"Computing Means and Bands","title":"Computing Means and Bands","text":"Computing means and bands is done by calling compute_meansbands. If desired, you can also write your computed means and bands as matrices by calling meansbands_matrix_all.","category":"page"},{"location":"means_bands/#","page":"Computing Means and Bands","title":"Computing Means and Bands","text":"For example, to compute means and bands for an unconditional, full-distribution forecast of states and observables:","category":"page"},{"location":"means_bands/#","page":"Computing Means and Bands","title":"Computing Means and Bands","text":"m = AnSchorfheide()\ncompute_meansbands(m, :mode, :none, [:forecaststates, forecastobs])","category":"page"},{"location":"means_bands/#","page":"Computing Means and Bands","title":"Computing Means and Bands","text":"compute_meansbands","category":"page"},{"location":"means_bands/#DSGE.compute_meansbands","page":"Computing Means and Bands","title":"DSGE.compute_meansbands","text":"compute_meansbands(m, input_type, cond_type, output_vars; forecast_string = \"\",\n    verbose = :low, kwargs...)\n\ncompute_meansbands(m, input_type, cond_type, output_var, df; forecast_string = \"\",\n    population_data = DataFrame(), population_forecast = DataFrame(),\n    verbose = :none, kwargs...)\n\ncompute_meansbands(m, input_type, cond_type, output_var, var_name, df;\n    forecast_string = \"\", population_data = DataFrame(),\n    population_forecast = DataFrame(), verbose = :low, kwargs...)\n\nCompute means and bands for pseudo-observables, observables, and shocks, and write the results to a file. Other methods are for one output_var and one var_name respectively.\n\nKeyword Arguments\n\nforecast_string::String: forecast identifier string (the value \"fcid=value\" in the forecast output filename). Required when input_type == :subset\ndensity_bands::Vector{Float64}: a vector of percent values (between 0 and 1) for which to compute density bands\nminimize::Bool: if true, choose shortest interval, otherwise just chop off lowest and highest (percent/2)\nverbose: level of error messages to be printed to screen. One of :none, :low, :high\n\n\n\n\n\n","category":"function"},{"location":"means_bands/#The-MeansBands-Type-1","page":"Computing Means and Bands","title":"The MeansBands Type","text":"","category":"section"},{"location":"means_bands/#","page":"Computing Means and Bands","title":"Computing Means and Bands","text":"MeansBands","category":"page"},{"location":"means_bands/#DSGE.MeansBands","page":"Computing Means and Bands","title":"DSGE.MeansBands","text":"mutable struct MeansBands\n\nStores the means and bands of results for a particular set of outputs from the forecast step.\n\nSpecifically, forecasts can be made for any element in the Cartesian product of 4 sets:\n\ninput_type: some subset of the parameter draws from the estimation step. See forecast_one for all possible options.\ncond_type: conditional type. See forecast_one for all possible options.\nproduct: a particular result computed in the forecast. This could be one of the following:\n\n  - `hist`: smoothed histories\n  - `forecast`: forecasted values\n  - `shockdec`: shock decompositions\n  - `irf`: impulse responses\n\nvariable class: the category in which a particular variable, like :y_t, falls. Options are:\n\n  - `state`: state (from `m.endogenous_states` or `m.endogenous_states_augmented`)\n  - `obs`: observable (from `m.observables`)\n  - `pseudo`: pseudoobservable (from `pseudo_measurement` equation)\n  - `shock`: shock (from `m.exogenous_shocks`)\n\nNote that the Cartesian product (product x class) is the set of options for output_vars in the forecast_one function signature.\n\nFields\n\nmetadata::Dict{Symbol,Any}: Contains metadata keeping track of the input_type, cond_type, product (history, forecast, shockdec, etc), and variable class (observable, pseudoobservable, state, etc) stored in this MeansBands structure.\nmeans::DataFrame: a DataFrame of the mean of the time series\nbands::Dict{Symbol,DataFrame}: a Dict mapping variable names to DataFrames containing confidence bands for each variable. See find_density_bands for more information.\n\n\n\n\n\n","category":"type"},{"location":"altpolicy/#Alternative-Policies-1","page":"Alternative Policies","title":"Alternative Policies","text":"","category":"section"},{"location":"altpolicy/#","page":"Alternative Policies","title":"Alternative Policies","text":"CurrentModule = DSGE","category":"page"},{"location":"altpolicy/#Procedure-1","page":"Alternative Policies","title":"Procedure","text":"","category":"section"},{"location":"altpolicy/#","page":"Alternative Policies","title":"Alternative Policies","text":"This section describes forecasting under an alternative monetary policy rule. That is:","category":"page"},{"location":"altpolicy/#","page":"Alternative Policies","title":"Alternative Policies","text":"Filtering and smoothing is done under the historical monetary policy rule, i.e. the one defined in the eqcond method for the given model.\nBefore forecasting, the state space matrices are recomputed under the alternative policy rule.\nForecasts and IRFs are computed under the alternative rule.","category":"page"},{"location":"altpolicy/#","page":"Alternative Policies","title":"Alternative Policies","text":"Note that shock decompositions (and the two associated products, trends and deterministic trends) cannot currently be computed under an alternative policy.","category":"page"},{"location":"altpolicy/#","page":"Alternative Policies","title":"Alternative Policies","text":"The user defines some instance of the AltPolicy type (described below) and sets it as the value for the :alternative_policy Setting. Then the function calls made to forecast and compute means and bands remain the same as usual (see Forecasting and Computing Means and Bands).","category":"page"},{"location":"altpolicy/#","page":"Alternative Policies","title":"Alternative Policies","text":"For example, suppose you have defined the functions taylor93_eqcond and taylor93_solve corresponding to Taylor (1993)'s proposed monetary policy rule. Then you can run:","category":"page"},{"location":"altpolicy/#","page":"Alternative Policies","title":"Alternative Policies","text":"m = AnSchorfheide()\nm <= Setting(:alternative_policy, AltPolicy(:taylor93, taylor93_eqcond, taylor93_solve))\nforecast_one(m, :mode, :none, [:forecastobs, :forecastpseudo])\ncompute_meansbands(m, :mode, :none, [:forecastobs, :forecastpseudo])","category":"page"},{"location":"altpolicy/#The-AltPolicy-Type-1","page":"Alternative Policies","title":"The AltPolicy Type","text":"","category":"section"},{"location":"altpolicy/#","page":"Alternative Policies","title":"Alternative Policies","text":"DSGE.AltPolicy","category":"page"},{"location":"altpolicy/#DSGE.AltPolicy","page":"Alternative Policies","title":"DSGE.AltPolicy","text":"mutable struct AltPolicy\n\nType defining an alternative policy rule.\n\nFields\n\nkey::Symbol: alternative policy identifier\neqcond::Function: a version of DSGE.eqcond which computes the equilibrium condition matrices under the alternative policy. Like DSGE.eqcond, it should take in one argument of mutable struct AbstractDSGEModel and return the Γ0, Γ1, C, Ψ, and Π matrices.\nsolve::Function: a version of DSGE.solve which solves the model under the alternative policy. Like DSGE.solve, it should take in one argument of mutable struct AbstractDSGEModel and return the TTT, RRR, and CCC matrices.\nforecast_init::Function: a function that initializes forecasts under the alternative policy rule. Specifically, it accepts a model, an nshocks x n_forecast_periods matrix of shocks to be applied in the forecast, and a vector of initial states for the forecast. It must return a new matrix of shocks and a new initial state vector. If no adjustments to shocks or initial state vectors are necessary under the policy rule, this field may be omitted.\ncolor::Colorant: color to plot this alternative policy in. Defaults to blue.\nlinestyle::Symbol: line style for forecast plots under this alternative policy. See options from Plots.jl. Defaults to :solid.\n\n\n\n\n\n","category":"type"},{"location":"scenarios/#Alternative-Scenarios-1","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"","category":"section"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"CurrentModule = DSGE","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"An alternative scenario is a set of forecasted paths for a subset of observables (\"targets\"), along with a subset of shocks (\"instruments\") which get the model to hit those paths. Each scenario has a description which tells a story about the scenario targets and instruments – for example, \"High Spreads\" or \"Persistent Consumer Optimism\".","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"In an alternative scenario, we first solve for the values for specific shocks at specific horizons that create the paths of the observables imposed in the scenario. Only then do we run a regular forecast, imposing the path of shocks we just solved for and letting these propagate through the economy. (Compare to adding more periods of conditional data – in this case, the model is free to use as many shocks as it wants to explain the deviation of the conditional data from trend in these additional periods.) This allows us to answer questions like:","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"Suppose a spread shock hits tomorrow, increasing spreads by 50 basis points. How large of a spread shock did this correspond to it? How does the economy respond? Suppose it is a discount factor shock instead.\nSuppose a discount factor shock hits for the next two quarters, causing a 100 basis point increase in spreads for two quarters, then reverting to zero (increase from baseline).  How does the economy respond?","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"We will conceive of all alternative scenario forecasts as occuring in \"deviations from baseline forecast\". That is, if the scenario consists of spreads increasing by 50 basis points, then we want this to be independent of the underlying model and baseline forecast, such that in our own forecast, spreads actually are 50 basis points above our baseline forecast.","category":"page"},{"location":"scenarios/#Basic-Scenarios-1","page":"Alternative Scenarios","title":"Basic Scenarios","text":"","category":"section"},{"location":"scenarios/#The-Scenario-Type-1","page":"Alternative Scenarios","title":"The Scenario Type","text":"","category":"section"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"The Scenario type encodes the basic information needed to forecast an alternative scenario. For now, we'll consider the following fields of the Scenario type:","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"key::Symbol: scenario identifier, appearing in file names\ndescription::String: longer name, e.g. \"High Spreads\"\ntarget_names::Vector{Symbol}: names of the observables targeted in this scenario. For the High Spreads scenario, this would just be [:obs_spread]. These observable names should correspond to keys in m.observables\ninstrument_names::Vector{Symbol}: names of the shocks used to hit the target paths, which should correspond to keys in m.exogenous_shocks. There must be at least as many instruments as there are targets. If this field is the empty array, then all model shocks will be used\ntargets::DataFrame: contains the specific target values, given in deviations from some baseline forecast. No :date field is required, since the scenario is assumed to begin in the first forecasted period of the model\ninstruments::DataFrame: initially empty DataFrame, which is populated after backing out the necessary shocks to hit the targets\nvintage::String: scenario vintage in yymmdd format, which can be different from the data vintage\nn_draws::Int: number of scenario draws. A scenario draw is one set of target paths. Often, multiple draws of target paths will be associated with one scenario name, which collectively make up a forecast distribution for the particular scenario. This field is usually initialized to 0 and then updated upon reading in the target draws","category":"page"},{"location":"scenarios/#Setting-Up-Input-Target-Paths-1","page":"Alternative Scenarios","title":"Setting Up Input Target Paths","text":"","category":"section"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"If you only want to forecast one scenario draw, it is sufficient to create an instance of the Scenario type and hard-code in the target paths. However, when forecasting multiple draws, it is convenient to load target draws from a file.","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"This file's name should be of the form inpath(m, \"scenarios\", <key>_<vintage>.jld) and should contain the following datasets:","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"arr::Array{Float64, 3}: array of target values of size ndraws x ntargets x horizon\ntarget_indices::OrderedDict{Symbol, Int}: maps target names to their indices along the ntargets dimension","category":"page"},{"location":"scenarios/#Forecasting-Scenarios-1","page":"Alternative Scenarios","title":"Forecasting Scenarios","text":"","category":"section"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"Forecasting scenarios is similar to running a normal full-distribution forecast, with some exceptions:","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"All draws of a particular scenario are forecasted under the modal parameters.\nSmoothed histories, shock decompositions, and IRFs are not supported.\nWe zero out the entries in the Q matrix (the variance-covariance of the shocks epsilon_t) corresponding to shocks which are not scenario instruments.\nForecasting is done in deviations from baseline. That is, let s^a_t and s^b_t be the state vectors under the alternative and baseline scenarios respectively, and define y^a_t and y^b_t analogously for observable vectors. Then the state space in deviations is\nbeginalign*\ns^a_t - s^b_t = T(s^a_t-1 - s^b_t-1) + R epsilon_t  mathrm(transition) \ny^a_t - y^b_t = Z(y^a_t - y^b_t)  mathrm(measurement)\nendalign*","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"The main function to run is forecast_scenario, which is similar in spirit to forecast_one. forecast_scenario loads the modal parameters for the model and calls forecast_scenario_draw, which does the following for each input draw:","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"Call load_scenario_targets! to read the ith scenario draw from the input file\nFilter and smooth to back out the necessary shocks, treating the targeted paths like data\nForecast paths for all variables (not just the targeted observables) using the smoothed shocks\nCheck that the forecasted observables match the targets and return","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"When all draws for the scenario have been forecasted, the forecasted observables and pseudo-observables are written to the files given by get_scenario_output_files.","category":"page"},{"location":"scenarios/#Computing-Means-and-Bands-1","page":"Alternative Scenarios","title":"Computing Means and Bands","text":"","category":"section"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"Computing means and bands is carried out by scenario_means_bands, which is likewise similar to compute_meansbands for regular forecasts. The default output_vars passed into scenario_means_bands are","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"output_vars = [:forecastutobs, :forecastobs, :forecast4qobs,\n               :forecastutpseudo, :forecastpseudo, :forecast4qpseudo]","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"The product :forecastut refers to untransformed forecasts, i.e. forecasts in model units (in deviations from baseline).","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"Since these forecasts are given in deviations from baseline, different transformations are used than in the usual case. These don't add back population growth to per-capita variables and approximate annualizing log differences (exponentiating and raising to the fourth power) by multiplying by four. The mapping from usual to scenario transformation is given by get_scenario_transform.","category":"page"},{"location":"scenarios/#Additional-Features-1","page":"Alternative Scenarios","title":"Additional Features","text":"","category":"section"},{"location":"scenarios/#Other-Scenario-Fields-1","page":"Alternative Scenarios","title":"Other Scenario Fields","text":"","category":"section"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"The remaining fields in the Scenario type are used to forecast scenarios using additional bells and whistles:","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"shock_scaling::Float64: after filtering and smoothing shocks, multiply them by shock_scaling before forecasting. Defaults to 1.0.\ndraw_states::Bool: if true, use the simulation smoother rather than the Kalman smoother to smooth shocks. This generates uncertainty around the target paths. Defaults to false.\naltpolicy::AltPolicy: solve for shocks under the historical policy rule, then switch to the alternative policy (see Alternative Policies) before forecasting. Defaults to AltPolicy(:historical, eqcond, solve).","category":"page"},{"location":"scenarios/#SwitchingScenarios-1","page":"Alternative Scenarios","title":"SwitchingScenarios","text":"","category":"section"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"Suppose you want to simulate a set of scenario draws which all start from some default scenario, switch to an alternative scenario (which we call the original scenario) with some probability in each period, and then revert back to the default scenario with some probability. This functionality is encoded in the SwitchingScenario type, which has the following fields:","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"key::Symbol: identifier, not the same as the original scenario's key\ndescription::Symbol: defaults to the original scenario's description\nvintage::String: defaults to the original scenario's vintage\noriginal::SingleScenario\ndefault::SingleScenario\nprobs_enter::Vector{Float64}: gives the probability of leaving the default scenario and entering the original scenario in each period\nprobs_exit::Vector{Float64}: gives the probability of leaving the original scenario and reverting to the default scenario in each period","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"SingleScenario is the abstract supertype of both Scenario and SwitchingScenario.","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"SwitchingScenarios cannot be forecasted like ordinary Scenarios. Instead, after simulating draws from both the original and default scenarios, you call simulate_switching to use these already-forecasted draws. For each draw i of the original scenario:","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"Randomly select a draw j of the default scenario.\nIn each period t (beginning in period 1), determine whether to switch to the original scenario with probability probs_enter[t]. If remaining in the default scenario, use the period t forecasted values of the jth default scenario draw.\nSuppose the last period in the default scenario is t0. Then for each subsequent period t0 + h, decide whether to revert to the default scenario with probability probs_exit[t0 + h]. If remaining in the original scenario, use the period h (not t0 + h) forecasted values of the ith original scenario draw.\nLet t1 be the last period in the original scenario. Then for each remaining period t1 + h, use the period t1 + h forecasted values of the jth default scenario draw.","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"simulate_switching saves simulated draws in the same format as forecast_scenario. Transforming and computing means and bands for SwitchingScenarios using scenario_means_bands is the same as for regular Scenarios.","category":"page"},{"location":"scenarios/#Aggregating-Multiple-Scenarios-1","page":"Alternative Scenarios","title":"Aggregating Multiple Scenarios","text":"","category":"section"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"Finally, we are sometimes interested in aggregating the forecast draws from multiple scenarios. We define the ScenarioAggregate type, which has the following fields:","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"key::Symbol\ndescription::String\nscenarios::Vector{AbstractScenario}: vector of component scenarios, some of which might be themselves ScenarioAggregates\nsample::Bool: indicates whether to\nproportions::Vector{Float64}: vector of relative scenario proportions\ntotal_draws::Int: desired final number of draws\nreplace::Bool: indicates whether to sample with replacement\nvintage::String","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"In addition to the default constructor, there are two more ScenarioAggregate constructors, corresponding to the two possible values of sample. The key, description, vector of component scenarios, and vintage are always specified.","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"sample = false: No additional fields are required for this constructor, as the component scenario draws are kept in their original proportions. The proportions and total_draws fields are initialized to dummy values and are updated when the component draws are read in. replace is set to false.\nsample = true: Additionally specify proportions, total_draws, and replace. Draws from scenarios[i] are sampled into the aggregate distribution with probability proportions[i]","category":"page"},{"location":"scenarios/#","page":"Alternative Scenarios","title":"Alternative Scenarios","text":"SingleScenario and ScenarioAggregate are both subtypes of the abstract type AbstractScenario. The actual sampling and aggregating of scenario draws happens entirely in scenario_means_bands, which calls functions that dispatch on the AbstractScenario subtype passed in. In particular, this means that we don't save a separate \"raw\" output file with the individual draws that made it into a particular ScenarioAggregate – we only save the resulting MeansBands.","category":"page"},{"location":"forecast_decomposition/#forecast-decomp-1","page":"Forecast Decomposition","title":"Forecast Decomposition","text":"","category":"section"},{"location":"forecast_decomposition/#","page":"Forecast Decomposition","title":"Forecast Decomposition","text":"CurrentModule = DSGE","category":"page"},{"location":"forecast_decomposition/#","page":"Forecast Decomposition","title":"Forecast Decomposition","text":"Separate from the standard Forecasting routines, we have also implemented a function decompose_forecast for explaining why a forecast changes as new data becomes available and new estimations are run.","category":"page"},{"location":"forecast_decomposition/#","page":"Forecast Decomposition","title":"Forecast Decomposition","text":"DSGE.decompose_forecast","category":"page"},{"location":"forecast_decomposition/#DSGE.decompose_forecast","page":"Forecast Decomposition","title":"DSGE.decompose_forecast","text":"decompose_forecast(m_new, m_old, df_new, df_old, input_type, cond_new, cond_old,\n    classes; verbose = :low, kwargs...)\n\ndecompose_forecast(m_new, m_old, df_new, df_old, params_new, params_old,\n    cond_new, cond_old, classes; check = false)\n\nexplains the differences between an old forecast and a new forecast by decomposing the differences into three sources:\n\n(1) Data revisions, (2) News (e.g. new data that has become available since the old forecast), (3) Re-estimation (i.e. changes in model parameters).\n\nInputs\n\nm_new::M and m_old::M where M<:AbstractDSGEModel\ndf_new::DataFrame and df_old::DataFrame\ncond_new::Symbol and cond_old::Symbol\nclasses::Vector{Symbol}: some subset of [:states, :obs, :pseudo]\n\nMethod 1 only:\n\ninput_type::Symbol: estimation type to use. Parameters will be loaded using load_draws(m_new, input_type) and load_draws(m_old, input_type) in this method\n\nMethod 2 only:\n\nparams_new::Vector{Float64} and params_old::Vector{Float64}: single parameter draws to use\n\nKeyword Arguments\n\ncheck::Bool: whether to check that the individual components add up to the correct total difference in forecasts. This roughly doubles the runtime\n\nMethod 1 only:\n\nverbose::Symbol\n\nOutputs\n\nThe first method returns nothing. The second method returns decomp::Dict{Symbol, Matrix{Float64}}, which has keys of the form :decomp<component><class> and values of size Ny x Nh, where\n\nNy is the number of variables in the given class\nNh is the number of common forecast periods, i.e. periods between date_forecast_start(m_new) and date_forecast_end(m_old)\n\n\n\n\n\n","category":"function"},{"location":"forecast_decomposition/#","page":"Forecast Decomposition","title":"Forecast Decomposition","text":"For an example of how to use this functionality, see decompose_forecast.jl on the Github page (or directly inside the directory where DSGE.jl* has been downloaded).","category":"page"},{"location":"plotting/#Plotting-1","page":"Plotting","title":"Plotting","text":"","category":"section"},{"location":"plotting/#","page":"Plotting","title":"Plotting","text":"CurrentModule = DSGE","category":"page"},{"location":"plotting/#","page":"Plotting","title":"Plotting","text":"The DSGE plotting code uses Plots. We typically use GR and Plotly as backends; however, the goal of Plots is that all backends should be supported interchangeably. In each of the functions listed below, there are methods which take in an AbstractModel and those which take in some lower-level input arguments (typically one or more MeansBands). See the individual function docstring for details.","category":"page"},{"location":"plotting/#Plotting-Estimation-Results-1","page":"Plotting","title":"Plotting Estimation Results","text":"","category":"section"},{"location":"plotting/#","page":"Plotting","title":"Plotting","text":"plot_prior_posterior: plot the prior distribution overlaid on a histogram of posterior draws","category":"page"},{"location":"plotting/#Plotting-Forecasts-1","page":"Plotting","title":"Plotting Forecasts","text":"","category":"section"},{"location":"plotting/#","page":"Plotting","title":"Plotting","text":"plot_history_and_forecast: plot a historical and forecasted series, possibly with uncertainty bands (if for a full-distribution forecast)\nplot_forecast_comparison: plot two sets of histories and forecasts in one plot\nhair_plot: plot many forecasts as \"hairs\" coming out of some realized data series\nplot_shock_decomposition: plot the contributions of individual shocks as a bar plot, with a line for the detrended mean forecast\nplot_impulse_response: plot impulse response functions","category":"page"},{"location":"plotting/#Other-Plots-1","page":"Plotting","title":"Other Plots","text":"","category":"section"},{"location":"plotting/#","page":"Plotting","title":"Plotting","text":"plot_altpolicies: plot forecasts under several alternative policies in one plot\nplot_scenario: plot a forecast conditional on some alternative scenario, in deviations from some baseline","category":"page"},{"location":"algorithms/#Standard-Algorithms-1","page":"Algorithms","title":"Standard Algorithms","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"CurrentModule = DSGE","category":"page"},{"location":"algorithms/#Solving-the-Model-1","page":"Algorithms","title":"Solving the Model","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"gensys","category":"page"},{"location":"algorithms/#DSGE.gensys","page":"Algorithms","title":"DSGE.gensys","text":"gensys(Γ0, Γ1, c, Ψ, Π)\ngensys(Γ0, Γ1, c, Ψ, Π, div)\ngensys(F::LinearAlgebra.GeneralizedSchur, c, Ψ, Π)\ngensys(F::LinearAlgebra.GeneralizedSchur, c, Ψ, Π, div)\n\nGenerate state-space solution to canonical-form DSGE model.\n\nSystem given as\n\nΓ0*y(t) = Γ1*y(t-1) + c + Ψ*z(t) + Π*η(t),\n\nwith z an exogenous variable process and η being endogenously determined one-step-ahead expectational errors.\n\nReturned system is\n\ny(t) = G1*y(t-1) + C + impact*z(t) + ywt*inv(I-fmat*inv(L))*fwt*z(t+1)\n\nReturned values are\n\nG1, C, impact, fmat, fwt, ywt, gev, eu, loose\n\nIf z(t) is i.i.d., the last term drops out.\n\nIf div is omitted from argument list, a div>1 is calculated.\n\nReturn codes\n\neu[1] = 1 for existence\neu[2] = 1 for uniqueness\neu[1] = -1 for existence only with not-s.c. z\neu = [-2, -2] for coincident zeros\neu = [-3, -3] if a LAPACKException is thrown while computing the Schur decomposition\n\nNotes\n\nWe constrain Julia to use the complex version of the schurfact routine regardless of the types of Γ0 and Γ1, to match the behavior of Matlab.  Matlab always uses the complex version of the Schur decomposition, even if the inputs are real numbers.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#algs-optimization-1","page":"Algorithms","title":"Optimization","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"csminwel\noptimize!","category":"page"},{"location":"algorithms/#DSGE.csminwel","page":"Algorithms","title":"DSGE.csminwel","text":"csminwel(fcn::Function, grad::Function, x0::Vector, H0::Matrix=1e-5.*eye(length(x0)), args...;\n         xtol::Real=1e-32, ftol::Float64=1e-14, grtol::Real=1e-8, iterations::Int=1000,\n         store_trace::Bool = false, show_trace::Bool = false, extended_trace::Bool = false,\n         verbose::Symbol = :none, rng::AbstractRNG = MersenneTwister(0), kwargs...)\n\nMinimizes fcn using the csminwel algorithm.\n\nArguments\n\nfcn::Function : The objective function\ngrad::Function : The gradient of the objective function. This argument can be omitted if\n\nan analytical gradient is not available, which will cause a numerical gradient to be calculated.\n\nx0::Vector: The starting guess for the optimizer\n\nOptional Arguments\n\nH0::Matrix: An initial guess for the Hessian matrix – must be\n\npositive definite. If none is given, then a scaled down identity matrix is used.\n\nargs...:  Other positional arguments to be passed to f on each\n\nfunction call\n\nKeyword Arguments\n\nftol::{T<:Real}=1e-14: Threshold for convergence in terms of change\n\nin function value across iterations.\n\niterations::Int=100: Maximum number of iterations\nkwargs...: Other keyword arguments to be passed to f on each\n\nfunction call\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#DSGE.optimize!","page":"Algorithms","title":"DSGE.optimize!","text":"optimize!(m::Union{AbstractDSGEModel,AbstractVARModel}, data::Matrix;\n          method::Symbol       = :csminwel,\n          xtol::Real           = 1e-32,  # default from Optim.jl\n          ftol::Float64        = 1e-14,  # Default from csminwel\n          grtol::Real          = 1e-8,   # default from Optim.jl\n          iterations::Int      = 1000,\n          store_trace::Bool    = false,\n          show_trace::Bool     = false,\n          extended_trace::Bool = false,\n          verbose::Symbol      = :none)\n\nWrapper function to send a model to csminwel (or another optimization routine).\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Hessian-Approximation-1","page":"Algorithms","title":"Hessian Approximation","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"Modules = [DSGE]\nPages   = [\"hessian.jl\", \"hessizero.jl\"]\nOrder   = [:function, :type]","category":"page"},{"location":"algorithms/#DSGE.hessian!-Union{Tuple{T}, Tuple{Union{AbstractDSGEModel, AbstractVARModel},Array{T,1},AbstractArray}} where T<:AbstractFloat","page":"Algorithms","title":"DSGE.hessian!","text":"hessian!(m::Union{AbstractDSGEModel,AbstractVARModel}, x::Vector{T}, data::AbstractArray;\n         verbose::Symbol = :none) where {T<:AbstractFloat}\n\nCompute Hessian of DSGE/VAR posterior function evaluated at x.\n\n\n\n\n\n","category":"method"},{"location":"algorithms/#DSGE.hessizero-Union{Tuple{T}, Tuple{Function,Array{T,1}}} where T<:AbstractFloat","page":"Algorithms","title":"DSGE.hessizero","text":"hessizero(fcn::Function, x::Vector{T};\n          check_neg_diag::Bool=false,\n          verbose::Symbol=:none,\n          distr::Bool=true) where T<:AbstractFloat\n\nCompute Hessian of function fcn evaluated at x.\n\nArguments\n\ncheck_neg_diag: Throw an error if any negative diagonal elements are detected.\nverbose: Print verbose output\ndistr: Use available parallel workers to increase performance.\n\n\n\n\n\n","category":"method"},{"location":"algorithms/#Sampling-1","page":"Algorithms","title":"Sampling","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"metropolis_hastings","category":"page"},{"location":"algorithms/#DSGE.metropolis_hastings","page":"Algorithms","title":"DSGE.metropolis_hastings","text":"function metropolis_hastings(propdist::Distribution,\n                             loglikelihood::Function,\n                             parameters::ParameterVector{S},\n                             data::Matrix{T},\n                             cc0::T,\n                             cc::T;\n                             n_blocks::Int = 1,\n                             n_param_blocks::Int64 = 1,\n                             n_sim::Int64          = 100,\n                             n_burn::Int64         = 0,\n                             mhthin::Int64         = 1,\n                             adaptive_accpt::Bool  = false,\n                             α::T = 1.0,      c::T = 1.0,\n                             verbose::Symbol=:low,\n                             savepath::String = \"mhsave.h5\",\n                             rng::MersenneTwister = MersenneTwister(0),\n                             testing::Bool = false) where {S<:Number, T<:AbstractFloat}\n\nImplements the Metropolis-Hastings MCMC algorithm for sampling from the posterior distribution of the parameters.\n\nArguments\n\nproposal_dist: The proposal distribution that Metropolis-Hastings begins sampling from.\nm: The model object\ndata: Data matrix for observables\ncc0: Jump size for initializing Metropolis-Hastings.\ncc: Jump size for the rest of Metropolis-Hastings.\n\nOptional Arguments\n\nn_blocks::Int = 1: Number of blocks (for memory-management purposes)\nn_param_blocks::Int = 1: Number of parameter blocks\nn_sim::Int    = 100: Number of simulations. Note: # saved observations will be   nsim * nparamblocks * (nblocks - b_burn).\nn_burn::Int   = 0: Length of burn-in period\nmhthin::Int   = 1: Thinning parameter (for mhthin = d, keep only every dth draw)\nadaptive_accpt::Bool = false: Whether or not to adaptively adjust acceptance prob.\nα::T = 1.0: Tuning parameter (step size) for proposal density computation in adaptive case\nc::T = 0.5: Tuning parameter (mixture proportion) for proposal density computation in   adaptive case\nverbose::Bool: The desired frequency of function progress messages printed to standard out. One of:\n\n   - `:none`: No status updates will be reported.\n   - `:low`: Status updates provided at each block.\n   - `:high`: Status updates provided at each draw.\n\nsavepath::String = \"mhsave.h5\": String specifying path to output file\nrng::MersenneTwister = MersenneTwister(0): Chosen seed (overridden if testing = true)\ntesting::Bool = false: Conditional for use when testing (determines fixed seeding)\n\n\n\n\n\nmetropolis_hastings(propdist::Distribution, m::Union{AbstractDSGEModel,AbstractVARModel},\n    data::Matrix{T}, cc0::T, cc::T; verbose::Symbol = :low) where {T<:AbstractFloat}\n\nWrapper function for DSGE models which calls Metropolis-Hastings MCMC algorithm for sampling from the posterior distribution of the parameters.\n\nArguments\n\npropdist: The proposal distribution that Metropolis-Hastings begins sampling from.\nm: The model object\ndata: Data matrix for observables\ncc0: Jump size for initializing Metropolis-Hastings.\ncc: Jump size for the rest of Metropolis-Hastings.\n\nOptional Arguments\n\nverbose: The desired frequency of function progress messages printed to standard out. One of:\n\n   - `:none`: No status updates will be reported.\n   - `:low`: Status updates provided at each block.\n   - `:high`: Status updates provided at each draw.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#State-Space-Filters-and-Smoothers-1","page":"Algorithms","title":"State Space Filters and Smoothers","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"See StateSpaceRoutines.jl.","category":"page"},{"location":"algorithms/#Sequential-Monte-Carlo-1","page":"Algorithms","title":"Sequential Monte Carlo","text":"","category":"section"},{"location":"algorithms/#","page":"Algorithms","title":"Algorithms","text":"See SMC.jl.","category":"page"},{"location":"contributing/#contributing-1","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"","category":"section"},{"location":"contributing/#Notes-for-DSGE.jl-Contributors-1","page":"Contributing to DSGE.jl","title":"Notes for DSGE.jl Contributors","text":"","category":"section"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"We are continuing to add more features to this package. Please see the README for details.","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"As these steps are under development, we would welcome improvements to the existing code from the community. Some examples could be:","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"Performance improvements\nAlternatives to algorithms used here (optimization, hessian, etc.)\nExtension of the DSGE-VAR and VAR code\nExtension of model averaging techniques beyond PoolModel\nOther general improvements\nAdding documentation/test coverage\nAdding existing notable DSGE models into the models/ directory","category":"page"},{"location":"contributing/#Git-Recommendations-For-Pull-Requests-1","page":"Contributing to DSGE.jl","title":"Git Recommendations For Pull Requests","text":"","category":"section"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"These are adapted from JuliaLang.","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"Avoid working from the master branch of your fork, creating a new branch will make it easier if DSGE's master changes and you need to update your pull request.\nTry to squash together small commits that make repeated changes to the same section of code so your pull request is easier to review, and Julia's history won't have any broken intermediate commits. A reasonable number of separate well-factored commits is fine, especially for larger changes.\nIf any conflicts arise due to changes in DSGE's master, prefer updating your pull request branch with git rebase versus git merge or git pull, since the latter will introduce merge commits that clutter the git history with noise that makes your changes more difficult to review.\nDescriptive commit messages are good.\nUsing git add -p or git add -i can be useful to avoid accidentally committing unrelated changes.\nGitHub does not send notifications when you push a new commit to a pull request, so please add a comment to the pull request thread to let reviewers know when you've made changes.\nWhen linking to specific lines of code in discussion of an issue or pull request, hit the y key while viewing code on GitHub to reload the page with a URL that includes the specific version that you're viewing. That way any lines of code that you refer to will still make sense in the future, even if the content of the file changes.\nWhitespace can be automatically removed from existing commits with git rebase.\nTo remove whitespace for the previous commit, run git rebase --whitespace=fix HEAD~1.\nTo remove whitespace relative to the master branch, run git rebase --whitespace=fix master.","category":"page"},{"location":"contributing/#DSGE-Julia-Style-Guide-1","page":"Contributing to DSGE.jl","title":"DSGE Julia Style Guide","text":"","category":"section"},{"location":"contributing/#Intro-1","page":"Contributing to DSGE.jl","title":"Intro","text":"","category":"section"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"This document lists Julia coding recommendations consistent with best practices in the software development community. The recommendations are based on guidelines for other languages collected from a number of sources and on personal experience. These guidelines are written with the New York Fed DSGE code in mind. All pull requests submitted should follow these general style guidelines.","category":"page"},{"location":"contributing/#Naming-conventions-1","page":"Contributing to DSGE.jl","title":"Naming conventions","text":"","category":"section"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"Emphasize readability! Our goal is for the code to mimic the mathematical notation used in New York Fed DSGE papers as closely as possible.","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"The names of variables should document their meaning or","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"use. Variables with a large scope should have especially meaningful names. Variables with a small scope can have short names.","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"Exhibit consistency with the existing codebase.\nModules and type names in UpperCamelCase\nVariable names in snake_case.","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"Variable names should be in lower case, using underscores to separate parts of a compound variable name. For example, steady_state and equilibrium_conditions are two fields in the Model990() object that follow this convention. Also notice that, though the words could be shortened, they are spelled out for maximum clarity.","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"The prefix n_ should be used for variables representing the","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"number of objects (e.g. n_parameters or n_anticipated_shocks) use the suffix s as is natural in spoken language.","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"Negative Boolean variable names should be avoided. A problem arises","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"when such a name is used in conjunction with the logical negation operator as this results in a double negative. It is not immediately apparent what !isNotFound means.  Use isFound. Avoid isNotFound.","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"Named constants can be all uppercase using underscore to separate words:","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"MAX_ITERATIONS","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"Naming mathematical objects","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"Variables with mathematical significance should use unicode characters and imitate LaTeX syntax.  For example, ρ should be used to name the autocorrelation coefficient in an AR(1) process, and σ should be used to name standard deviation. Parameters in the text should keep the same symbol in the code (e.g. α in the code is the same α as in this paper, and takes on it usual significance as the capital share in a Cobb-Douglas output function.","category":"page"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"General conventions\nUnderscores can and should be used when the variable refers to a mathematical object that has a subscript. (In this case, we are imitating LaTeX syntax.) For example, r_m in LaTeX should be represented by the variable r_m.\nIf the mathematical object has multiple subscripts, for example x_ij, simply concatenate the subscripts: x_ij.\nIf the object has superscripts as well as subscripts, for example y^f_t, separate the superscripts with an underscore and place them first: y_f_t.\nFor compatibility with existing code, variables with numeric subscripts should exclude the underscore: G0, ψ1.\nMatrices that have mathematical significance (e.g. the matrices of the transition and measurement equations) should be upper case, as they are in mathematical notation, and can repeat the letter to avoid collisions: TTT or QQ.\nSymbols such as overbars (which indicate mean values) and  tildes (which indicate log-deviations from the steady state) are written using a 3- or 4-letter abbreviation immediately after the variable they modify: kbar_t, ztil (z tilde).\nStars indicating steady-state variables are included as subscripts: π_star_t\nSuffixes\nTime: Consistent with the previous bullet points, the suffix _t as in x_t signifies the value of x at time t. The suffix _t1 signifies the value of x at time t-1.\nShocks: The suffix _sh refers to a model shock.\nPrefixes\nThe prefix eq_ refers to an equilibrium condition.\nThe prefix obs_ refers to an observable.\nThe prefix E refers to the expectation operator.\nThe prefix I refers to the indicator operator.\nObservables with the prefix g refer to growth rates.","category":"page"},{"location":"contributing/#Code-Formatting-Guidelines-1","page":"Contributing to DSGE.jl","title":"Code Formatting Guidelines","text":"","category":"section"},{"location":"contributing/#","page":"Contributing to DSGE.jl","title":"Contributing to DSGE.jl","text":"Indent 4 spaces\nWrap lines at 92 characters\nUse whitespace to enhance readability\nNo trailing whitespace","category":"page"},{"location":"MatlabToJuliaTransition/#The-DSGE-MATLAB-to-Julia-Transition:-Improvements-and-Challenges-1","page":"MATLAB to Julia Transition: Estimation","title":"The DSGE MATLAB to Julia Transition: Improvements and Challenges","text":"","category":"section"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"Zac Cranko, Pearl Li, Spencer Lyon, Erica Moszkowski, Micah Smith, Pablo Winant  ","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"December 3, 2015","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"The FRBNY DSGE model is a relatively large New Keynesian model augmented with financial frictions and a variety of innovations. Here at the Fed, we use it both for forecasting and policy analysis. Research using this model includes looking at the dynamics of inflation during the great recession, the effects of forward guidance, and much more.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"When we were approached by the folks at QuantEcon about a possible collaboration, we jumped at the idea, as it would give us an opportunity to rework our code in an arguably faster language, redesign it from the ground up, and release it open source for the benefit of the community. A full-fledged package for the FRBNY DSGE model would also provide QuantEcon another opportunity to highlight its contribution to high-performance, quantitative economic modeling. Julia was the language of choice, recommended by the QuantEcon group for its high performance and suitability for this breed of technical computing.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"In this post, we’ll discuss our experiences redesigning our code from the ground up, the resulting performance changes, and the challenges we faced working with such a young language.  We created a suite to assess the performance of our Julia code, relative to our MATLAB code. We focus on both the core functions used in solving and estimating the model, as well as on longer-running routines of greater scope. These tests were conducted on a single core on an Intel® Xeon® E5-2697 v2 2.70GHz CPU running GNU/Linux:","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"Benchmark times relative to MATLAB (smaller is better)","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"Test MATLAB (14a) Julia (0.4.0)\ngensys 1.00 0.17\nsolve 1.00 0.09\nkalman_filter 1.00 0.75\nposterior 1.00 0.26\ncsminwel 1.00 0.33\nhessian 1.00 0.23\nmetropolis_hastings 1.00 0.11","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"We ultimately achieve an increase of speed that reduces running time to 1/10th to 3/4th that of the MATLAB code. The Metropolis-Hastings sampling step is the most time consuming, and hence the relevant one in terms of assessing speed improvement. On the basis of this step, we conclude that DSGE.jl* is approximately ten times faster than the MATLAB code.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"How much of this increase is due to native performance adventures of Julia, and how much is simply due to the improvements in design that came from rebuilding this project from the ground up? It is of course difficult to say, and it is important to emphasize that one cannot be sure what portion of the performance increase can be attributed to inherent language features as opposed to design differences. Indeed, our MATLAB code suffers from many inefficiencies due to its long, cumulative development, and support for a plethora of models and features. Meanwhile, these design issues have been largely addressed in our Julia package. To best isolate differences in the languages themselves, we can look at our code to compute the model solution with gensys and apply the Kalman filter with kalman_filter. These two functions have relatively little redesign and optimization as compared to the MATLAB code and provide the most comparable, though still imperfect, measurements of performance. The reduction of 1/5th to 3/4th in computing time, therefore, could be taken as a first estimate of Julia's advantage in this single arena of computation.","category":"page"},{"location":"MatlabToJuliaTransition/#Code-Improvements-1","page":"MATLAB to Julia Transition: Estimation","title":"Code Improvements","text":"","category":"section"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"Julia provides versatile language features that allow us to improve our code's performance and clarity in several fundamental ways. First and foremost of these is the highly integrated, robust, and flexible type system that lends itself naturally to our DSGE model. At the center of the DSGE.jl* package is the model object. Here, one can store all information associated with the model – including the numerous parameters, priors, states, equilibrium conditions, computational settings, and flags – in one place.  By simply passing the model object as an argument to any function, the function has access to all of the model's fields.  By comparison, our MATLAB code stored all variables directly in the global workspace – an approach that scaled poorly as model specifications become more and more complex. To illustrate just how unwieldy our MATLAB code was, many of our function calls required more than 20 positional arguments, a serious challenge for usage and human-readability:","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"[post_new,like_new,zend_new,ZZ_new,DD_new,QQ_new] = ...\n    feval('objfcnmhdsge',para_new,bounds,YY,YY0,nobs,nlags,nvar,mspec,npara,...\n    trspec,pmean,pstdd,pshape,TTT_new,RRR_new,CCC_new,valid_new,para_mask,...\n    coint,cointadd,cointall,YYcoint0,args_nant_antlags{:});","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"While several of these arguments (e.g., coint) relate to a feature not-implemented in Julia, one can still see the excesses of providing so much information about the model separately in function calls.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"The same code in Julia:","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"post_out = posterior!(m, para_new, data; mh=true)","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"Certainly, one could approximate a \"model object\" in MATLAB by using its own object-oriented classes, or by \"bundling\" model attributes into a struct or other data structure.  However, MATLAB classes are both relatively complicated and slower than non-object implementations. And using structs in this way results in copies of all model variables made on every function call.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"Indeed, changes like this reduce the number of lines of code in DSGE.jl*, a rough proxy for ease of maintenance. We find that the fixed cost of setting up the type system is offset by savings in core programs.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"Language Lines of code (hundreds)\nMatlab 63\nJulia 37","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"A type-based approach allows us to take advantage of method dispatch in Julia by defining different model types for different model specifications. As detailed in the README file, changes to the model's equilibrium conditions and measurement equation are referred to as changes in a model's \"specification.\"  In the Julia code, model specifications have a 1:1 correspondence with concrete types.  Where necessary, a single function can have multiple methods defined, that are customized for different model types. For example, the augment_states function augments the model's transition matrices after it has been solved.  We can pass any model object m to augment_states, and Julia ensures that the proper, model-specific method is dispatched:","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"TTT, RRR, CCC = augment_states(m, TTT_gensys, RRR_gensys, CCC_gensys)","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"In our MATLAB code, on the other hand, we would approximate this type of dispatch by using a switch statement over a model identifier variable. For the hundreds of models we have worked with in a development capacity, this led to bloat in our model solution code. In Julia, we encapsulate this behavior within the model definition itself.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"It is easy to see that all model types constructed for use with DSGE.jl* are closely related: they will have the same fields, and are passed to the same methods.  If it sounds to you like we have an implicit interface here, you’re right. Rather than implementing each object as a standalone type, we define an abstract type, AbstractModel, to serve as the parent for all model types. Because most of our routines are not model-specific, we need only define them once (with an argument of type AbstractModel) and Julia's dispatch system takes care of the rest. We similarly define model parameters as subtypes of a common abstract type AbstractParameter. This allows us to abstract to one notion of a model parameter, while implementing different kinds of parameters in different ways. We also use parameterized (generic) types to increase the flexibility of model parameters (as well as elsewhere in our codebase):","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"# A parameter contains values of data type `T` and embeds a transformation of\n# type `U`\nabstract Parameter{T,U<:Transform} <: AbstractParameter{T}\n\n# One transformation used is the identity\ntype UnscaledParameter{T,U} <: Parameter{T,U}\n    # ...\nend","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"These functions expect the model object to have certain fields, and for those fields to have certain types. (As an example of Julia's youthful status as a language, discussion continues, as of this writing, on an appropriate manner to explicitly introduce interfaces.)","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"With a clear interface in place, running new model specifications using DSGE.jl* is relatively straightforward. (See here for detailed instructions).","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"Julia's JIT compilation provides significant performance boosts in some areas. For example, we allow a variable number of anticipated monetary policy shocks, beginning in 2008Q4, that we use to treat the zero lower bound. In our MATLAB code, we suffer some dynamic code generation to implement this feature.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"if exist('nant','var')\n    for i=1:nant\n        eval(strcat('rm_tl',num2str(i),'  = ',num2str(nstates+i)));\n        eval(strcat('rm_tl',num2str(i),'  = ',num2str(nstates+i)));\n    end\nend","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"Julia's faster evaluation of such statements reduces this performance hit, as these symbols can be associated with the model object.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"[symbol(\"rm_tl$i\") for i = 1:n_anticipated_shocks(m)]\n# ...\n[symbol(\"rm_shl$i\") for i = 1:n_anticipated_shocks(m)]","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"Granted, there may be better solutions to our problem in both languages, but similar situations involving code generation are easily addressed in Julia.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"We have found that a number of Julia features make working with DSGE.jl* simply more pleasant and user-friendly than working with our old codebase. Julia's clearly integrated testing infrastructure has made our development workflow significantly more robust. Unicode support means that code can correspond more closely to actual model equations, reducing the headache associated with translating from \"math\" to \"code\".  (Inline Markdown documentation helps in a similar way.) Operator overloading and user-defined syntax make it easy to be much more expressive with our code.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"julia> m[:α]                         # Access value of param α from model m\njulia> m <= parameter(:ϵ_p, 10.000)  # Add parameter ϵ_p to model\njulia> Γ0, Γ1, C, Ψ, Π  = eqcond(m)  # Get equilibrium conditions","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"We have found that Julia's highly integrated, Git-based package manager is an improvement over MATLAB's decentralized FileExchange. As Julia users, we can now pull in high-quality, fully tested, community-supported external packages that can each be installed or updated with a single command.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"julia> Pkg.add(\"QuantEcon\")          # That's it!","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"This reduces the need for users to create their own, likely lower-quality functionality, increasing developer and code performance. (Or the need to fight for the toolbox licenses available to their department.)","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"We acknowledge that our package is far from perfect. Possible improvements to DSGE.jl* are many and varied. We may experiment with alternative, modern, numerical routines to improve speed. Ultimately, powerful metaprogramming support would allow user to specify model equations more literally, in mathematical notation. We welcome improvements to the existing code from the community.","category":"page"},{"location":"MatlabToJuliaTransition/#Challenges-1","page":"MATLAB to Julia Transition: Estimation","title":"Challenges","text":"","category":"section"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"Converting the FRBNY DSGE model from MATLAB, a mature and well-supported language, to an extremely young language like Julia involved no shortage of challenges. Significant changes to the Julia language itself are introduced in rapid succession, and using DSGE.jl* with a new Julia version inevitably floods the user’s screen with deprecation warnings. There is significant difficulty in finding written resources on the language beyond the Julia Manual itself. Google searches frequently return discussions in GitHub Issues, which are unhelpful to elementary users and can be actively misleading at times.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"Differences between the behavior of MATLAB and Julia’s core linear algebra libraries led to many roadblocks in the development of DSGE.jl*. Julia uses multithreaded BLAS functions for some linear algebra functions.  Using a different number of threads can change the results of matrix decomposition when the matrix is singular. This indeterminacy caused significant problems for our testing suite, both in comparing output matrices to MATLAB results and in testing for reproducibility among Julia outputs.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"We ran into similar numerical problems while porting the model solution algorithm, gensys. At one point, the generalized Schur (QZ) decomposition is computed, yielding the decompositions A=QSZ' and B=QTZ'. In MATLAB, upper triangular matrices S and T are returned. In Julia, meanwhile, the default behavior is to return a real decomposition with upper Hessenberg (blocked diagonal) matrices S and T. Differing behaviors like this in the two languages might expose a user without deep knowledge of the procedure to errors.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"Finally, dealing with a recently introduced language can make it more difficult for new users to produce performant code.  A typical economist, especially one coming from a MATLAB background, may be unfamiliar with the nature and use of language concepts like type stability, parametric types, and preallocation. Julia's profiler and debugger lack the flexibility of those in MATLAB, and can make it difficult to identify the source of errors or performance bottlenecks. And Julia IDEs, like Juno, while admirable, are not as mature or featured as the MATLAB IDE.","category":"page"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"It is important to note again that similar improvements could have been made to our MATLAB code directly. (As we would be the first ones to admit.) Regardless, the Julia paradigm results in code that is high-quality from the outset.","category":"page"},{"location":"MatlabToJuliaTransition/#Conclusion-1","page":"MATLAB to Julia Transition: Estimation","title":"Conclusion","text":"","category":"section"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"After months of hard work, we are pleased to be able to increase the performance of our model and provide our project for the benefit of the community. For those considering similar projects, we find the benefits of a transition to Julia are significant. One should, however, be realistic about the challenges that will be faced transitioning to a young language.","category":"page"},{"location":"MatlabToJuliaTransition/#Disclaimer-1","page":"MATLAB to Julia Transition: Estimation","title":"Disclaimer","text":"","category":"section"},{"location":"MatlabToJuliaTransition/#","page":"MATLAB to Julia Transition: Estimation","title":"MATLAB to Julia Transition: Estimation","text":"This post reflects the experience of the authors with Julia and MATLAB and does not represent an endorsement by the Federal Reserve Bank of New York or the Federal Reserve System of any particular product or service. The views expressed in this post are those of the authors and do not necessarily reflect the position of the Federal Reserve Bank of New York or the Federal Reserve System. Any errors or omissions are the responsibility of the authors.","category":"page"},{"location":"julia_forecasting/#Macroeconomic-Forecasting-with-DSGEs-Using-Julia-and-Parallel-Computing-1","page":"MATLAB to Julia Transition: Forecast","title":"Macroeconomic Forecasting with DSGEs Using Julia and Parallel Computing","text":"","category":"section"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Marco Del Negro, Abhi Gupta, Pearl Li, Erica Moszkowski","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"April 17, 2017","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"In December 2015, we announced DSGE.jl, our open-source, Julia-language package for working with dynamic stochastic general equilibrium (DSGE) models. At that time, DSGE.jl* contained only the code required to specify, solve, and estimate such models using Bayesian methods. Now, we present the additional code needed to produce economic forecasts using estimated DSGE models. This new code replicates our MATLAB codebase while being more efficient, easier to read, and open source.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"As we noted in our last post and its corresponding technical post, porting our code to Julia presented us with the opportunity to improve both our code's performance and our team's workflow. While the estimation step was largely a direct port, we redesigned the forecast section to obtain code that is faster and easier to use. In this post, we will discuss the performance improvements we have achieved in forecasting the DSGE model, as well as the design principles and Julia tools (particularly related to parallel computing) that helped us achieve those results.","category":"page"},{"location":"julia_forecasting/#Performance-Improvements-1","page":"MATLAB to Julia Transition: Forecast","title":"Performance Improvements","text":"","category":"section"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"To motivate our decision to redesign the forecasting code, we first present some overall performance comparisons between our MATLAB and Julia codebases. Because the design of the code has changed significantly, these results should not be taken as a horse race between Julia and MATLAB. Rather, they should indicate the extent to which our design decisions, in conjunction with the power of the Julia language, have improved the process of running a DSGE forecast.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"These tests were conducted on a single core on an Intel® Xeon® E5-2697 v2 2.70GHz CPU running GNU/Linux. The exception is computing all the full-distribution results, which was done using 50 parallel workers.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Benchmark Times Relative to MATLAB 2014a (Smaller is Better)","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Test MATLAB (2014a) Julia (0.4.5)\nSimulation smoothing 1.00 0.38\nForecasting 1.00 0.24\nComputing shock decompositions 1.00 0.12\nFull set of forecast outputs (modal parameters) 1.00 0.10\nFull set of forecast outputs (full distribution of parameters) 1.00* 0.22","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"*Unlike the other steps being tested, the full-distribution forecast timing was run in MATLAB 2009a. Our code relies on MATLAB parallelization features that were deprecated with the introduction of the Parallel Computing Toolbox.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Post estimation, we produce a number of forecast-related outputs, either at the mode or using the full estimated posterior distribution. The tasks involved include smoothing, forecasting (both enforcing the zero lower bound and not), and computing shock decompositions (exercises that allow us to account for the evolution of observed variables in terms of their driving forces).","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"With our most recent model, which is available in DSGE.jl*, we can compute all the full-distribution forecast outputs in approximately fifteen minutes. In comparison, the same computations in MATLAB typically take about seventy minutes. As a result, we can experiment with different options and correct mistakes much more flexibly than we could previously.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"In the next sections, we discuss the design principles that guided our port, as well as the Julia parallel programming tools that enabled us to write efficient parallel code.","category":"page"},{"location":"julia_forecasting/#Design-Principles-1","page":"MATLAB to Julia Transition: Forecast","title":"Design Principles","text":"","category":"section"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Our goal when porting the forecast step to Julia was to write code that could efficiently produce easy-to-interpret results. Furthermore, because we are often interested in looking at just one kind of result (for instance, impulse response functions), we wanted it to be equally simple to produce a single, very specific output as it was to produce all results. These two goals translated into two related principles that guided our Julia code development: type-orientation and modularity.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Type-Orientation","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"As we discussed in our previous post, Julia's type system allows us to write very clean, well-structured code, and we use types heavily throughout the codebase. For example, in the forecast step, we use types heavily to keep track of the information we need to download and transform our input data. As an example, let's consider the process of downloading and transforming the GDP series for use in the DSGE model. First, using the FredData.jl package, we pull the aggregate nominal GDP series (in dollars) from the Federal Reserve Economic Database (FRED) programmatically. Before the estimation, we transform this series into the appropriate units for the log-linearized model: quarter-to-quarter log differences of real, per-capita GDP. After estimating and forecasting the model, we finally transform the results into the units most frequently discussed by policymakers and researchers: annualized GDP growth in percentage terms.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"We wanted a simple way to keep track of all of the information associated with the GDP variable in a single place. To do this, we created a new Julia type called an Observable. An instance of the Observable type bundles together the name of the variable, sources used to create the series, and all transformations associated with that series. An instance of this Observable type has the following fields:","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"type Observable\n    key::Symbol\n    input_series::Vector{Symbol}\n    fwd_transform::Function\n    rev_transform::Function\n    name::UTF8String\n    longname::UTF8String\nend","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"The key, name, and longname fields serve similar but slightly different purposes. The key is used as the primary way we refer to the GDP variable in the code: when we construct the entire dataset, we create a DataFrame (2-dimensional table) and label each series with its key. By contrast, name is a longer-form name that we intend to use to label plots, while longname is more of a description of the series. This information helps us to label variables easily and keep the code clear.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"The more interesting fields are input_series, fwd_transform, and rev_transform. The input_series field is a vector of Symbols, each of which must be of the form :SERIES__SOURCE. In the case of GDP, this field is the vector [:GDP__FRED, :CNP16OV__FRED, :GDPDEF__FRED]. All of these series come from FRED, and in particular, we use the nominal GDP, working-age civilian population, and GDP deflator series to construct the real per-capita GDP growth.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"The fwd_transform and rev_transform fields encode the transformations we make to the GDP series to go from raw data to model units and from model units to output units, respectively. These fields are particularly interesting because they must be populated by objects that are of type Function. That's right—a function is an instance of the Function type! Therefore, a given function is really no different than any other variable in Julia. That means we can define any function we want (abstract, named, with or without keyword arguments) and assign the name of that function to the fwd_transform and rev_transform fields. In the data step of the code, for instance, we can retrieve the name of the function by querying the Observable object and then apply the function to an appropriate set of arguments. This is a very direct method of looking up which transforms to apply, and simultaneously provides the opportunity for us to abstract common transformations into an appropriately named function. Abstraction is a technique for encapsulating low-level functionality or pieces of data into a well-named, reusable function or type. In our case, abstracting transformations into functions is useful because multiple observables can make use of the same commonly used functions.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Finally, we can construct the gdp observable as follows:","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"data_series = [:GDP__FRED, :CNP16OV__FRED, :GDPDEF__FRED]\nfwd_transform = function (levels) ... end    # an anonymous function definition\nrev_transform = loggrowthtopct_annualized_percapita\nobs_gdp = Observable(:obs_gdp, data_series, fwd_transform, rev_transform,\n    \"Real GDP  Growth\", \"Real GDP Growth Per Capita\")","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"We then store obs_gdp in a Dict{Symbol, Observable}, a lookup table that allows us to look up Observable objects, which is in turn stored in the observables field of the model object. We can query the model object for the rev_transform ofgdp_obs by simply calling m.observables[:gdp_obs].rev_transform (where m is an instance of a model type). Since this information is stored inside the model object for every observable, it is automatically available to every function that accepts a model object—helping us keep our function calls manageable and our data organized.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"We have found Julia's type system to be a helpful way to abstract the details associated with transforming data to and from various units. Observables are clearly a DSGE.jl*-specific example of a user-defined type, but we hope this discussion illustrates how Julia types and effective abstraction can help economists structure and clarify their code.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Modularity","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Most software systems (and economic models, for that matter) are designed to produce a wide variety of outputs. Macroeconomists often want to produce tables of parameters, impulse response functions, and time series plots for different economic variables. Often, users want to choose which of a set of possible outputs to compute. In a DSGE model, it is common to compute smoothed histories and forecasts of observables and unobservable states, shock decompositions (which decompose the path of each economic variable into the shocks responsible for its fluctuations), and impulse response functions. Additionally, users may want to change various settings. In our case, we can choose to forecast using the modal parameters or a selection of draws from the posterior distribution of the parameters. We can decide whether or not to enforce the zero lower bound on nominal interest rates. We can use no data from the current quarter, condition on only financial data from the current quarter, or use both financial data and GDP data from the current quarter. We can choose from several different smoothers to compute smoothed histories of states and observables.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Producing and storing all of these results takes both time and disk space. As users of our own old codebase, we found that these costs were often burdensome if we only wanted to produce a single result (for instance, an unconditional shock decomposition for GDP growth). This occurred because the top-level forecast function always called every subroutine, computed every output, and returned all outputs. Redesigning the codebase gave us the opportunity to write code that could produce specific outputs in addition to all outputs.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Fundamentally, in the DSGE forecast, there are three pieces of information we need to produce the specific outputs desired by the user. First, does the user want to produce a modal forecast or a full distribution forecast with uncertainty bands? Second, does she want to condition on any data from the current quarter? And third, which kinds of outputs does she want to produce (forecasts, shock decompositions, etc.)?  Once we know the answers to these questions, we can logically determine which outputs need to be produced and which can be ignored. Therefore, we present the user with one top-level function, which takes in these arguments and determines which subroutines need to be run.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"This modular approach to control flow can be taken in any language, but it is an important component of developing a large software system or economic model and thus we decided it was important to mention. Writing modular, type-oriented, and well-abstracted code improves the robustness of our workflow by making our code and results easier to interpret and less prone to error. In the next section, we'll discuss the main reason our Julia codebase is so fast: we are able to exploit Julia's parallel programming tools.","category":"page"},{"location":"julia_forecasting/#Parallel-Computing-1","page":"MATLAB to Julia Transition: Forecast","title":"Parallel Computing","text":"","category":"section"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"The types of forecast-related computations we do are naturally suited to parallelization. While our MATLAB code was parallelized to an extent (and was written before the advent of the MATLAB Parallel Computing Toolbox!), we decided to reassess our design when we ported the forecast step to Julia. We considered two approaches: \"parallel maps\" and distributed storage. The first is largely similar to our MATLAB parallelization implementation, while the latter takes advantage of the DistributedArrays.jl package and represents a substantial design shift. Over the course of development, we learned a great deal about writing effective parallelized Julia code and about parallel computing in general. Though the distributed storage approach did not end up improving on the parallel mapping approach, our final Julia code is faster and better designed than the original MATLAB implementation.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Like many academic institutions, the New York Fed's Research Group maintains a Linux-based cluster for use by the economists and RAs. This setup allows us to distribute computing jobs across multiple processes on multiple compute nodes, so that non-serially dependent jobs can be executed at the same time. However, our jobs must also coexist with those of other researchers, which limits both the amount of CPU time and memory we can use before disrupting other work. Our code is designed to take advantage of the features of and respect the constraints of this environment.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"During the estimation step, we simulate drawing a large number of parameters (typically 100,000) from their posterior distribution. In the forecast step, these draws are read in and used to compute the desired outputs for our observed variables and the latent states. As discussed before, these outputs can include smoothed shock times series, forecasts, shock decompositions, and impulse response functions. Since these computations are independent for each parameter draw, forecasting using the full distribution lends itself well to parallelization.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"To reduce our impact on other users of the cluster, we make use of a \"blocking\" scheme in our Julia code. The parameter draws are read in blocks of typically 5,000 draws. These draws are then immediately distributed using Julia's pmap function (\"parallel map\") to worker processes, each of which carries out the entire forecast step for just one draw. When all of the draws from that block have completed, the originator process re-collects the forecast outputs and saves them to disk. This repeats until all blocks have completed. Through this blocking, we can avoid keeping too much data in memory, since we only operate on a fraction of the parameter draws at any given time. However, we can still write structured output files using the HDF5 file format, which allows us to write to specific subsets of pre-allocated arrays, so that the end result is as if we had computed all the draws at once without blocking.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Before settling on this version, we also tried using Julia's DistributedArrays.jl package, which distributes large arrays in memory over many processes. This allowed us to hold all of our parameter draws and their corresponding forecast outputs in memory at the same time, and it allowed each process to operate on the parameter draws it held locally without needing to copy data back and forth between processes. However, using distributed arrays also forced us to explicitly handle lower-level tasks like assigning parameter draws to processes. Since each process handled a predetermined set of draws, it was not easy to reallocate draws if some of the compute nodes on which the processes lived happened to be busier than others on a particular day. Switching to pmap allowed us to abstract away from many of these concerns, as it has already been optimized to take advantage of the aforementioned independence of parameter draws.","category":"page"},{"location":"julia_forecasting/#StateSpaceRoutines.jl-1","page":"MATLAB to Julia Transition: Forecast","title":"StateSpaceRoutines.jl","text":"","category":"section"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"A big benefit of using Julia is the large and growing package ecosystem, which allows all users to access high-quality open-source code. Thanks to this system, Julia developers can focus their development time on the issues and projects they really care about, without having to repeatedly reinvent the wheel. For example, DSGE.jl* depends on the DataFrames.jl package to help us manage data and dates. Similarly, DSGE.jl* is available for members of the community to modify, extend, and make use of as they see fit. In this spirit, we have decided to break out some DSGE-independent components of DSGE.jl* into their own package.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"DSGE models define a linear system that links observed variables to unobserved states. In order to actually perform inference on these latent states, we apply the Kalman filter and smoothing algorithms. State space models are commonly used across many disciplines, and indeed the routines we use in DSGE.jl* can be applied to any sort of linear state space model. As such, we have decided to move the filtering and smoothing routines that we have historically used with the DSGE model into StateSpaceRoutines.jl, a new package that will provide DSGE.jl*-independent filtering and smoothing routines.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"StateSpaceRoutines.jl currently features the one filter and four smoothers we most commonly use in DSGE.jl. On the filtering front, we implement the standard Kalman filter found in James Hamilton's *Time Series Analysis StateSpaceRoutines.jl also contains two Kalman smoothers and two simulation smoothers. In addition to the Kalman smoother presented in Time Series Analysis, we also have Jan Koopman's disturbance smoother from his paper Disturbance Smoother for State Space Models. The two simulation smoothers are based on Carter and Kohn's On Gibbs Sampling for State Space Models and Durbin and Koopman's A Simple and Efficient Simulation Smoother for State Space Time Series Analysis. In our experience, the Koopman smoother is faster than the standard Kalman smoother, as it does not require us to calculate the pseudo- inverses of the predicted variance matrices. For the same reason, we have also found that the Durbin and Koopman simulation smoother is faster than the Carter and Kohn one. All of these methods support time-varying matrices and variances. We use this feature to model pre–zero-lower-bound and zero-lower-bound regimes in our DSGE models, but the functionality is general enough to be applied to a wider range of models with regime switching or time varying matrices and variances. We hope that the broader Julia community finds these functions as useful as we have!","category":"page"},{"location":"julia_forecasting/#Disclaimer-1","page":"MATLAB to Julia Transition: Forecast","title":"Disclaimer","text":"","category":"section"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"This post reflects the experience of the authors with Julia and MATLAB and does not represent an endorsement by the Federal Reserve Bank of New York or the Federal Reserve System of any particular product or service. The views expressed in this post are those of the authors and do not necessarily reflect the position of the Federal Reserve Bank of New York or the Federal Reserve System. Any errors or omissions are the responsibility of the authors.","category":"page"},{"location":"julia_forecasting/#References-1","page":"MATLAB to Julia Transition: Forecast","title":"References","text":"","category":"section"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Carter, C. and Cohn, R. (1994). On Gibbs Sampling for State Space models. Biometrika.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Durbin, K. and Koopman, S. (2002). A Simple and Efficient Smoother for State Space Time Series Analysis. Biometrika.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Hamilton, J. (1994). Time Series Analysis. Princeton: Princeton University Press.","category":"page"},{"location":"julia_forecasting/#","page":"MATLAB to Julia Transition: Forecast","title":"MATLAB to Julia Transition: Forecast","text":"Koopman, S. (1993). Disturbance Smoother for State Space Models. Biometrika.","category":"page"},{"location":"license/#License-1","page":"License","title":"License","text":"","category":"section"},{"location":"license/#","page":"License","title":"License","text":"Copyright (c) 2015, Federal Reserve Bank of New York All rights reserved.","category":"page"},{"location":"license/#","page":"License","title":"License","text":"Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:","category":"page"},{"location":"license/#","page":"License","title":"License","text":"Redistributions of source code must retain the above copyright notice, this list of","category":"page"},{"location":"license/#","page":"License","title":"License","text":"conditions and the following disclaimer.","category":"page"},{"location":"license/#","page":"License","title":"License","text":"Redistributions in binary form must reproduce the above copyright notice, this list of","category":"page"},{"location":"license/#","page":"License","title":"License","text":"conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.","category":"page"},{"location":"license/#","page":"License","title":"License","text":"Neither the name of the copyright holder nor the names of its contributors may be used to","category":"page"},{"location":"license/#","page":"License","title":"License","text":"endorse or promote products derived from this software without specific prior written permission.","category":"page"},{"location":"license/#","page":"License","title":"License","text":"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","category":"page"}]
}
